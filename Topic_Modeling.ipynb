{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFAEOkycuZP1kzNxbje+Nc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhinavs476/Topic-Modeling/blob/main/Topic_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "1Dw6jFl6XMPy",
        "outputId": "db59ce12-6d6d-43d0-dd83-8bae2f6a9322"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     id  year                                              title event_type  \\\n",
              "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
              "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
              "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
              "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
              "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
              "\n",
              "                                            pdf_name          abstract  \\\n",
              "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
              "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
              "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
              "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
              "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
              "\n",
              "                                          paper_text  \n",
              "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
              "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
              "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
              "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
              "4  Neural Network Ensembles, Cross\\nValidation, a...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2a6309dc-7f2e-4ad3-a4f5-2b89940f0d5f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1-self-organization-of-associative-database-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a6309dc-7f2e-4ad3-a4f5-2b89940f0d5f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2a6309dc-7f2e-4ad3-a4f5-2b89940f0d5f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2a6309dc-7f2e-4ad3-a4f5-2b89940f0d5f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f7805f17-dcc5-4e83-b631-6152eeda7af6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f7805f17-dcc5-4e83-b631-6152eeda7af6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f7805f17-dcc5-4e83-b631-6152eeda7af6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "papers",
              "summary": "{\n  \"name\": \"papers\",\n  \"rows\": 7241,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2098,\n        \"min\": 1,\n        \"max\": 7284,\n        \"num_unique_values\": 7241,\n        \"samples\": [\n          1466,\n          3336,\n          6755\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1987,\n        \"max\": 2017,\n        \"num_unique_values\": 31,\n        \"samples\": [\n          1992,\n          1990,\n          2012\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7241,\n        \"samples\": [\n          \"Independent Component Analysis for Identification of Artifacts in Magnetoencephalographic Recordings\",\n          \"Near-Maximum Entropy Models for Binary Neural Representations of Natural Images\",\n          \"Nearest-Neighbor Sample Compression: Efficiency, Consistency, Infinite Dimensions\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"event_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Oral\",\n          \"Spotlight\",\n          \"Poster\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdf_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7241,\n        \"samples\": [\n          \"1466-independent-component-analysis-for-identification-of-artifacts-in-magnetoencephalographic-recordings.pdf\",\n          \"3336-near-maximum-entropy-models-for-binary-neural-representations-of-natural-images.pdf\",\n          \"6755-nearest-neighbor-sample-compression-efficiency-consistency-infinite-dimensions.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3923,\n        \"samples\": [\n          \"Recommendation for e-commerce with a mix of durable and nondurable goods has characteristics that distinguish it from the well-studied media recommendation problem. The demand for items is a combined effect of form utility and time utility, i.e., a product must both be intrinsically appealing to a consumer and the time must be right for purchase. In particular for durable goods, time utility is a function of inter-purchase duration within product category because consumers are unlikely to purchase two items in the same category in close temporal succession. Moreover, purchase data, in contrast to ratings data, is implicit with non-purchases not necessarily indicating dislike. Together, these issues give rise to the positive-unlabeled demand-aware recommendation problem that we pose via joint low-rank tensor completion and product category inter-purchase duration vector estimation. We further relax this problem and propose a highly scalable alternating minimization approach with which we can solve problems with millions of users and millions of items in a single thread. We also show superior prediction accuracies on multiple real-world data sets.\",\n          \"Stochastic Neighbor Embedding (SNE) has shown to be quite promising for data visualization.  Currently, the most popular implementation, t-SNE, is restricted to a particular Student t-distribution as its embedding distribution. Moreover, it uses a gradient descent algorithm that may require users to tune parameters such as the learning step size, momentum, etc., in finding its optimum. In this paper, we propose the Heavy-tailed Symmetric Stochastic Neighbor Embedding (HSSNE) method, which is a generalization of the t-SNE to accommodate various heavy-tailed embedding similarity functions. With this generalization, we are presented with two difficulties.  The first is how to select the best embedding similarity among all heavy-tailed functions and the second is how to optimize the objective function once the heave-tailed function has been selected. Our contributions then are: (1) we point out that various heavy-tailed embedding similarities can be characterized by their negative score functions. Based on this finding, we present a parameterized subset of similarity functions for choosing the best tail-heaviness for HSSNE; (2) we present a fixed-point optimization algorithm that can be applied to all heavy-tailed functions and does not require the user to set any parameters; and (3) we present two empirical studies, one for unsupervised visualization showing that our optimization algorithm runs as fast and as good as the best known t-SNE implementation and the other for semi-supervised visualization showing quantitative superiority using the homogeneity measure as well as qualitative advantage in cluster separation over t-SNE.\",\n          \"Adaptive schemes, where tasks are assigned based on the data collected thus far, are widely used in practical crowdsourcing systems to efficiently allocate the budget. However, existing theoretical analyses of crowdsourcing systems suggest that the gain of adaptive task assignments is minimal. To bridge this gap, we investigate this question under a strictly more general probabilistic model, which has been recently introduced to model practical crowdsourcing data sets. Under this generalized Dawid-Skene model, we characterize the fundamental trade-off between budget and accuracy, and introduce a novel adaptive scheme that matches this fundamental limit. We further quantify the gain of adaptivity, by comparing the trade-off with the one for non-adaptive schemes, and confirm that the gain is significant and can be made arbitrarily large depending on the distribution of the difficulty level of the tasks at hand.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7237,\n        \"samples\": [\n          \"Statistical Performance of Convex Tensor\\nDecomposition\\nRyota Tomioka?\\nTaiji Suzuki?\\nDepartment of Mathematical Informatics,\\nThe University of Tokyo\\nTokyo 113-8656, Japan\\ntomioka@mist.i.u-tokyo.ac.jp\\ns-taiji@stat.t.u-tokyo.ac.jp\\n\\nKohei Hayashi?\\nGraduate School of Information Science,\\nNara Institute of Science and Technology\\nNara 630-0192, Japan\\nkohei-h@is.naist.jp\\n\\n?\\n\\n?\\n\\nHisashi Kashima?,?\\nBasic Research Programs PRESTO,\\nSynthesis of Knowledge for Information Oriented Society, JST\\nTokyo 102-8666, Japan\\nkashima@mist.i.u-tokyo.ac.jp\\n?\\n\\nAbstract\\nWe analyze the statistical performance of a recently proposed convex tensor decomposition algorithm. Conventionally tensor decomposition has been formulated as non-convex optimization problems, which hindered the analysis of their\\nperformance. We show under some conditions that the mean squared error of\\nthe convex method scales linearly with the quantity we call the normalized rank\\nof the true tensor. The current analysis naturally extends the analysis of convex\\nlow-rank matrix estimation to tensors. Furthermore, we show through numerical\\nexperiments that our theory can precisely predict the scaling behaviour in practice.\\n\\n1 Introduction\\nTensors (multi-way arrays) generalize matrices and naturally represent data having more than two\\nmodalities. For example, multi-variate time-series, for instance, electroencephalography (EEG),\\nrecorded from multiple subjects under various conditions naturally form a tensor. Moreover, in\\ncollaborative ?ltering, users? preferences on products, conventionally represented as a matrix, can\\nbe represented as a tensor when the preferences change over time or context.\\nFor the analysis of tensor data, various models and methods for the low-rank decomposition of\\ntensors have been proposed (see Kolda & Bader [12] for a recent survey). These techniques have\\nrecently become increasingly popular in data-mining [1, 14] and computer vision [25, 26]. Besides\\nthey have proven useful in chemometrics [4], psychometrics [24], and signal processing [20, 7, 8].\\nDespite empirical success, the statistical performance of tensor decomposition algorithms has not\\nbeen fully elucidated. The dif?culty lies in the non-convexity of the conventional tensor decomposition algorithms (e.g., alternating least squares [6]). In addition, studies have revealed many\\ndiscrepancies (see [12]) between matrix rank and tensor rank, which make extension of studies on\\nthe performance of low-rank matrix models (e.g., [9]) challenging.\\nRecently, several authors [21, 10, 13, 23] have focused on the notion of tensor mode-k rank (instead\\nof tensor rank), which is related to the Tucker decomposition [24]. They discovered that regularized\\nestimation based on the Schatten 1-norm, which is a popular technique for recovering low-rank\\nmatrices via convex optimization, can also be applied to tensor decomposition. In particular, the\\n1\\n\\n\\fConvex\\nTucker (exact)\\nOptimization tolerance\\n\\n0\\n\\n10\\n\\n?3\\n\\n10\\n\\n0\\n\\n0.2\\n0.4\\n0.6\\n0.8\\nFraction of observed elements\\n\\n1\\n\\nFigure 1: Result of estimation of rank-(7, 8, 9) tensor of dimensions\\n50???? 50 ? 20 from partial\\n???\\n? ? W ? ??? is plotted against the\\nmeasurements; see [23] for the details. The estimation error ???W\\nF\\nfraction of observed elements m = M/N . Error bars over 10 repetitions are also shown. Convex\\nrefers to the convex tensor decomposition based on the minimization problem (7). Tucker (exact)\\nrefers to the conventional (non-convex) Tucker decomposition [24] at the correct rank. Gray dashed\\nline shows the optimization tolerance 10?3 . The question is how we can predict the point where the\\ngeneralization begins (roughly m = 0.35 in this plot).\\n\\nstudy in [23] showed that there is a clear transition at certain number of samples where the error\\ndrops dramatically from no generalization to perfect generalization (see Figure 1).\\nIn this paper, motivated by the above recent work, we mathematically analyze the performance of\\nconvex tensor decomposition. The new convex formulation for tensor decomposition allows us to\\ngeneralize recent results on Schatten 1-norm-regularized estimation of matrices (see [17, 18, 5, 19]).\\nUnder a general setting we show how the estimation error scales with the mode-k ranks of the true\\ntensor. Furthermore, we analyze the speci?c settings of (i) noisy tensor decomposition and (ii)\\nrandom Gaussian design. In the ?rst setting, we assume that all the elements of a low-rank tensor\\nis observed with noise and the goal is to recover the underlying low-rank structure. This is the most\\ncommon setting a tensor decomposition algorithm is used. In the second setting, we assume that\\nthe unknown tensor is a coef?cient of a tensor-input scalar-output regression problem and the input\\ntensors (design) are randomly given from independent Gaussian distributions. Surprisingly, it turns\\nout that the random Gaussian setting can precisely predict the phase-transition-like behaviour in\\nFigure 1. To the best of our knowledge, this is the ?rst paper that rigorously studies the performance\\nof a tensor decomposition algorithm.\\n\\n2\\n\\nNotation\\n\\nIn this section, we introduce the notations we use in this paper. Moreover, we introduce a H?olderlike inequality (3) and the notion of mode-k decomposability (5), which play central roles in our\\nanalysis.\\nQK\\nLet X ? Rn1 ????nK be a K-way tensor. We denote the number of elements in X by N = k=1 nk .\\n?\\nThe inner product between two tensors ?W, X ? is de?ned as ?W, X ? = vec(W)\\np ), where\\n??? ??? vec(X\\nvec is a vectorization. In addition, we de?ne the Frobenius norm of a tensor ???X ???F = ?X , X ?.\\nQ\\nThe mode-k unfolding X (k) is the nk ? n\\n? \\\\k (?\\nn\\\\k := k? ?=k nk? ) matrix obtained by concatenating\\nthe mode-k ?bers (the vectors obtained by ?xing every index of X but the kth index) of X as column\\nvectors. The mode-k rank of a tensor X , denoted by rankk (X ), is the rank of the mode-k unfolding\\nX (k) (as a matrix). Note that when K = 2 and X is actually a matrix, and X (2) = X (1) ? . We say\\na tensor X is rank (r1 , . . . , rK ) when rk = rankk (X ) for k = 1, . . . , K. Note that the mode-k rank\\ncan be computed in a polynomial time, because it boils down to computing a matrix rank, whereas\\ncomputing tensor rank is NP complete [11]. See [12] for more details.\\nSince for each k, the convex envelope of the mode-k rank is given as the Schatten 1-norm [18]\\n(known as the trace norm [22] or the nuclear norm [3]), it is natural to consider the following\\n2\\n\\n\\f??? ???\\noverlapped Schatten 1-norm ???W ???S of a tensor W ? Rn1 ?????nK (see also [21]):\\n1\\n\\n??? ???\\n???W ???\\n\\nS1\\n\\n=\\n\\nK\\n?\\n1 X?\\n?W (k) ? ,\\nS1\\nK\\n\\n(1)\\n\\nk=1\\n\\nwhere W (k) is the mode-k unfolding of W. Here ? ? ?S1 is the Schatten 1-norm for a matrix\\nXr\\n?W ?S1 =\\n?j (W ),\\nj=1\\n\\nwhere ?j (W ) is the jth largest singular-value of W . The dual norm of the Schatten 1-norm is the\\nSchatten ?-norm (known as the spectral norm) as follows:\\n?X?S? = max ?j (X).\\nj=1,...,r\\n\\nSince the two norms ? ? ?S1 and ? ? ?S? are dual to each other, we have the following inequality:\\n|?W , X?| ? ?W ?S1 ?X?S? ,\\n(2)\\nwhere ?W , X? is the inner product of W and X.\\nThe same inequality holds for the overlapped Schatten 1-norm (1) and its dual norm. The dual norm\\nof the overlapped Schatten 1-norm can be characterized by the following lemma.\\n??? ???\\nLemma 1. The dual norm of the overlapped Schatten 1-norm denoted as ???????S ? is de?ned as the\\n1\\nin?mum of the maximum mode-k spectral norm over the tensors whose average equals the given\\ntensor X as follows:\\n??? ???\\n(k)\\n???X ??? ? =\\nmax ?Y (k) ?S? ,\\ninf\\nS1\\n1\\n(1) +Y (2) +???+Y (K) =X\\nk=1,...,K\\nY\\n(\\n)\\nK\\n(k)\\n\\nwhere Y (k) is the mode-k unfolding of Y (k) . Moreover, the following upper bound on the dual norm\\n??? ???\\n??????? ? is valid:\\nS1\\n\\n??? ???\\n???X ???\\n\\nS1?\\n\\n??? ???\\n1 XK\\n?X (k) ?S? .\\n? ???X ???mean :=\\nk=1\\nK\\n\\n??? ???\\nProof. The ?rst part can be shown by solving the dual of the maximization problem ???X ???S ? :=\\n1\\n??? ???\\nsup ?W, X ? s.t. ???W ???S1 ? 1. The second part is obtained by setting Y (k) = PK K1/c ? X /ck ,\\nwhere ck = ?X (k) ?S? , and using Jensen?s inequality.\\n\\nk? =1\\n\\nk\\n\\nAccording to Lemma 1, we have the ??following\\n? ??? ??? H?\\n?o??lder-like\\n??? inequality\\n??? ??? ???\\n|?W, X ?| ? ???W ???S1 ???X ???S ? ? ???W ???S1 ???X ???mean .\\n\\n(3)\\n??? ??? ??? ???\\nNote that the above bound is tighter than the more intuitive relation | ?W, X ? | ? ???W ???S ???X ???S\\n1\\n?\\n??? ???\\n(???X ???S? := max1,...,K ?X (k) ?S? ), which one might come up as an analogy to the matrix case (2).\\n1\\n\\nFinally, let W ? ? Rn1 ?????nK be the low-rank tensor that we wish to recover. We assume that W ?\\nis rank (r1 , . . . , rK ). Thus, for each k we have\\nW ?(k) = U k S k V k\\n(k = 1, . . . , K),\\nwhere U k ? Rnk ?rk and V k ? Rn? \\\\k ?rk are orthogonal, and S k ? Rrk ?rk is diagonal. Let\\n? ? Rn1 ?????nK be an arbitrary tensor. We de?ne the mode-k orthogonal complement ???k of an\\nunfolding ?(k) ? Rnk ??n\\\\k of ? with respect to the true low-rank tensor W ? as follows:\\n??k\\n\\n???k = (I nk ? U k U k ? )?(k) (I n? \\\\k ? V k V k ? ).\\n\\n(4)\\n\\n:= ?(k) ? ???k is\\nthe true tensor W ?(k) .\\n\\nIn addition\\nthe component having overlapped row/column space with the\\nunfolding of\\nNote that the decomposition ?(k) = ??k + ???k is de?ned for\\neach mode; thus we use subscript k instead of (k).\\nUsing the decomposition de?ned above we have the following equality, which we call mode-k decomposability of the Schatten 1-norm:\\n?W ?(k) + ???k ?S1 = ?W ?(k) ?S1 + ????k ?S1 (k = 1, . . . , K).\\n(5)\\nThe above decomposition is de?ned for each mode and thus it is weaker than the notion of decomposability discussed by Negahban et al. [15].\\n3\\n\\n\\f3\\n\\nTheory\\n\\nIn this section, we ?rst present a deterministic result that holds under a certain choice of regularization constant ?M and an assumption called the restricted strong convexity. Then, we focus on\\nspecial cases to justify the choice of regularization constant and the restricted strong convexity assumption. We analyze the setting of (i) noisy tensor decomposition and (ii) random Gaussian design\\nin Section 3.2 and Section 3.3, respectively.\\n3.1\\n\\nMain result\\n\\nOur goal is to estimate an unknown rank (r1 , . . . , rK ) tensor W ? ? Rn1 ????nK from observations\\nyi = ?Xi , W ? ? + ?i (i = 1, . . . , M ).\\n(6)\\nHere the noise ?i follows the independent zero-mean Gaussian distribution with variance ? 2 .\\nWe employ the regularized empirical risk minimization problem proposed in [21, 10, 13, 23] for the\\nestimation of W as follows:\\n??? ???\\n1\\nminimize\\n?y ? X(W)?22 + ?M ???W ???S1 ,\\n(7)\\nn\\n?????n\\n1\\nK\\n2M\\nW?R\\nwhere y = (y1 , . . . , yM )? is the collection of observations; X : Rn1 ?????nK ? RM is a linear\\noperator that maps W to the M dimensional output vector X(W) = (?X1 , W? , . . . , ?XM , W?) ? ?\\nRM . The Schatten 1-norm term penalizes every mode of W to be jointly low-rank (see Equation (1));\\n?M > 0 is the regularization constant. Accordingly, the solution of the minimization problem (7) is\\ntypically a low-rank tensor when ?M is suf?ciently large. In addition, we denote the adjoint operator\\nPM\\nof X as X? : RM ? Rn1 ?????nK ; that is X? (?) = i=1 ?i Xi ? Rn1 ?????nK .\\n? ? W?\\nThe ?rst step in our analysis is to characterize the particularity of the residual tensor ? := W\\nas in the following lemma.\\n???\\n???\\n? be the solution of the minimization problem (7) with ?M ? 2???X? (?)???\\nLemma 2. Let W\\n/M ,\\nmean\\n? ? W ? , where W ? is the true low-rank tensor. Let ?(k) = ?? + ??? be the\\nand let ? := W\\nk\\nk\\ndecomposition de?ned in Equation (4). Then we have the following inequalities:\\n1. rank(??k ) ? 2rk for each k = 1, . . . , K.\\nPK\\nPK\\n?\\n??\\n2.\\nk=1 ??k ?S1 .\\nk=1 ??k ?S1 ? 3\\nProof. The proof uses the mode-k decomposability (5) and is analogous to that of Lemma 1 in\\n[17].\\nThe second ingredient of our analysis is the restricted strong convexity. Although, ?strong? may\\nsound like a strong assumption, the point is that we require this assumption to hold only for the\\nparticular residual tensor we characterized in Lemma 2. The assumption can be stated as follows.\\nAssumption 1 (Restricted strong convexity). We suppose that there is a positive constant ?(X) such\\nthat the operator X satis?es the inequality\\n??? ???2\\n1\\n?X(?)?22 ??(X)???????F ,\\n(8)\\nM\\nPK\\nfor all ? ? Rn1 ?????nK such that for each k = 1, . . . , K, rank(??k ) ? 2rk and k=1 ????k ?S1 ?\\nPK\\n3 k=1 ???k ?S1 , where ??k and ???k are de?ned through the decomposition (4).\\nNow using the above two ingredients, we are ready to prove the following deterministic guarantee\\non the performance of the estimation procedure (7).\\n???\\n???\\n? be the solution of the minimization problem (7) with ?M ? 2???X? (?)???\\nTheorem 1. Let W\\n/M .\\nmean\\nSuppose that the operator X satis?es the restricted strong convexity condition. Then the following\\nbound is true:\\nPK ?\\n???\\n???\\n? ? W ? ??? ? 32?M k=1 rk .\\n???W\\n(9)\\nF\\n?(X)K\\n4\\n\\n\\f? ? W ? . Combining the fact that the objective value for W\\n?\\nProof. Let ? = W\\n??? ? ???\\n??? is??smaller\\n?\\n??than\\n? ??? that for\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\nW , the H?older-like inequality (3), the triangular inequality W S ? W S ? ?????S , and\\n1\\n1\\n1\\n???\\n???\\nthe assumption ???X? (?)/M ???\\n? ?M /2, we obtain\\nmean\\n\\n??? ???\\n???\\n???\\n??? ???\\n??? ???\\n1\\n(10)\\n?X(?)?22 ? ???X? (?)/M ???mean ???????S1 + ?M ???????S1 ? 2?M ???????S1 .\\n2M\\nNow the left-hand side can be lower-bounded using the restricted strong convexity (8). On the other\\nhand, using Lemma 2, the right-hand side can be upper-bounded as follows:\\n??? ???\\n??? ???\\n??? ???\\n?\\n??????? ? 1 PK (???k ?S1 + ????k ?S1 ) ? 4 PK ???k ?S1 ? 4 ? F PK\\n2rk , (11)\\nk=1\\nk=1\\nk=1\\nK\\nK\\nK\\nS1\\n??? ???\\nwhere the last inequality follows because ???????F = ??(k) ?F for k = 1, . . . , K. Combining inequalities (8), (10), and (11), we obtain our claim (9).\\nNegahban et al. [15] (see also [17]) pointed out that the key properties for establishing a sharp convergence result for a regularized M -estimator is the decomposability of the regularizer and the restricted strong convexity. What we have shown suggests that the weaker mode-k decomposability (5)\\nsuf?ce to obtain the above convergence result for the overlapped Schatten 1-norm (1) regularization.\\n3.2 Noisy Tensor Decomposition\\nIn this subsection, we consider the setting where all the elements are observed (with noise) and the\\ngoal is to recover the underlying low-rank tensor without noise.\\nSince all the elements are observed only once, X is simply a vectorization\\n(M =\\n???\\n??? N ), and the left2\\n? ? W ? ??? . Therefore, the\\n?\\n?\\n?\\n=\\nW\\nhand side of inequality (10)???gives the\\nquantity\\nof\\ninterest\\n?X(?)?\\n2\\nF\\n???\\nremaining task is to bound ???X? (?)???mean as in the following lemma.\\nLemma 3. Suppose\\n???\\n?that\\n?? X : n1 ?? ? ??nK ? N is a vectorization of a tensor. With high probability\\nthe quantity ???X? (?)???mean is concentrated around its mean, which can be bounded as follows:\\nK\\n???\\n???\\n?\\np\\n? X ??\\nE???X? (?)???mean ?\\nnk + n\\n? \\\\k .\\nK\\n\\n(12)\\n\\nk=1\\n\\n???\\n???\\nSetting the regularization constant as ?M = c0 E???X? (?)???mean /N , we obtain the following theorem.\\nTheorem 2. Suppose that X : n1 ?? ? ??nK ? N is a vectorization of a tensor. There are universal\\nconstants c0 and c1 , such that, with high probability, any solution of the minimization problem (7)\\nPK ?\\np\\nwith regularization constant ?M = c0 ? k=1 ( nk + n\\n? \\\\k )/(KN ) satis?es the following bound:\\n?\\n!2 ?\\n!2\\nK\\nK\\nX\\nX\\n???\\n???2\\n??\\n?\\np\\n?\\n1\\n1\\n?\\n2\\n? ? W ??? ? c1 ?\\n???W\\nnk + n\\n? \\\\k\\nrk .\\nF\\nK\\nK\\nk=1\\n\\nk=1\\n\\nProof. Combining Equations (10)?(11) with the fact that X is simply a vectorization and M = N ,\\nwe have\\n?\\n1\\n? ? W ? ?F ? 16 2?M PK ?rk .\\n?W\\nN\\n\\nK\\n\\nk=1\\n\\nSubstituting the choice of regularization constant ?M and squaring both sides, we obtain our claim.?\\nWe can simplify the result of Theorem 2 by noting that n\\n? \\\\k = N/nk ? nk , when the dimenPK ? 2\\n1\\nsions are of the same order. Introducing the notation ?r?1/2 = ( K\\nrk ) and n?1 :=\\nk=1\\n(1/n1 , . . . , 1/nK ), we have\\n???\\n???\\n? ? W ? ???2\\n???W\\n?\\n?\\nF\\n? Op ? 2 ?n?1 ?1/2 ?r?1/2 .\\n(13)\\nN\\nWe call the quantity r? = ?n?1 ?1/2 ?r?1/2 the normalized rank, because r? = r/n when the dimensions are balanced (nk = n and rk = r for all k = 1, . . . , K).\\n5\\n\\n\\f3.3\\n\\nRandom Gaussian Design\\n\\nIn this subsection, we consider the case the elements of the input tensors Xi (i = 1, . . . , M ) in the\\nobservation model (6) are distributed according to independent identical standard Gaussian distributions. We call this setting random Gaussian design.\\n???\\n???\\nFirst we show an upper bound on the norm ???X? (?)???mean , which we use to specify the scaling of\\nthe regularization constant ?M in Theorem 1.\\nLemma 4. Let X : Rn1 ?????nK ? RM be a random Gaussian design. In addition, we assume\\nthat\\n?i is sampled independently from N (0, ? 2 ). Then with high probability the quantity\\n?\\n??? ? the ??noise\\n???X (?)???\\nis concentrated around its mean, which can be bounded as follows:\\nmean\\n???\\n???\\nE???X? (?)???\\n\\nmean\\n\\n?\\nK\\n?\\np\\n? M X ??\\n?\\nnk + n\\n? \\\\k .\\nK\\nk=1\\n\\nNext the following lemma, which is a generalization of a result presented in Negahban and Wainwright [17, Proposition 1], provides a ground for the restricted strong convexity assumption (8).\\nLemma 5. Let X : Rn1 ?????nK ? RM be a random Gaussian design. Then it satis?es\\n?r\\n!\\nr\\nK\\nn\\n? \\\\k ?????? ??????\\n1 X\\n?X(?)?2\\n1 ?????? ??????\\nnk\\n?\\n? F?\\n+\\n? S1 ,\\n?\\n4\\nK\\nM\\nM\\nM\\nk=1\\n\\nwith probability at least 1 ? 2 exp(?N/32).\\nProof. The proof is analogous to that of Proposition 1 in [17] except that we use H?older-like inequality (3) for tensors instead of inequality (2) for matrices.\\nFinally, we obtain the following convergence bound.\\nTheorem 3. Under the random Gaussian design setup, there are universal constants c0 , c1 , and c2\\nPK ?\\nPK ? 2\\np\\n1\\n1\\nsuch that for a sample size M ? c1 ( K\\nn\\n? \\\\k ))2 ( K\\nrk ) , any solution of the\\nk=1 ( nk +\\n?\\nPk=1\\np\\n?\\nK\\nminimization problem (7) with regularization constant ?M = c0 ? k=1 ( nk + n\\n? \\\\k )/(K M )\\nsatis?es the following bound:\\nPK ?\\nPK ? 2\\np\\n1\\n1\\n???\\n???\\n?2 ( K\\nn\\n? \\\\k ))2 ( K\\nk=1 ( nk +\\nk=1 rk )\\n? ? W ? ???2 ? c2\\n???W\\n,\\nF\\nM\\nwith high probability.\\nAgain we can simplify the result of Theorem 3 as follows: for sample size M ? c1 N r? we have\\n?\\n?\\n?1\\n???\\n???\\n? ? W ? ???2 ? Op ? 2 N ?n ?1/2 ?r?1/2 ,\\n???W\\n(14)\\nF\\nM\\nwhere r? = ?n?1 ?1/2 ?r?1/2 is the normalized rank. Note that the condition on the number of\\nsamples M does not depend on the noise variance ? 2 . Therefore in the limit ? 2 ? 0, the bound (14)\\nis suf?ciently small but only valid for sample size M that exceeds c1 N r?, which implies a threshold\\nbehavior as in Figure 1.\\nNote also that in the matrix case (K = 2), r1 = r2 = r and N ?n?1 ?1/2 = O(n1 + n2 ). Therefore\\n? ? W ? ?2 ?\\nwe can restate the above result as for sample size M ? c1 r(n1 + n2 ), we have ?W\\nF\\nOp (r(n1 + n2 )/M ), which is compatible with the result in [17, 18].\\n\\n4\\n\\nExperiments\\n\\nIn this section, we conduct two numerical experiments to con?rm our analysis in Section 3.2 and\\nSection 3.3.\\n6\\n\\n\\f?4\\n\\n3\\n\\nx 10\\n\\n0.03\\n\\nsize=[50 50 20] ?M=0.03/N\\nsize=[50 50 20] ?M=0.33/N\\n\\nsize=[50 50 20] ?M=2.34/N\\n\\nsize=[50 50 20] ? =0.54/N\\n\\n0.025\\n\\nM\\n\\nsize=[100 100 50] ?M=0.66/N\\n\\nsize=[100 100 50] ?M=0.69/N\\n\\nMean squared error\\n\\nMean squared error\\n\\nsize=[50 50 20] ? =6/N\\nM\\n\\nsize=[100 100 50] ?M=0.06/N\\n2\\n\\nsize=[50 50 20] ?M=0.33/N\\n\\nsize=[100 100 50] ? =1.11/N\\nM\\n\\n1\\n\\n0.02\\n\\nsize=[100 100 50] ? =4.5/N\\nM\\n\\nsize=[100 100 50] ?M=12/N\\n0.015\\n\\n0.01\\n\\n0.005\\n\\n0\\n0\\n\\n0.2\\n\\n0.4\\n0.6\\nNormalized rank\\n\\n0.8\\n\\n0\\n0\\n\\n1\\n\\n(a) Small noise (? = 0.01).\\n\\n0.2\\n\\n0.4\\n0.6\\nNormalized rank\\n\\n0.8\\n\\n1\\n\\n(b) Large noise (? = 0.1).\\n\\nFigure 2: Result of noisy tensor decomposition for tensors of size 50 ? 50 ? 20 and 100 ? 100 ? 50.\\n\\n4.1\\n\\nNoisy Tensor Decomposition\\n\\nWe randomly generated low-rank tensors of dimensions n(1) = (50, 50, 20) and n(2) =\\n(100, 100, 50) for various ranks (r1 , . . . , rK ). For a speci?c rank, we generated the true tensor\\nby drawing elements of the r1 ? ? ? ? ? rK ?core tensor? from the standard normal distribution and\\nmultiplying its each mode by an orthonormal factor randomly drawn from the Haar measure. As\\ndescribed in Section 3.2, the observation y consists of all the elements of the original tensor once\\n(M = N ) with additive independent Gaussian noise with variance ? 2 . We used the alternating\\ndirection method of multipliers (ADMM) for ?constraint? approaches described in [23, 10] to solve\\nthe minimization problem (7). The whole experiment was repeated 10 times and averaged.\\n???\\n???\\n? ? W ? ???2 /N is plotted against\\nThe results are shown in Figure 2. The mean squared error ???W\\nF\\nthe normalized rank r? = ?n?1 ?1/2 ?r?1/2 (of the true tensor) de?ned in Equation (13). Since the\\nchoice of the regularization constant ?M only depends on the size of the tensor and not on the ranks\\nof the underlying tensor in Theorem 2, we ?x the regularization constant to some different values\\nand report the dependency of the estimation error on the normalized rank r? of the true tensor.\\nFigure 2(a) shows the result for small noise (? = 0.01) and Figure 2(b) shows the result for large\\n???\\n???\\n? ? W ? ???2 grows linearly\\nnoise (? = 0.1). As predicted by Theorem 2, the squared error ???W\\nF\\nagainst the normalized rank r?. This behaviour is consistently observed not only around the preferred\\nregularization constant value (triangles) but also in the over-?tting case (circles) and the under?tting case (crosses). Moreover, as predicted by Theorem 2, the preferred regularization constant\\nvalue scales linearly and the squared error scales quadratically to the noise standard deviation ?.\\nAs predicted by Lemma 3, the curves for the smaller 50 ? 50 ? 20 tensor and those for the larger\\n100 ? 100 ? 50 tensor seem to agree when the regularization constant\\nis scaled by the factor two.\\np\\nNote that the dominant term in inequality (12) is the second term n\\n? \\\\k , which is roughly scaled by\\nthe factor two from 50 ? 50 ? 20 to 100 ? 100 ? 50.\\n4.2\\n\\nTensor completion from partial observations\\n\\nIn this subsection, we repeat the simulation originally done by Tomioka et al. [23] and demonstrate\\nthat our results in Section 3.3 can precisely predict the empirical scaling behaviour with respect to\\nboth the size and rank of a tensor.\\nWe present results for both matrix completion (K = 2) and tensor completion (K = 3). For\\nthe matrix case, we randomly generated low-rank matrices of dimensions 50 ? 20, 100 ? 40, and\\n250 ? 200. For the tensor case, we randomly generated low-rank tensors of dimensions 50 ? 50 ? 20\\nand 100 ? 100 ? 50. We generated the matrices or tensors as in the previous subsection for various\\nranks. We randomly selected some elements of the true matrix/tensor for training and kept the\\n7\\n\\n\\f1\\n\\n0.8\\n\\n0.8\\n\\n0.6\\n0.4\\nsize=[50 20]\\nsize=[100 40]\\nsize=[250 200]\\n\\n0.2\\n0\\n0\\n\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nNormalized rank ||n?1|| ||r||\\n1/2\\n\\nFraction at Error<=0.01\\n\\nFraction at err<=0.01\\n\\n1\\n\\n0.6\\n0.4\\n0.2\\n0\\n0\\n\\n0.6\\n\\n1/2\\n\\n(a) Matrix completion (K = 2).\\n\\nsize=[50 50 20]\\nsize=[100 100 50]\\n0.2\\n0.4\\n0.6\\nNormalized rank ||n?1||1/2||r||1/2\\n\\n0.8\\n\\n(b) Tensor completion (K = 3).\\n\\nFigure 3: Scaling behaviour of matrix/tensor completion with respect to the size n and the rank r.\\n\\nremaining elements for testing. No observation noise is added. We used the ADMM for ?as a\\nmatrix? and ?constraint? approaches described in [23] to solve the minimization problem (7) for\\nmatrix completion and tensor completion, respectively. Since there is no observation noise, we\\nchose the regularization constant ? ? 0. A single experiment for a speci?c size and rank can be\\nvisualized as in Figure 1.\\n?In\\n?? Figure ?3,\\n??? we plot the minimum fraction of observations m = M/N that achieved error\\n? ? W ??? smaller than 0.01 against the normalized rank r? = ?n?1 ?1/2 ?r?1/2 (of the true ten???W\\nF\\nsor) de?ned in Equation (13). The matrix case is plotted in Figure 3(a) and the tensor case is plotted\\nin Figure 3(b). Each series (blue crosses or red circles) corresponds to different matrix/tensor size\\nand each data-point corresponds to a different core size (rank). We can see that the fraction of observations m = M/N scales linearly against the normalized rank r?, which agrees with the condition\\nM/N ? c1 ?n?1 ?1/2 ?r?1/2 = c1 r? in Theorem 3 (see Equation (14)). The agreement is especially\\ngood for tensor completion (Figure 3(b)), where the two series almost overlap. Interestingly, we\\ncan see that when compared at the same normalized rank, tensor completion is easier than matrix\\ncompletion. For example, when nk = 50 and rk = 10 for each k = 1, . . . , K, the normalized rank\\nis 0.2. From Figure 3, we can see that we only need to see 30% of the entries in the tensor case to\\nachieve error smaller than 0.01, whereas we need about 60% of the entries in the matrix case.\\n\\n5\\n\\nConclusion\\n\\nWe have analyzed the statistical performance of a tensor decomposition algorithm based on the\\noverlapped Schatten 1-norm regularization (7). Numerical experiments show that our theory can\\npredict the empirical scaling behaviour well. The fraction of observation m = M/N at the threshold\\npredicted by our theory is proportional to the quantity we call the normalized rank, which re?nes\\nconjecture (sum of the mode-k ranks) in [23].\\nThere are numerous directions that the current study can be extended. In this paper, we have focused\\non the convergence of the estimation error; it would be meaningful to also analyze the condition for\\nthe consistency of the estimated rank as in [2]. Second, although we have succeeded in predicting\\nthe empirical scaling behaviour, the setting of random Gaussian design does not match the tensor\\ncompletion setting in Section 4.2. In order to analyze the latter setting, the notion of incoherence in\\n[5] or spikiness in [16] might be useful. This might also explain why tensor completion is easier than\\nmatrix completion at the same normalized rank. Moreover, when the target tensor is only low-rank\\nin a certain mode, Schatten 1-norm regularization fails badly (as predicted by the high normalized\\nrank). It would be desirable to analyze the ?Mixture? approach that aims at this case [23]. In\\na broader context, we believe that the current paper could serve as a basis for re-examining the\\nconcept of tensor rank and low-rank approximation of tensors based on convex optimization.\\nAcknowledgments. We would like to thank Franz Kir?aly and Hiroshi Kajino for their valuable\\ncomments and discussions. This work was supported in part by MEXT KAKENHI 22700138,\\n23240019, 23120004, 22700289, and NTT Communication Science Laboratories.\\n8\\n\\n\\fReferences\\n[1] E. Acar and B. Yener. Unsupervised multiway data analysis: A literature survey. IEEE T. Knowl. Data.\\nEn., 21(1):6?20, 2009.\\n[2] F.R. Bach. Consistency of trace norm minimization. J. Mach. Learn. Res., 9:1019?1048, 2008.\\n[3] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.\\n[4] R. Bro. PARAFAC. Tutorial and applications. Chemometr. Intell. Lab., 38(2):149?171, 1997.\\n[5] E. J. Candes and B. Recht. Exact matrix completion via convex optimization. Found. Comput. Math.,\\n9(6):717?772, 2009.\\n[6] J.D. Carroll and J.J. Chang. Analysis of individual differences in multidimensional scaling via an n-way\\ngeneralization of ?Eckart-Young? decomposition. Psychometrika, 35(3):283?319, 1970.\\n[7] P. Comon. Tensor decompositions. In J. G. McWhirter and I. K. Proudler, editors, Mathematics in signal\\nprocessing V. Oxford University Press, 2002.\\n[8] L. De Lathauwer and J. Vandewalle. Dimensionality reduction in higher-order signal processing and\\nrank-(r1 , r2 , . . . , rn ) reduction in multilinear algebra. Linear Algebra Appl., 391:31?55, 2004.\\n[9] K. Fukumizu. Generalization error of linear neural networks in unidenti?able cases. In Algorithmic\\nLearning Theory, pages 51?62. Springer, 1999.\\n[10] S. Gandy, B. Recht, and I. Yamada. Tensor completion and low-n-rank tensor recovery via convex optimization. Inverse Problems, 27:025010, 2011.\\n[11] J. H?astad. Tensor rank is NP-complete. Journal of Algorithms, 11(4):644?654, 1990.\\n[12] T. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM Review, 51(3):455?500,\\n2009.\\n[13] J. Liu, P. Musialski, P. Wonka, and J. Ye. Tensor completion for estimating missing values in visual data.\\nIn Prof. ICCV, 2009.\\n[14] M. M?rup. Applications of tensor (multiway array) factorizations and decompositions in data mining.\\nWiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 1(1):24?40, 2011.\\n[15] S. Negahban, P. Ravikumar, M. Wainwright, and B. Yu. A uni?ed framework for high-dimensional\\nanalysis of m-estimators with decomposable regularizers. In Y. Bengio, D. Schuurmans, J. Lafferty,\\nC. K. I. Williams, and A. Culotta, editors, Advances in NIPS 22, pages 1348?1356. 2009.\\n[16] S. Negahban and M.J. Wainwright. Restricted strong convexity and weighted matrix completion: Optimal\\nbounds with noise. Technical report, arXiv:1009.2118, 2010.\\n[17] S. Negahban and M.J. Wainwright. Estimation of (near) low-rank matrices with noise and highdimensional scaling. Ann. Statist., 39(2), 2011.\\n[18] B. Recht, M. Fazel, and P.A. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via\\nnuclear norm minimization. SIAM Review, 52(3):471?501, 2010.\\n[19] A. Rohde and A.B. Tsybakov.\\n39(2):887?930, 2011.\\n\\nEstimation of high-dimensional low-rank matrices.\\n\\nAnn. Statist.,\\n\\n[20] N.D. Sidiropoulos, R. Bro, and G.B. Giannakis. Parallel factor analysis in sensor array processing. IEEE\\nT. Signal Proces., 48(8):2377?2388, 2000.\\n[21] M. Signoretto, L. De Lathauwer, and J.A.K. Suykens. Nuclear norms for tensors and their use for convex\\nmultilinear estimation. Technical Report 10-186, ESAT-SISTA, K.U.Leuven, 2010.\\n[22] N. Srebro, J. D. M. Rennie, and T. S. Jaakkola. Maximum-margin matrix factorization. In Lawrence K.\\nSaul, Yair Weiss, and L?eon Bottou, editors, Advances in NIPS 17, pages 1329?1336. MIT Press, Cambridge, MA, 2005.\\n[23] R. Tomioka, K. Hayashi, and H. Kashima. Estimation of low-rank tensors via convex optimization.\\nTechnical report, arXiv:1010.0789, 2011.\\n[24] L. R. Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 31(3):279?311,\\n1966.\\n[25] M. Vasilescu and D. Terzopoulos. Multilinear analysis of image ensembles: Tensorfaces. Computer\\nVision?ECCV 2002, pages 447?460, 2002.\\n[26] H. Wang and N. Ahuja. Facial expression decomposition. In Proc. 9th ICCV, pages 958 ? 965, 2003.\\n\\n9\\n\\n\\f\",\n          \"Effects of Spatial and Temporal Contiguity on\\nthe Acquisition of Spatial Information\\n\\nThea B. Ghiselli-Crippa and Paul W. Munro\\nDepartment of Information Science and Telecommunications\\nUniversity of Pittsburgh\\nPittsburgh, PA 15260\\ntbgst@sis.pitt.edu, munro@sis.pitt.edu\\n\\nAbstract\\nSpatial information comes in two forms: direct spatial information (for\\nexample, retinal position) and indirect temporal contiguity information,\\nsince objects encountered sequentially are in general spatially close. The\\nacquisition of spatial information by a neural network is investigated\\nhere. Given a spatial layout of several objects, networks are trained on a\\nprediction task. Networks using temporal sequences with no direct spatial information are found to develop internal representations that show\\ndistances correlated with distances in the external layout. The influence\\nof spatial information is analyzed by providing direct spatial information\\nto the system during training that is either consistent with the layout or\\ninconsistent with it. This approach allows examination of the relative\\ncontributions of spatial and temporal contiguity.\\n\\n1 Introduction\\nSpatial information is acquired by a process of exploration that is fundamentally temporal, whether it be on a small scale, such as scanning a picture, or on a larger one, such as\\nphysically navigating through a building, a neighborhood, or a city. Continuous scanning\\nof an environment causes locations that are spatially close to have a tendency to occur in\\ntemporal proximity to one another. Thus, a temporal associative mechanism (such as a\\nHebb rule) can be used in conjunction with continuous exploration to capture the spatial\\nstructure of the environment [1]. However, the actual process of building a cognitive map\\nneed not rely solely on temporal associations, since some spatial information is encoded in\\nthe sensory array (position on the retina and proprioceptive feedback). Laboratory studies\\nshow different types of interaction between the relative contributions of temporal and spatial contiguities to the formation of an internal representation of space. While Clayton and\\nHabibi's [2] series of recognition priming experiments indicates that priming is controlled\\nonly by temporal associations, in the work of McNamara et al. [3] priming in recognition is observed only when space and time are both contiguous. In addition, Curiel and\\nRadvansky's [4] work shows that the effects of spatial and temporal contiguity depend on\\nwhether location or identity information is emphasized during learning. Moreover, other\\nexperiments ([3]) also show how the effects clearly depend on the task and can be quite\\ndifferent if an explicitly spatial task is used (e.g., additive effects in location judgments).\\n\\n\\fT. B. Ghiselli-Crippa and P W. Munro\\n\\n18\\n\\nlabels\\n\\nlabels\\n\\nlabels\\n(A coeff.)\\n\\nlabels\\n\\nlabels\\n\\ncoordinates\\n\\ncoordinates\\n(B coeff.)\\n\\nlabels\\n\\nFigure 1: Network architectures: temporal-only network (left); spatio-temporal network\\nwith spatial units part of the input representation (center); spatio-temporal network with\\nspatial units part of the output representation (right) .\\n\\n2 Network architectures\\nThe goal of the work presented in this paper is to study the structure of the internal representations that emerge from the integration of temporal and spatial associations. An\\nencoder-like network architecture is used (see Figure 1), with a set of N input units and a\\nset of N output units representing N nodes on a 2-dimensional graph. A set of H units is\\nused for the hidden layer. To include space in the learning process, additional spatial units\\nare included in the network architecture. These units provide a representation of the spatial\\ninformation directly available during the learning/scanning process. In the simulations described in this paper, two units are used and are chosen to represent the (x, y) coordinates of\\nthe nodes in the graph . The spatial units can be included as part of the input representation\\nor as part of the output representation (see Figure 1, center and right panels): both choices\\nare used in the experiments, to investigate whether the spatial information could better benefit training as an input or as an output [5]. In the second case, the relative contribution of\\nthe spatial information can be directly manipulated by introducing weighting factors in the\\ncost function being minimized. A two-term cost function is used, with a cross-entropy term\\nfor the N label units and a squared error term for the 2 coordinate units,\\n\\nri indicates the actual output of unit i and ti its desired output. The relative influence of\\n\\nthe spatial information is controlled by the coefficients A and B.\\n\\n3\\n\\nLearning tasks\\n\\nThe left panel of Figure 2 shows an example of the type of layout used; the effective\\nlayout used in the study consists of N = 28 nodes. For each node, a set of neighboring\\nnodes is defined, chosen on the basis of how an observer might scan the layout to learn the\\nnode labels and their (spatial) relationships; in Figure 2, the neighborhood relationships are\\nrepresented by lines connecting neighboring nodes. From any node in the layout, the only\\nallowed transitions are those to a neighbor, thus defining the set of node pairs used to train\\nthe network (66 pairs out of C(28, 2) = 378 possible pairs). In addition, the probability\\nof occurrence of a particular transition is computed as a function of the distance to the\\ncorresponding neighbor. It is then possible to generate a sequence of visits to the network\\nnodes, aimed at replicating the scanning process of a human observer studying the layout.\\n\\n\\f19\\n\\nSpatiotemporal Contiguity Effects on Spatial Information Acquisition\\n\\neraser\\n\\nknife\\n\\ncup\\n\\ncoin\\n\\neraser\\n\\nbutton\\n\\nFigure 2: Example of a layout (left) and its permuted version (right). Links represent\\nallowed transitions. A larger layout of 28 units was used in the simulations.\\n\\nThe basic learning task is similar to the grammar learning task of Servan-Schreiber et al.\\n[6] and to the neighborhood mapping task described in [1] and is used to associate each of\\nthe N nodes on the graph and its (x, y) coordinates with the probability distribution of the\\ntransitions to its neighboring nodes. The mapping can be learned directly, by associating\\neach node with the probability distribution of the transitions to all its neighbors: in this\\ncase, batch learning is used as the method of choice for learning the mapping. On the\\nother hand, the mapping can be learned indirectly, by associating each node with itself\\nand one of its neighbors, with online learning being the method of choice in this case;\\nthe neighbor chosen at each iteration is defined by the sequence of visits generated on\\nthe basis of the transition probabilities. Batch learning was chosen because it generally\\nconverges more smoothly and more quickly than online learning and gives qualitatively\\nsimilar results. While the task and network architecture described in [1] allowed only\\nfor temporal association learning, in this study both temporal and spatial associations are\\nlearned simultaneously, thanks to the presence of the spatial units. However, the temporalonly (T-only) case, which has no spatial units, is included in the simulations performed\\nfor this study, to provide a benchmark for the evaluation of the results obtained with the\\nspatio-temporal (S- T) networks.\\nThe task described above allows the network to learn neighborhood relationships for which\\nspatial and temporal associations provide consistent information, that is, nodes experienced\\ncontiguously in time (as defined by the sequence) are also contiguous in space (being spatial neighbors). To tease apart the relative contributions of space and time, the task is kept\\nthe same, but the data employed for training the network is modified: the same layout is\\nused to generate the temporal sequence, but the x , y coordinates of the nodes are randomly\\npermuted (see right panel of Figure 2). If the permuted layout is then scanned following the\\nsame sequence of node visits used in the original version, the net effect is that the temporal\\nassociations remain the same, but the spatial associations change so that temporally neighboring nodes can now be spatially close or distant: the spatial associations are no longer\\nconsistent with the temporal associations. As Figure 4 illustrates, the training pairs (filled\\ncircles) all correspond to short distances in the original layout, but can have a distance\\nanywhere in the allowable range in the permuted layout. Since the temporal and spatial\\ndistances were consistent in the original layout, the original spatial distance can be used\\nas an indicator of temporal distance and Figure 4 can be interpreted as a plot of temporal\\ndistance vs. spatial distance for the permuted layout.\\nThe simulations described in the following include three experimental conditions: temporal\\nonly (no direct spatial information available); space and time consistent (the spatial coordinates and the temporal sequence are from the same layout); space and time inconsistent\\n(the spatial coordinates and the temporal sequence are from different layouts).\\n\\n\\fT. B. Ghise/li-Crippa and P. W. Munro\\n\\n20\\n\\nHidden unit representations are compared using Euclidean distance (cosine and inner product measures give consistent results); the internal representation distances are also used to\\ncompute their correlation with Euclidean distances between nodes in the layout (original\\nand permuted). The correlations increase with the number of hidden units for values of\\nH between 5 and 10 and then gradually taper off for values greater than 10. The results\\npresented in the remainder of the paper all pertain to networks trained with H = 20 and\\nwith hidden units using a tanh transfer function; all the results pertaining to S-T networks\\nrefer to networks with 2 spatial output units and cost function coefficients A = 0.625 and\\nB = 6.25.\\n\\n4 Results\\nFigure 3 provides a combined view of the results from all three experiments. The left panel\\nillustrates the evolution of the correlation between internal representation distances and\\nlayout (original and permuted) distances. The right panel shows the distributions of the\\ncorrelations at the end of training (1000 epochs). The first general result is that, when spatial information is available and consistent with the temporal information (original layout),\\nthe correlation between hidden unit distances and layout distances is consistently better\\nthan the correlation obtained in the case of temporal associations alone. The second general result is that, when spatial information is available but not consistent with the temporal\\ninformation (permuted layout), the correlation between hidden unit distances and original\\nlayout distances (which represent temporal distances) is similar to that obtained in the case\\nof temporal associations alone, except for the initial transient. When the correlation is computed with respect to the permuted layout distances, its value peaks early during training\\nand then decreases rapidly, to reach an asymptotic value well below the other three cases.\\nThis behavior is illustrated in the box plots in the right panel of Figure 3, which report the\\ndistribution of correlation values at the end of training.\\n\\n4.1\\n\\nTemporal-only vs. spatio-temporal\\n\\nAs a first step in this study, the effects of adding spatial information to the basic temporal\\nassociations used to train the network can be examined. Since the learning task is the same\\nfor both the T-only and the S-T networks except for the absence or presence of spatial\\ninformation during training, the differences observed can be attributed to the additional\\nspatial information available to the S-T networks. The higher correlation between internal\\nrepresentation distances and original layout distances obtained when spatial information is\\n\\n0\\n\\n~\\n\\n.,\\n\\n8\\n\\n.,\\n\\nS and T CO\\\"Isistent\\n\\n0\\n\\n.\\n\\n0\\n\\n.\\n\\nT-o\\\"\\nSand T InCOnsistent\\n\\n0\\n\\ni:i\\n\\n-==~\\n\\n0\\n\\n(corr with T distance)\\n\\nii\\n\\n...\\n\\n?8 \\\"\\n\\n\\\"0\\n\\n0\\n\\n=s:\\n...........\\nE:2\\n\\nS and T Ir'ICOOSlStent\\n(corr. Wflh S distance)\\n\\n'\\\"ci\\n\\n~\\n\\n--'----'\\n\\nN\\n\\n0\\n\\n0\\n0\\n\\n0\\n0\\n\\n200\\n\\n400\\n600\\nOllnber 01 epochs\\n\\n800\\n\\n1000\\n\\nSandT\\n\\ncon_atent\\n\\nT-only\\n\\nSandT\\nSandT\\nInconsistent\\nineon.stant\\n(corr \\\" th T ast ) (corr wth 5 dst )\\n\\nFigure 3: Evolution of correlation during training (0 - 1000 epochs) (left). Distributions of\\ncorrelations at the end of training (1000 epochs) (right).\\n\\n\\fSpatiotemporal Contiguity Effects on Spatial Information Acquisition\\n\\n-\\n\\n21\\n\\nN\\n\\ndHU = 0.6 + 3.4d T + 0.3ds - 2.1( dT)2 + 0.4( d S )2 - 0.4d T ds\\n\\n0\\n\\n.,\\n\\n25\\n\\n0\\n\\n\\\",\\nE\\n~\\n\\n'\\\"\\n0\\n\\n15\\n\\n...\\n0\\n\\n05\\nN\\n\\n0\\n\\n15\\n0\\n0\\n\\n00\\n\\n02\\n\\n04\\n\\n08\\n\\n10\\n\\n12\\n\\nFigure 4: Distances in the original layout\\n(x) vs_ distances in the permuted layout\\n(y)_ The 66 training pairs are identified by\\nfilled circles_\\n\\n\\\"\\nFigure 5: Similarities (Euclidean distances)\\nbetween internal representations developed\\nby a S-T network (after 300 epochs)_ Figure\\n4 projects the data points onto the x, y plane_\\n\\navailable (see Figure 3) is apparent also when the evolution of the internal representations\\nis examined_ As Figure 6 illustrates, the presence of spatial information results in better\\ngeneralization for the pattern pairs outside the training set While the distances between\\ntraining pairs are mapped to similar distances in hidden unit space for both the T-only and\\nthe S-T networks, the T-only network tends to cluster the non-training pairs into a narrow\\nband of distances in hidden unit space. In the case of the S-T network instead, the hidden\\nunit distances between non-training pairs are spread out over a wider range and tend to\\nreflect the original layout distances.\\n4.2\\n\\nPermuted layout\\n\\nAs described above, with the permuted layout it is possible to decouple the spatial and\\ntemporal contributions and therefore study the effects of each. A comprehensive view of\\nthe results at a particular point during training (300 epochs) is presented in Figure 5, where\\nthe x, y plane represents temporal distance vs. spatial distance (see also Figure 4) and the z\\naxis represents the similarity between hidden unit representations. The figure also includes\\na quadratic regression surface fitted to the data points. The coefficients in the equation of\\nthe surface provide a quantitative measure of the relative contributions of spatial (ds) and\\ntemporal distances (dT ) to the similarity between hidden unit representations (d HU ):\\n(2)\\n\\nIn general, after the transient observed in early training (see Figure 3), the largest and most\\nsignificant coefficients are found for dT and (dT?, indicating a stronger dependence of\\ndHU on temporal distance than on spatial distance.\\nThe results illustrated in Figure 5 represent the situation at a particular point during training\\n(300 epochs). Similar plots can be generated for different points during training, to study\\nthe evolution of the internal representations. A different view of the evolution process is\\nprovided by Figure 7, in which the data points are projected onto the x,Z plane (top panel)\\nand the y,z plane (bottom panel) at four different times during training. In the top panel,\\n\\n14\\n\\n\\fT. B. Ghiselli-Crippa and P W Munro\\n\\n22\\n\\n,.. ,..\\n.. roo\\n:::\\n\\n~\\n_\\n\\n0\\n\\n?\\n\\nN\\n\\n~ ~\\n~ ~\\n\\n~\\n\\n-. .. -\\n\\n:::\\n~\\n\\n~\\n\\n02\\n\\n\\\"\\n\\n06\\n\\nO.\\n\\n\\\"_d\\n\\n\\\"\\n\\n12\\n\\n.\\n\\n,,\\n\\n~'\\n\\n~ :\\n~\\n\\n~\\n~,\\n\\n02\\n\\nos\\n\\n\\\"-'\\n\\n..\\n\\n'\\n\\n02\\n\\n.. .\\n06\\n\\n-\\n\\n,\\n\\ntP\\n\\n.\\n\\nDO\\n\\n0\\n\\n, ,\\n.I'\\n\\n~\\n\\n12\\n\\n.',\\n\\n00\\n\\n02\\n\\n\\\" \\\"-'\\n06\\n\\n.\\\"\\n\\n02\\n\\n~\\n\\n~\\n\\n.~.\\n','\\n\\n~\\n\\n.. .. \\\" \\\"\\n\\n~\\n\\n06\\n\\n00\\n\\n02\\n\\n\\\"_d\\n\\n_\\n\\n0\\n\\n?\\n\\nN\\n\\n~\\n\\n:\\n\\n~,\\n\\n~ ~\\n\\n..\\n\\nf/Po\\n\\n<P\\n\\n\\\"\\n\\ne,\\n\\n.\\n\\nDO\\n\\n.:.\\n\\n~\\n\\n00\\n\\n02\\n\\n\\\"\\n\\n.. ..\\n\\n\\\"-'\\n\\n10\\n\\n12\\n\\n.. .. .. \\\"\\n\\n12\\n\\n\\\" _d\\n\\n:::\\n\\n~\\n,\\n\\n~\\n12\\n\\n0\\nN\\n\\n~ ~\\n\\n',~-,\\n\\n00\\n\\n~\\n\\n~\\n\\n~\\n_\\n?\\n\\n~\\n\\n~\\n\\n12\\n\\n\\\"\\n\\n0\\n\\n00\\n\\n~\\n\\n:::\\n\\ng\\n10\\n\\n~\\n\\n~\\n\\n~ ~\\n\\nO.\\n\\n,\\n\\n:::\\n\\n;; ~\\n\\n.~\\n00\\n\\n00\\n\\n~.\\n\\n~\\n\\n,\\n\\n\\\"_d\\n\\n,\\n\\n:; ~\\n\\n~\\n\\n~\\n\\n~\\n\\n00\\n\\n~\\n\\n~\\n\\n~\\n\\n~\\n\\ni\\n\\n~\\n\\n~\\n\\n~,\\n\\n:::\\n\\n~\\n\\n~\\n\\n~\\n_\\n\\n0\\n\\n?\\n\\nN\\n\\n~\\n\\n~\\n\\n~\\n\\n~\\n\\no\\n\\n,.~,o\\n\\n: s\\n\\nrIP 0\\n\\n00\\n\\n0\\n\\n?\\n\\n','\\n\\n00\\n\\n02\\n\\n\\\"\\n\\nO.\\n\\n\\\"-'\\n\\no.\\n\\n\\\"\\n\\n12\\n\\nFigure 6: Internal representation distances vs. original layout distances: S-T network (top)\\nvs. T-only network (bottom). The training pairs are identified by filled circles. The presence\\nof spatial information results in better generalization for the pairs outside the training set.\\nthe internal representation distances are plotted as a function of temporal distance (i.e., the\\nspatial distance from the original layout), while in the bottom panel they are plotted as a\\nfunction of spatial distance (from the permuted layout). The higher asymptotic correlation\\nbetween internal representation distances and temporal distances, as opposed to spatial\\ndistances (see Figure 3), is apparent also from the examination of the evolutionary plots,\\nwhich show an asymptotic behavior with respect to temporal distances (see Figure 7, top\\npanel) very similar to the T-only case (see Figure 6, bottom panel).\\n\\n5 Discussion\\nThe first general conclusion that can be drawn from the examination of the results described\\nin the previous section is that, when the spatial information is available and consistent with\\nthe temporal information (original layout), the similarity structure of the hidden unit representations is closer to the structure of the original layout than that obtained by using\\ntemporal associations alone. The second general conclusion is that, when the spatial information is available but not consistent with the temporal information (permuted layout),\\nthe similarity structure of the hidden unit representations seems to correspond to temporal\\nmore than spatial proximity. Figures 5 and 7 both indicate that temporal associations take\\nprecedence over spatial associations. This result is in agreement with the results described\\nin [1], showing how temporal associations (plus some high-level constraints) significantly\\ncontribute to the internal representation of global spatial information. However, spatial information certainly is very beneficial to the (temporal) acquisition of a layout, as proven by\\nthe results obtained with the S-T network vs. the T-only network.\\nIn terms of the model presented in this paper, the results illustrated in Figures 5 and 7 can\\nbe compared with the experimental data reported for recognition priming ([2], [3], [4]),\\nwith distance between internal representations corresponding to reaction time. The results\\nof our model indicate that distances in both the spatially far and spatially close condition\\nappear to be consistently shorter for the training pairs (temporally close) than for the nontraining pairs (temporally distant), highlighting a strong temporal effect consistent with the\\ndata reported in [2] and [4] (for spatially far pairs) and in [3] (only for the spatially close\\n\\n\\fSpatiotemporal Contiguity Effects on Spatial Information Acquisition\\n\\n~~\\n\\n0_\\n\\nri\\n\\n; -~-'\\n~. ~~.. .\\nSl\\n...........\\n0\\n\\n\\\" ...... .\\nj!I!A\\n\\n..\\n,.\\n\\n0\\n\\n,.\\n\\n~\\n\\n~\\n\\n~\\n\\n23\\n\\n\\\\\\n\\n0\\n\\n?\\n\\nlfIiiIo\\n\\n'0'\\n\\n110\\n\\n0\\n\\n~'--_ _ _ _ _-.J\\n\\n00\\n\\n02\\n\\nO.\\n\\n01\\n\\n01\\n\\n10\\n\\n12\\n\\n00\\n\\n02\\n\\nO.\\n\\n~\\n\\n01\\n\\n01\\n\\n10\\n\\n12\\n\\n00\\n\\n02\\n\\n0.4\\n\\n01\\n\\n01\\n\\n10\\n\\n12\\n\\n02\\n\\n0\\\"\\n\\n01\\nIn_d (S)\\n\\n01\\n\\n'0\\n\\n12\\n\\n~l.-\\n\\n00\\n\\n0.2\\n\\no.\\n\\not\\n.._d(S)\\n\\n02\\n\\nO.\\n\\n01\\n\\n10\\n\\n12\\n\\n0.0\\n\\n02\\n\\n04\\n\\n08\\n...u:I (S)\\n\\n01\\n\\noa\\n\\n10\\n\\n12\\n\\nIn_den\\n\\nL..-_ _ _ _- . l\\n00\\n\\n0 0\\n\\nl'I_d (T)\\n\\nIn_d(TI\\n\\nIn _d (T}\\n\\nall\\n\\n10\\n\\n12\\n\\n00\\n\\n_ _ _ _ _-.J\\n02\\n\\nO.\\n\\n06\\n\\noa\\n\\n10\\n\\n12\\n\\n!rUi (S)\\n\\nFigure 7: Internal representation distances vs. temporal distances (top) and vs. spatial\\ndistances (bottom) for a S-T network (permuted layout). The training pairs are identified\\nby filled circles. The asymptotic behavior with respect to temporal distances (top panel) is\\nsimilar to the T-only condition. The bottom panel indicates a weak dependence on spatial\\ndistances.\\ncase). For the training pairs (temporally close), slightly shorter distances are obtained for\\nspatially close pairs vs. spatially far pairs; this result does not provide support for the\\nexperimental data reported in either [3] (strong spatial effect) or [2] (no spatial effect).\\nFor the non-training pairs (temporally distant), long distances are found throughout, with\\nno strong dependence on spatial distance; this effect is consistent with all the reported\\nexperimental data. Further simulations and statistical analyses are necessary for a more\\nconclusive comparison with the experimental data.\\nReferences\\n[1] Ghiselli-Crippa, TB. & Munro, P.w. (1994). Emergence of global structure from local associations. In J.D. Cowan, G. Tesauro, & J. Alspector (Eds.), Advances in Neural Information Processing\\nSystems 6, pp. 1101-1108. San Francisco, CA: Morgan Kaufmann.\\n[2] Clayton, K.N. & Habibi, A. (1991). The contribution of temporal contiguity to the spatial priming\\neffect. Journal of Experimental Psychology: Learning. Memory. and Cognition 17:263-271.\\n[3] McNamara, TP., Halpin. J.A. & Hardy, J.K. (1992). Spatial and temporal contributions to the\\nstructure of spatial memory. Journal of Experimental Psychology: Learning. Memory. and Cognition\\n18:555-564.\\n[4] Curiel, J.M. & Radvansky, G.A. (1998). Mental organization of maps. Journal of Experimental\\nPsychology: Learning. Memory. and Cognition 24:202-214.\\n[5] Caruana, R. & de Sa, VR. (1997). Promoting poor features to supervisors: Some inputs work\\nbetter as outputs . In M.e. Mozer, M.I. Jordan, & T Petsche (Eds.), Advances in Neural Information\\nProcessing Systems 9, pp. 389-395. Cambridge, MA: MIT Press.\\n[6] Servan-Schreiber, D., Cleeremans, A. & McClelland, J.L. (1989). Learning sequential structure\\nin simple recurrent networks. In D.S. Touretzky (Ed.), Advances in Neural Information Processing\\nSystems 1, pp. 643-652. San Mateo, CA: Morgan Kaufmann.\\n\\n\\fNeural Representation of Multi-Dimensional\\nStimuli\\n\\nChristian W. Eurich, Stefan D. Wilke and Helmut Schwegler\\nInstitut fUr Theoretische Physik\\nUniversitat Bremen, Germany\\n(eurich,swilke,schwegler)@physik.uni-bremen.de\\n\\nAbstract\\nThe encoding accuracy of a population of stochastically spiking neurons\\nis studied for different distributions of their tuning widths. The situation\\nof identical radially symmetric receptive fields for all neurons, which\\nis usually considered in the literature, turns out to be disadvantageous\\nfrom an information-theoretic point of view. Both a variability of tuning widths and a fragmentation of the neural population into specialized\\nsubpopulations improve the encoding accuracy.\\n\\n1 Introduction\\nThe topic of neuronal tuning properties and their functional significance has focused much\\nattention in the last decades. However, neither empirical findings nor theoretical considerations have yielded a unified picture of optimal neural encoding strategies given a sensory\\nor motor task. More specifically, the question as to whether narrow tuning or broad tuning\\nis advantageous for the representation of a set of stimulus features is still being discussed.\\nEmpirically, both situations are encountered: small receptive fields whose diameter is less\\nthan one degree can, for example, be found in the human retina [7] , and large receptive\\nfields up to 180 0 in diameter occur in the visual system of tongue-projecting salamanders\\n[10]. On the theoretical side, arguments have been put forward for small [8] as well as for\\nlarge [5, 1,9, 3, 13] receptive fields.\\nIn the last years, several approaches have been made to calculate the encoding accuracy\\nof a neural population as a function of receptive field size [5, 1,9,3, 13]. It has turned\\nout that for a firing rate coding, large receptive fields are advantageous provided that D 2:\\n3 stimulus features are encoded [9, 13]. For binary neurons, large receptive fields are\\nadvantageous also for D = 2 [5,3].\\nHowever, so far only radially symmetric tuning curves have been considered. For neural\\npopulations which lack this symmetry, the situation may be very different. Here we study\\nthe encoding accuracy of a popUlation of stochastically spiking neurons. A Fisher information analysis performed on different distributions of tunings widths will indeed reveal a\\nmuch more detailed picture of neural encoding strategies.\\n\\n\\fC. W. Eurich. S. D. Wilke and H. Schwegler\\n\\nJ J6\\n\\n2 Model\\nConsider a D-dimensional stimulus space, X. A stimulus is characterized by a position\\nx\\n(Xl, ... , XD) E X, where the value of feature i, Xi (i\\n1, ... , D), is measured\\nrelative to the total range of values in the i-th dimension such that it is dimensionless.\\nInformation about the stimulus is encoded by a popUlation of N stochastically spiking\\nneurons. They are assumed to have independent spike generation mechanisms such that the\\njoint probability distribution for observing n = (n(l), ... ,n(k), ... ,n(N?) spikes within a\\ntime interval T, Ps(n; x), can be written in the form\\n\\n=\\n\\n=\\n\\nN\\n\\nPs(n;x) =\\n\\nII\\n\\nps(k) (n(k);\\n\\nx),\\n\\n(1)\\n\\nk=l\\nwhere Ps(k) (n(k); x) is the single-neuron probability distribution of the number of observed\\nspikes given the stimulus at position x. Note that (1) does not exclude a correlation of the\\nneural firing rates, i.e., the neurons may have common input or even share the same tuning\\nfunction.\\nThe firing rates depend on the stimulus via the local values of the tuning functions, such that\\nx) can be written in the form Ps(k) (n(k); x) = S (n(k), j(k) (x), T), where the\\ntuning function of neuron k, j(k) (x), gives its mean firing rate in response to the stimulus\\nat position x. We assume here a form of the tuning function that is not necessarily radially\\nsymmetric,\\nPs(k) (n(k);\\n\\nf(') (x)\\n\\n= F4>\\n\\n(t\\n\\n(Xi\\n\\n~~r) )2) =, F? ( e( ')2) ,\\n\\n(2)\\n\\nwhere e(k) = (c~k), ... , c};?) is the center of the tuning curve of neuron k, O'~k) is its\\ntuning width in the i-th dimension, k )2 := (Xi - c~k?)2/O'ik)2 for i = 1, ... ,D, and\\n~(k)2 := ~~k)2 + ... + ~~)2. F > 0 denotes the maximal firing rate of the neurons, which\\nrequires that maxz~o fj>(z) = 1.\\n\\nd\\n\\nWe assume that the tuning widths O't), . .. ,O'~) of each neuron k are drawn from a distribution PO' (0'1, ... ,O'D). For a population oftuning functions with centers e(l), ... , e(N), a\\ndensity 1}(x) is introduced according to 1}(x) := L:~=l 8(x - e(k?).\\nThe encoding accuracy can be quantified by the Fisher information matrix, J, which is\\ndefined as\\n(3)\\n\\nwhere E[ . ..J denotes the expectation value over the probability distribution P(n; x) [2].\\nThe Fisher information yields a lower bound on the expected error of an unbiased estimator\\nthat retrieves the stimulus x from the noisy neural activity (Cramer-Rao inequality) [2]. The\\nminimal estimation error for the i-th feature Xi, ti,min, is given by t;,min = (J - 1 )ii which\\nreduces to t;,min = 1/ Jii(X) if J is diagonal.\\nWe shall now derive a general expression for the popUlation Fisher information. In the\\nnext chapter, several cases and their consequences for neural encoding strategies will be\\ndiscussed.\\nFor model neuron (k), the Fisher information (3) reduces to\\n(k)\\n\\nJ ij\\n\\n.\\n\\n(k)\\n\\n(X'O'I\\n\\n(k) _\\n\\\"\\\"'O'D) -\\n\\n1\\n\\n(k)\\nO'i\\n\\n(k)Aq..\\nO'j\\n\\n(\\n\\n~\\n\\n(k)2\\n\\n,F,T\\n\\n)\\n\\n(k) (k)\\n\\n~i ~j\\n\\n,\\n\\n(4)\\n\\n\\f117\\n\\nNeural Representation of Multi-Dimensional Stimuli\\n\\nwhere the dependence on the tuning widths is indicated by the list of arguments. The\\nfunction A.p depends on the shape of the tuning function and is given in [13]. The independence assumption (1) implies that the population Fisher information is the sum of\\n. d??d\\nI\\n\\\",N J(k)(\\n(k)\\n(k)) . U7\\nt he contn?b?\\nutlOns 0 f the III\\nIVI ua neurons, L.Jk=1 ij x; 0\\\"1 , ... ,0\\\"D\\nne now define\\na population Fisher information which is averaged over the distribution of tuning widths\\nPt:T(0\\\"1, . .. ,O\\\"D):\\nN\\n\\n(Jij (x)) 17 =\\n\\nL / d0\\\"1 . .. dO\\\"D Pt:T(0\\\"1,? .. , O\\\"D) Ji~k) (x; 0\\\"1, ? .. , O\\\"D) .\\n\\n(5)\\n\\nk= 1\\n\\nIntroducing the density of tuning curves, 1J(x), into (5) and assuming a constant distribution, 1J(x) == 1J == const., one obtains the result that the population Fisher information\\nbecomes independentofx and that the off-diagonal elements of J vanish [13]. The average\\npopulation Fisher information then becomes\\n(Jij)t:T =\\n\\n1JD K.p (F, r, D ) \\\\/\\n\\nflt:l\\n0\\\"1) ~\\n0\\\";\\nVij,\\n17\\n\\n(6)\\n\\nwhere K.p depends on the geometry of the tuning curves and is defined in [13].\\n\\n3 Results\\nIn this section, we consider different distributions of tuning widths in (6) and discuss advantageous and disadvantageous strategies for obtaining a high representational accuracy\\nin the neural population.\\nRadially symmetric tuning curves.\\nthe tuning-width distribution reads\\n\\nFor radially symmetric tuning curves of width a,\\nD\\n\\nPt:T(O\\\"l, .. . ,O\\\"D)\\n\\n= II O(O\\\"i -a);\\ni=l\\n\\nsee Fig. 1a for a schematic visualization of the arrangement of the tuning widths for the\\ncase D = 2. The average population Fisher information (6) for i = j becomes\\n(Jii)t:T =\\n\\n1JDK.p(F, r, D) aD -\\n\\n2,\\n\\n(7)\\n\\na result already obtained by Zhang and Sejnowski [13]. Equation (7) basically shows that\\nthe minimal estimation error increases with a for D = 1, that it does not depend on a for\\nD = 2, and that it decreases as a increases for D 2: 3. We shall discuss the relevance of\\nthis case below.\\nIdentical tuning curves without radial symmetry. Next we discuss tuning curves which\\nare identical but not radially symmetric; the tuning-width distribution for this case is\\nD\\n\\nPt:T(0\\\"1, . .. ,O\\\"D)\\n\\n=\\n\\nII\\n\\nO(O\\\"i -\\n\\nad,\\n\\ni=l\\n\\nwhere ai denotes the fixed width in dimension i. For i = j, the average population Fisher\\ninformation (6) reduces to [11,4]\\n(Jii)t:T = 1JDK.p ( F,\\n\\nr,\\n\\nD)\\n\\nDfl 1=1\\n0\\\"1\\n-2\\n\\nO\\\"i\\n\\n.\\n\\n(8)\\n\\n\\fc.\\n\\n118\\n\\n(a)\\n\\nW. Eurich, S. D. Wilke and H. Schwegler\\n\\n(b)\\n\\n/\\n\\nFigure 1: Visualization of different distributions of\\ntuning widths for D = 2. (a) Radially symmetric tuning curves. The dot indicates a fixed (j, while the diagonalline symbolizes a variation in (j discussed in [13].\\n(b) Identical tuning curves which are not radially symmetric. (c) Tuning widths uniformly distributed within\\na small rectangle. (d) Two sUbpopulations each of\\nwhich is narrowly tuned in one dimension and broadly\\ntuned in the other direction.\\n\\n.\\n\\n(c)\\n\\n(d)\\n\\n.\\n\\nb _ b\\n2\\n\\n.\\n\\nEquation (8) contains (7) as a special case. From (8) it becomes immediately clear that the\\nexpected minimal square encoding error for the i-th stimulus feature, ?~ min = 1/ (Jii(X))u,\\ndepends on i, i. e., the population specializes in certain features. The error obtained in\\ndimension i thereby depends on the tuning widths in all dimensions.\\nWhich encoding strategy is optimal for a population whose task it is to encode a single\\nfeature, say feature i, with high accuracy while not caring about the other dimensions? In\\norder to answer this question, we re-write (8) in terms of receptive field overlap.\\nFor the tuning functions f(k) (x) encountered empirically, large values ofthe single-neuron\\nFisher information (4) are typically restricted to a region around the center of the tuning\\nfunction, c(k). The fraction p({3) of the Fisher information that falls into a region ED\\nJ~(k)2 ~ (3 aroundc(k) is given by\\n\\nf\\np({3)\\n\\n:=\\n\\nd\\n\\nE; d\\nX\\n\\nD\\n\\nD\\n\\n\\\"\\\",D\\nX L....i=l\\n\\nX\\n\\n(k) ( )\\nJ ii X\\n\\n2:~t=l J~~)\\n( )\\nu\\nX\\n\\nj3\\n\\nf\\n\\nd~ ~D+l At/>(e, F, T)\\n\\no\\n\\n(9)\\n\\n00\\n\\nf\\n\\nd~ ~D+l At/>(~2, F, T)\\n\\no\\n\\nwhere the index (k) was dropped because the tuning curves are assumed to have identical shapes. Equation (9) allows the definition of an effective receptive field, RF~~,\\ninside of which neuron k conveys a major fraction Po of Fisher information, RF~~ :=\\n\\n{xl~ ~ {3o} , where (3o is chosen such that p({3o)\\n\\n= Po. The Fisher information a\\n\\nneuron k carries is small unless x E RF~~. This has the consequence that a fixed stimulus\\nx is actually encoded only by a subpopulation of neurons. The point x in stimulus space is\\ncovered by\\n27r D/ 2({30)D D _\\n(10)\\nNcode:= 1] Dr(D/2)\\n(Jj\\n\\n}1\\n\\nreceptive fields. With the help of (10), the average population Fisher information (8) can\\nbe re-written as\\n(11)\\n\\nEquation (11) can be interpreted as follows: We assume that the population of neurons\\nencodes stimulus dimension i accurately, while all other dimensions are of secondary importance. The average population Fisher information for dimension i, (Jii ) u, is determined\\nby the tuning width in dimension i, (ji, and by the size of the active subpopulation, N code '\\nThere is a tradeoff between these quantities. On the one hand, the encoding error can be\\ndecreased by decreasing (ji, which enhances the Fisher information carried by each single\\n\\n\\fNeural Representation ofMulti-Dimensional Stimuli\\n\\n119\\n\\nneuron. Decreasing ai, on the other hand, will also shrink the active subpopulation via\\n(10). This impairs the encoding accuracy, because the stimulus position is evaluated from\\nthe activity of fewer neurons. If (11) is valid due to a sufficient receptive field overlap,\\nNcode can be increased by increasing the tuning widths, aj, in all other dimensions j i- i.\\nThis effect is illustrated in Fig. 2 for D = 2.\\n\\nX2\\nc=:>\\n\\nx2, s\\n\\nX2\\n,II\\\"\\\\..\\\\\\n\\nU\\nx2,s\\n\\nFigure 2: Encoding strategy for a stimulus characterized by parameters Xl,s and X2,s' Feature Xl is to be encoded accurately. Effective receptive field shapes are indicated for both\\npopulations. If neurons are narrowly tuned in X2 (left), the active population (solid) is\\nsmall (here: Ncode = 3). Broadly tuned receptive fields for X2 (right) yield a much larger\\npopulation (here: Ncode = 27) thus increasing the encoding accuracy.\\nIt shall be noted that although a narrow tuning width ai is advantageous, the limit ai ---t 0\\nyields a bad representation. For narrowly tuned cells, gaps appear between the receptive\\nfields: The condition 17(X) == const. breaks down, and (6) is no longer valid. A more\\ndetailed calculation shows that the encoding error diverges as ai --* 0 [4]. The fact that\\nthe encoding error decreases for both narrow tuning and broad tuning - due to (11) - proves\\nthe existence of an optimal tuning width, An example is given in Fig. 3a.\\n3\\n\\nrTI~--~------~----~------~\\n\\n1\\\\\\n\\n(b)\\n\\nIi\\n\\n1\\\\\\n\\nIi\\n\\n0.8\\n\\nII\\nII\\nI;\\n\\n2\\n\\n1\\\\\\n\\nI ,\\n\\n;to.6\\n~\\n\\n~~~~;::~-:.~~;:\\n\\nA\\n\\nN~O.4\\nw\\n\\n----- ---- ----- -- ---\\n\\nv\\n\\n0.2\\n\\nO'----~--~--~-----'-------'\\n\\no\\n\\n0.5\\n\\n1\\nA\\n\\n1.5\\n\\n2\\n\\nFigure 3: (a) Example for the encoding behavior with narrow tuning curves arranged on\\na regular lattice of dimension D = 1 (grid spacing ~). Tuning curves are Gaussian, and\\nneural firing is modeled as a Poisson process, Dots indicate the minimal square encoding\\nerror averaged over a uniform distribution of stimuli, (E~in)' as a function ofa. The minimum is clearly visible. The dotted line shows the corresponding approximation according\\nto (8). The inset shows Gaussian tuning curves of optimal width, ao pt ~ 0.4~. (b) 9D()..)\\nas a function of ).. for different values of D.\\n\\n\\fc. W.\\n\\n120\\n\\nEurich, S. D. Wilke and H. Schwegler\\n\\nNarrow distribution of tuning curves. In order to study the effects of encoding the\\nstimulus with distributed tuning widths instead of identical tuning widths as in the previous\\ncases, we now consider the distribution\\n\\ng:i e\\nD\\n\\nPu(lT1,'\\\" ,lTD)\\n\\n=\\n\\n[lTi - (O'i -\\n\\ni)] e [(O'i + i) -lTi] ,\\n\\n(12)\\n\\ne\\n\\ndenotes the Heaviside step function. Equation (12) describes a uniform distriwhere\\nbution in a D-dimensional cuboid of size b1 , ... , bD around (0'1, .. . 0'D); cf. Fig. 1c. A\\nstraightforward calculation shows that in this case, the average population Fisher information (6) for i = j becomes\\n\\n(Jii)u\\n\\n= f/DKtj) (F, T, D) n~l\\nO'~ 0'1\\n\\n{\\n\\n1\\n1 + 12\\n\\n(bO'i 2+ 0 [( O'ib 4] }.\\ni )\\n\\ni )\\n\\n(13)\\n\\nA comparison with (8) yields the astonishing result that an increase in bi results in an\\nincrease in the i-th diagonal element of the average population Fisher information matrix\\nand thus in an improvement in the encoding of the i-th stimulus feature, while the encoding\\nin dimensions j :f. i is not affected. Correspondingly, the total encoding error can be\\ndecreased by increasing an arbitrary number of edge lengths of the cube. The encoding by\\na population with a variability in the tuning curve geometries as described is more precise\\nthan that by a uniform population. This is true/or arbitrary D. Zhang and Sejnowski [13]\\nconsider the more artificial situation of a correlated variability ofthe tuning widths: tuning\\ncurves are always assumed to be radially symmetric. This is indicated by the diagonal\\nline in Fig. 1a. A distribution of tuning widths restricted to this subset yields an average\\npopulation Fisher information ex: (O'D-2) and does not improve the encoding for D = 2 or\\n\\nD=3.\\nFragmentation into D subpopulations. Finally, we study a family of distributions of\\ntuning widths which also yields a lower minimal encoding error than the uniform population. Let the density of tuning curves be given by\\n1 D\\n\\nPu(lT1,'\\\" ,lTD) = D\\n\\nL 6( lTi i=l\\n\\nAO')\\n\\nII 6(lTj - 0'),\\n\\n(14)\\n\\nj?-i\\n\\nwhere A > O. For A = 1, the population is uniform as in (7). For A :f. 1, the population\\nis split up into D subpopulations; in subpopulation i, lTi is modified while lTj == 0' for\\nj :f. i. See Fig. Id for an example. The diagonal elements ofthe average population Fisher\\ninformation are\\n\\n(Jii)u\\n\\n{1 + (D = f/DKtj)(F, T, D) -D-2\\nIT\\nDA\\n\\nI)A 2 }\\n\\n'\\n\\n(15)\\n\\nwhere the term in brackets will be abbreviated as 9D(A). (Jii)u does not depend on i in\\nthis case because of the symmetry in the sUbpopulations. Equation (15) and the uniform\\ncase (7) differ by 9D(A) which will now be discussed. Figure 3b shows 9D(A) for different\\nvalues of D. For A = 1, 9D(A) = 1 and (7) is recovered as expected. 9D(A) = 1\\nalso holds for A = 1/ (D - 1) < 1: narrowing one tuning width in each subpopulation\\nwill at first decrease the resolution provided D 2: 3; this is due to the fact that Ncode is\\ndecreased. For A < 1/(D - 1), however, 9D(A) > 1, and the resolution exceeds (Jii)u in\\n(7) because each neuron in the i-th subpopulation carries a high Fisher information in the\\ni-th dimension. D = 2 is a special case where no impairment of encoding occurs because\\nthe effect of a decrease of Ncode is less pronounced. Interestingly, an increase in A also\\nyields an improvement in the encoding accuracy. This is a combined effect resulting from\\nan increase in Ncode on the one hand and the existence of D subpopulations, D - 1 of\\n\\n\\fNeural Representation of Multi-Dimensional Stimuli\\n\\n121\\n\\nwhich maintain their tuning widths in each dimension on the other hand. The discussion\\nof 9D(>\\\") leads to the following encoding strategy. For small >.., (Jii)u increases rapidly,\\nwhich suggests a fragmentation of the population into D subpopulations each of which\\nencodes one feature with high accuracy, i.e., one tuning width in each subpopulation is\\nsmall whereas the remaining tuning widths are broad. Like in the case discussed above, the\\ntheoretical limit of this method is a breakdown of the approximation of TJ == const. and the\\nvalidity of (6) due to insufficient receptive field overlap.\\n\\n4 Discussion and Outlook\\nWe have discussed the effects of a variation of the tuning widths on the encoding accuracy\\nobtained by a population of stochastically spiking neurons. The question of an optimal\\ntuning strategy has turned out to be more complicated than previously assumed. More\\nspecifically, the case which focused most attention in the literature - radially symmetric\\nreceptive fields [5, 1,9, 3, 13] - yields a worse encoding accuracy than most other cases we\\nhave studied: uniform populations with tuning curves which are not radially symmetric;\\ndistributions of tuning curves around some symmetric or non-symmetric tuning curve; and\\nthe fragmentation of the population into D subpopulations each of which is specialized in\\none stimulus feature.\\nIn a next step, the theoretical results will be compared to empirical data on encoding properties of neural popUlations. One aspect is the existence of sensory maps which consist\\nof neural subpopulations with characteristic tuning properties for the features which are\\nrepresented. For example, receptive fields of auditory neurons in the midbrain of the barn\\nowl have elongated shapes [6]. A second aspect concerns the short-term dynamics of receptive fields. Using single-unit recordings in anaesthetized cats, Worgotter et al. [12]\\nobserved changes in receptive field size taking place in 50-lOOms. Our findings suggest\\nthat these dynamics alter the resolution obtained for the corresponding stimulus features.\\nThe observed effect may therefore realize a mechanism of an adaptable selective signal\\nprocessing.\\n\\nReferences\\n[1] Baldi, P. & HeiJigenberg, W. (1988) BioI. Cybern. 59:313-318.\\n[2] Deco, G. & Obradovic, D. (1997) An Information-Theoretic Approach to Neural Computing.\\nNew York: Springer.\\n[3] Eurich, C. W. & Schwegler, H. (1997) BioI. Cybern. 76: 357-363.\\n\\n[4] Eurich, C. W. & Wilke, S. D. (2000) NeuraL Compo (in press).\\n[5] Hinton, G. E., McClelland, J. L. & Rumelhart, D. E (1986) In Rumelhart, D. E. & McClelland,\\nJ. L. (eds.), ParaLLeL Distributed Processing, Vol. 1, pp. 77-109. Cambridge MA: MIT Press.\\n[6] Knudsen, E. I. & Konishi, M. (1978) Science 200:795-797.\\n[7] Kuffter, S. W. (1953) 1. Neurophysiol. 16:37-68.\\n[8] Lettvin, J. Y., Maturana, H. R., McCulloch, W. S. & Pitts, W. H. (1959) Proc. Inst. Radio Eng.\\nNY 47:1940-1951.\\n[9] Snippe, H. P. & Koenderink, J. J. (1992) BioI. Cybern. 66:543-551.\\n[10] Wiggers, W., Roth, G., Eurich, C. W. & Straub, A. (1995) J. Camp. Physiol. A 176:365-377.\\n[11] Wilke, S. D. & Eurich, C. W. (1999) In Verleysen, M. (ed.), ESANN 99, European Symposium\\non Artificial Neural Networks, pp. 435-440. Brussels: D-Facto.\\n[12] Worgotter, F., Suder, K., Zhao, Y., Kerscher, N., Eysel, U. T. & Funke, K. (1998) Nature\\n396:165-168.\\n[13] Zhang, K. & Sejnowski, T. J. (1999) NeuraL Compo 11:75-84.\\n\\n\\f\",\n          \"Searching for Character Models\\n\\nJaety Edwards\\nDepartment of Computer Science\\nUC Berkeley\\nBerkeley, CA 94720\\njaety@cs.berkeley.edu\\n\\nDavid Forsyth\\nDepartment of Computer Science\\nUC Berkeley\\nBerkeley, CA 94720\\ndaf@cs.berkeley.edu\\n\\nAbstract\\nWe introduce a method to automatically improve character models for a\\nhandwritten script without the use of transcriptions and using a minimum\\nof document specific training data. We show that we can use searches for\\nthe words in a dictionary to identify portions of the document whose\\ntranscriptions are unambiguous. Using templates extracted from those\\nregions, we retrain our character prediction model to drastically improve\\nour search retrieval performance for words in the document.\\n\\n1 Introduction\\nAn active area of research in machine transcription of handwritten documents is reducing\\nthe amount and expense of supervised data required to train prediction models. Traditional\\nOCR techniques require a large sample of hand segmented letter glyphs for training. This\\nper character segmentation is expensive and often impractical to acquire, particularly if the\\ncorpora in question contain documents in many different scripts.\\nNumerous authors have presented methods for reducing the expense of training data by\\nremoving the need to segment individual characters. Both Kopec et al [3] and LeCun et al\\n[5] have presented models that take as input images of lines of text with their ASCII transcriptions. Training with these datasets is made possible by explicitly modelling possible\\nsegmentations in addition to having a model for character templates.\\nIn their research on ?wordspotting?, Lavrenko et al [4] demonstrate that images of entire\\nwords can be highly discriminative, even when the individual characters composing the\\nword are locally ambiguous. This implies that images of many sufficiently long words\\nshould have unambiguous transcriptions, even when the character models are poorly tuned.\\nIn our previous work, [2], the discriminatory power of whole words allowed us to achieve\\nstrong search results with a model trained on a single example per character.\\nThe above results have shown that A) one can learn new template models given images of\\ntext lines and their associated transcriptions, [3, 5] without needing an explicit segmentation\\nand that B) entire words can often be identified unambiguously, even when the models for\\nindividual characters are poorly tuned. [2, 4]. The first of these two points implies that\\ngiven a transcription, we can learn new character models. The second implies that for at\\nleast some parts of a document, we should be able to provide that transcription ?for free?,\\nby matching against a dictionary of known words.\\n\\n\\fs1\\n\\ns2\\n\\ns3\\n\\ns4\\n\\ns5\\n\\ns6\\n\\ns7\\n\\ns8\\n\\n?d\\n\\ndi\\n\\nix\\n\\nxe\\n\\ner\\n\\nri\\n\\nis\\n\\ns?\\n\\nFigure 1: A line, and the states that generate it. Each state st is defined by its left and\\nright characters ctl and ctr (eg ?x? and ?e? for s4 ). In the image, a state spans half of each\\nof these two characters, starting just past the center of the left character and extending to\\nthe center of the right character, i.e. the right half of the ?x? and the left half of the ?e?\\nin s4 . The relative positions of the two characters is given by a displacement vector dt\\n(superimposed on the image as white lines). Associating states with intracharacter spaces\\ninstead of with individual characters allows for the bounding boxes of characters to overlap\\nwhile maintaining the independence properties of the Markov chain.\\nIn this work we combine these two observations in order to improve character models\\nwithout the need for a document specific transcription. We provide a generic dictionary of\\nwords in the target language. We then identify ?high confidence? regions of a document.\\nThese are image regions for which exactly one word from our dictionary scores highly\\nunder our model. Given a set of high confidence regions, we effectively have a training\\ncorpus of text images with associated transcriptions. In these regions, we infer a segmentation and extract new character examples. Finally, we use these new exemplars to learn\\nan improved character prediction model. As in [2], our document in this work is a 12th\\ncentury manuscript of Terence?s Comedies obtained from Oxford?s Bodleian library [1].\\n\\n2 The Model\\nHidden Markov Models are a natural and widely used method for modeling images of text.\\nIn their simplest incarnation, a hidden state represents a character and the evidence variable\\nis some feature vector calculated at points along the line. If all characters were known to\\nbe of a single fixed width, this model would suffice. The probability of a line under this\\nmodel is given as\\nY\\np(line) = p(c1 |?)\\np(ct |ct?1 )p(im[w?(t?1):w?t]|ct )\\n(1)\\nt>1\\n\\nwhere ct represents the tth character on the line, ? represents the start state, w is the width\\nof a character, and im[w(t?1)+1:wt] represents the column of pixels beginning at column\\nw ? (t ? 1) + 1 of the image and ending at column w ? t, (i.e. the set of pixels spanned by\\nc)\\nUnfortunately, character?s widths do vary quite substantially and so we must extend the\\nmodel to accommodate different possible segmentations. A generalized HMM allows us to\\ndo this. In this model a hidden state is allowed to emit a variable length series of evidence\\nvariables. We introduce an explicit distribution over the possible widths of a character.\\nLetting dt be the displacement vector associated with the tth character, and ctx refer to the\\nx location of the left edge of a character on the line, the probability of a line under this\\nrevised model is\\nY\\np(line) = p(c1 |?)\\np(ct |ct?1 )p(dt |ct )p(im[ctx +1:ctx +d] |dt , ct )\\n(2)\\nt>1\\n\\nThis is the model we used in [2]. It performs far better than using an assumption of fixed\\nwidths, but it still imposes unrealistic constraints on the relative positions of characters. In\\n\\n\\fparticular, the portion of the ink generated by the current character is assumed to be independent of the preceding character. In other words, the model assumes that the bounding\\nboxes of characters do not overlap. This constraint is obviously unrealistic. Characters\\nroutinely overlap in our documents. ?f?s, for instance, form ligatures with most following characters. In previous work, we treated this overlap as noise, hurting our ability to\\ncorrectly localize templates. Under this model, local errors of alignment would also often propagate globally, adversely affecting the segmentation of the whole line. For search,\\nthis noisy segmentation still provides acceptable results. In this work, however, we need\\nto extract new templates, and thus correct localization and segmentation of templates is\\ncrucial.\\nIn our current work, we have relaxed this constraint, allowing characters to partially overlap. We achieve this by changing hidden states to represent character bigrams instead of\\nsingle characters (Figure 1). In the image, a state now spans the pixels from just past the\\ncenter of the left character to the pixel containing the center of the right character. We\\nadjust our notation somewhat to reflect this change, letting st now represent the tth hidden state and ctl and ctr be the left and right characters associated with s. dt is now the\\ndisplacement vector between the centers of ctl and ctr .\\nThe probability of a line under this, our actual, model is\\nY\\np(line) = p(s1 |?)\\np(st |st?1 )p(dt |ctl , ctr )p(im[stx +1:stx +dt ]|ctl , ctr , dt )\\n\\n(3)\\n\\nt>1\\n\\nThis model allows overlap of bounding boxes, but it does still make the assumption that\\nthe bounding box of the current character does not extend past the center of the previous\\ncharacter. This assumption does not fully reflect reality either. In Figure 1, for example,\\nthe left descender of the x extends back further than the center of the preceding character.\\nIt does, however, accurately reflect the constraints within the heart of the line (excluding\\nascenders and descenders). In practice, it has proven to generate very accurate segmentations. Moreover, the errors we do encounter no longer tend to affect the entire line, since\\nthe model has more flexibility with which to readjust back to the correct segmentation.\\n2.1 Model Parameters\\nOur transition distribution between states is simply a 3-gram character model. We train this\\nmodel using a collection of ASCII Latin documents collected from the web. This set does\\nnot include the transcriptions of our documents.\\nConditioned on displacement vector, the emission model for generating an image chunk\\ngiven a state is a mixture of gaussians. We associate with each character a set of image\\nwindows extracted from various locations in the document. We initialize these sets with\\none example a piece from our hand cut set (Figure 2). We adjust the probability of an image\\ngiven the state to include the distribution over blocks by expanding the last term of Equation\\n3 to reflect this mixture. Letting bck represent the k th exemplar in the set associated with\\ncharacter c, the conditional probability of an image region spanning the columns from x to\\nx? is given as\\nX\\np(imx:x? |ctl , ctr , dt ) =\\np(imx:x? |bctl i , bctr j , dt )\\n(4)\\ni,j\\n\\nIn principle, the displacement vectors should now be associated with an individual block,\\nnot a character. This is especially true when we have both upper and lower case letters.\\nHowever, our model does not seem particularly sensitive to this displacement distribution\\nand so in practice, we have a single, fairly loose, displacement distribution per character.\\nGiven a displacement vector, we can generate the maximum likelihood template image\\nunder our model by compositing the correct halves of the left and right blocks. Reshaping\\n\\n\\fthe image window into a vector, the likelihood of an image window is then modeled as\\na gaussian, using the corresponding pixels in the template as the means, and assuming\\na diagonal covariance matrix. The covariance matrix largely serves to mask out empty\\nregions of a character?s bounding box, so that we do not pay a penalty when the overlap of\\ntwo characters? bounding boxes contains only whitespace.\\n2.2 Efficiency Considerations\\nThe number of possible different templates for a state is O(|B| ? |B| ? |D|), where |B| is\\nthe number of different possible blocks and |D| is the number of candidate displacement\\nvectors. To make inference in this model computationally feasible, we first restrict the\\ndomain of d. For a given pair of blocks bl and br , we consider only displacement vectors\\nwithin some small x distance from a mean displacement mbl ,br , and we have a uniform\\ndistribution within this region. m is initialized from the known size of our single hand cut\\ntemplate. In the current work, we do not relearn the m. These are held fixed and assumed\\nto be the same for all blocks associated with the same letter.\\nEven when restricting the number of d?s under consideration as discussed above, it is computationally infeasible to consider every possible location and pair of blocks. We therefore\\nprune our candidate locations by looking at the likelihood of blocks in isolation and only\\nconsidering locations where there is a local optimum in the response function and whose\\nvalue is better than a given threshold. In this case our threshold for a given location is that\\nL(block) < .7L(background) (where L(x) represents the negative log likelihood of x).\\nIn other words, a location has to look at least marginally more like a given block than it\\nlooks like the background.\\nAfter pruning locations in this manner, we are left with a discrete set of ?sites,? where we\\ndefine a site as the tuple (block type, x location, y location). We can enumerate the set of\\npossible states by looking at every pair of sites whose displacement vector has a non-zero\\nprobability.\\n2.3 Inference In The Model\\nThe statespace defined above is a directed acyclic graph, anchored at the left edge and\\nright edges of a line of text. A path through this lattice defines both a transcription and\\na segmentation of the line into individual characters. Inference in this model is relatively\\nstraightforward because of our constraint that each character may overlap only one preceding and one following character, and our restriction of displacement vectors to a small\\ndiscrete range. The first restriction means that we need only consider binary relations between templates. The second preserves the independence relationships of an HMM. A\\ngiven state st is independent of the rest of the line given the values of all other states within\\ndmax of either edge of st (where dmax is the legal displacement vector with the longest\\nx component.) We can therefore easily calculate the best path or explicitly calculate the\\nposterior of a node by traversing the state graph in topological order, sorted from left to\\nright. The literature on Weighted Finite State Transducers ([6], [5]) is a good resource for\\nefficient algorithms on these types of statespace graph.\\n\\n3 Learning Better Character Templates\\nWe initialize our algorithm with a set of handcut templates, exactly 1 per character, (Figure\\n2), and our goal is to construct more accurate character models automatically from unsupervised data. As noted above, we can easily calculate the posterior of a given site under\\nour model. (Recall that a site is a particular character template at a given (x,y) location in\\nthe line.) The traditional EM approach to estimating new templates would be to use these\\n\\n\\fFigure 2: Original Training Data These 22 glyphs are our only document specific training\\ndata. We use the model based on these characters to extract the new examples shown below\\n\\nFigure 3: Examples of extracted templates We extract new templates from high confidence\\nregions. From these, we choose a subset to incorporate into the model as new exemplars.\\nTemplates are chosen iteratively to best cover the space of training examples. Notice that\\nfor ?q? and ?a?, we have extracted capital letters, of which there were no examples in\\nour original set of glyphs. This happens when the combination of constraints from the\\ndictionary the surrounding glyphs make a ?q? or ?a? the only possible explanation for\\nthis region, even though its local likelihood is poor.\\n\\nsites as training examples, weighted by their posteriors. Unfortunately, the constraints imposed by 3 and even 4-gram character models seem to be insufficient. The posteriors of\\nsites are not discriminative enough to get learning off the ground.\\nThe key to successfully learning new templates lies is the observation from our previous\\nwork [2], that even when the posteriors of individual characters are not discriminative, one\\ncan still achieve very good search results with the same model. The search word in effect\\nserves as its own language model, only allowing paths through the state graph that actually\\ncontain it, and the longer the word the more it constrains the model. Whole words impose\\nmuch tighter constraints than a 2 or 3-gram character model, and it is only with this added\\npower that we can successfully learn new character templates.\\nWe define the score for a search as the negative log likelihood of the best path containing\\nthat word. With sufficiently long words, it becomes increasingly unlikely that a spurious\\npath will achieve a high score. Moreover, if we are given a large dictionary of words and\\nno alternative word explains a region of ink nearly as well as the best scoring word, then\\nwe can be extremely confident that this is a true transcription of that piece of ink.\\nStarting with a weak character model, we do not expect to find many of these ?high confidence? regions, but with a large enough document, we should expect to find some. From\\nthese regions, we can extract new, reliable templates with which to improve our character\\nmodels. The most valuable of these new templates will be those that are significantly different from any in our current set. For example, in Figure 3, note that our system identifies\\ncapital Q?s, even though our only input template was lower case. It identifies this ink as\\na Q in much the same way that a person solves a crossword puzzle. We can easily infer\\nthe missing character in the string ?obv-ous? because the other letters constrain us to one\\npossible solution. Similarly, if other character templates in a word match well, then we can\\nunambiguously identify the other, more ambiguous ones. In our Latin case, ?Quid? is the\\nonly likely explanation for ?-uid?.\\n3.1 Extracting New Templates and Updating The Model\\nWithin a high confidence region we have both a transcription and a localization of template\\ncenters. It remains only to cut out new templates. We accomplish this by creating a template\\nimage for the column of pixels from the corresponding block templates and then assigning\\nimage pixels to the nearest template character (measured by Euclidean distance).\\nGiven a set of templates extracted from high confidence regions, we choose a subset of\\n\\n\\fScore Under Model\\n\\nworse\\n3400\\n3350\\n3300\\nbest\\nConfidence Margins\\n\\nFigure 4: Each line segment in the lower figure represents a proposed location for a word\\nfrom our dictionary. It?s vertical height is the score of that location under our model. A\\nlower score represents a better fit. The dotted line is the score of our model?s best possible\\npath. Three correct words, ?nec?, ?quin? and ?dari?, are actually on the best path. We\\ndefine the confidence margin of a location as the difference in score between the best\\nfitting word from our dictionary and the next best.\\n\\nFigure 5: Extracting Templates For a region with sufficiently high confidence margin, we\\nconstruct the maximum likelihood template from our current exemplars. left, and we assign\\npixels from the original image to a template based on its distance to the nearest pixel in\\nthe template image, extracting new glyph exemplars right. These new glyphs become the\\nexemplars for our next round of training.\\n\\ntemplates that best explain the remaining examples. We do this in a greedy fashion by\\nchoosing the example whose likelihood is lowest under our current model and adding it to\\nour set. Currently, we threshold the number of new templates for the sake of efficiency. Finally, given the new set of templates, we can add them to the model and rerun our searches,\\npotentially identifying new high confidence regions.\\n\\n4 Results\\nOur algorithm iteratively improves the character model by gathering new training data from\\nhigh confidence regions. Figure 3 shows that this method finds new templates significantly\\ndifferent from the originals. In this document, our set of examples after one round appears\\nto cover the space of character images well, at least those in lower case. Our templates are\\nnot perfect. The ?a?, for instance, has become associated with at least one block that is in\\nfact an ?o?. These mistakes are uncommon, particularly if we restrict ourselves to longer\\nwords. Those that do occur introduce a tolerable level noise into our model. They make\\ncertain regions of the document more ambiguous locally, but that local ambiguity can be\\novercome with the context provided by surrounding characters and a language model.\\nImproved Character Models We evaluate the method more quantitatively by testing the\\nimpact of the new templates on the quality of searches performed against the document.\\nTo search for a given word, we rank lines by the ratio of the maximum likelihood transcription/segmentation that contains the search word to the likelihood of the best possible\\nsegmentation/transcription under our model. The lowest possible search score is 1, happening when the search word is actually a substring of the maximum likelihood transcription.\\nHigher scores mean that the word is increasingly unlikely under our model. In Figure 7, the\\nfigure on the left shows the improvement in ranking of the lines that truly contain selected\\nsearch words. The odd rows (in red) are search results using only the original 22 glyphs,\\n\\n\\f20\\n40\\n60\\n80\\n100\\n\\n200\\n\\n300\\n\\n400\\n\\n500\\n\\n600\\n\\nRnd 2\\n\\nRnd 1\\n\\n2700\\n2650\\n2600\\ndotted (wrong):\\nsolid (correct):\\n1920\\n1900\\n1880\\n1860\\n1840\\ndotted (wrong):\\nsolid (correct):\\n\\niam\\n\\nnupta\\nnuptiis\\n\\ninquam\\n\\n(v|u)ideo\\nvidet\\n\\nnupta\\nnuptiis\\n\\npost inquam\\npostquam\\n\\n(v|u)ideo\\nvidet\\n\\nFigure 6: Search Results with (Rnd 1) initial templates only and with (Rnd 2) templates\\nextracted from high confidence regions. We show results that have a score within 5% of the\\nbest path. Solid Lines are the results for the correct word. Dotted lines represent other\\nsearch results, where we have made a few larger in order to show those words that are\\nthe closest competitors to the true word. Many alternative searches, like the highlighted\\n?post? are actually portions of the correct larger words. These restrict our selection of\\nconfidence regions, but do not impinge on search quality.\\nEach correct word has significantly improved after one round of template reestimation.\\n?iam? has been correctly identified, and is a new high confidence region. Both ?nuptiis?\\nand ?postquam? are now the highest likelihood words for their region barring smaller\\nsubsequences, and ?videt? has narrowed the gap between its competitor ?video?.\\nwhile the even rows (in green) use an additional 332 glyphs extracted from high confidence\\nregions. Search results are markedly improved in the second model. The word ?est?, for\\ninstance, only had 15 of 24 of the correct lines in the top 100 under the original model,\\nwhile under the learned model all 24 are not only present but also more highly ranked.\\nImproved Search Figure 6 shows the improved performance of our refitted model for\\na single line. Most words have greatly improved relative to their next best alternative.\\n?postquam? and ?iam? were not even considered by the original model and now are nearly\\noptimal. The right of Figure 7 shows the average precision/recall curve under each model\\nfor 21 words with more than 4 occurrences in the dataset. Precision is the percentage\\nof lines truly containing a word in the top n search results, and recall is the percentage\\nof all lines containing the word returned in the top n results. The learned model clearly\\ndominates. The new model also greatly improves performance for rare words. For 320\\nwords ocurring just once in the dataset, 50% are correctly returned as the top ranked result\\nunder the original model. Under the learned model, this number jumps to 78%.\\n\\n5 Conclusions and Future Work\\nIn most fonts, characters are quite ambiguous locally. An ?n? looks like a ?u?, looks like\\n?ii?, etc. This ambiguity is the major hurdle to the unsupervised learning of character\\ntemplates. Language models help, but the standard n-gram models provide insufficient\\nconstraints, giving posteriors for character sites too uninformative to get EM off the ground.\\n\\n\\fAggregate Precision/Recall Curve\\n\\nSelected Words, Top 100 Returned Lines\\n\\nPrecision\\n\\nest\\n(15,24)/24\\nnescio\\n( 1, 1)/ 1\\npostquam\\n( 0, 2)/ 2\\nquod\\n(14,14)/14\\nmoram\\n( 0, 2)/ 2\\nnon\\n( 8, 8)/ 8\\nquid\\n( 9, 9)/ 9\\n10 20 30 40 50 60 70 80 90100\\n\\n0.75\\n0.7\\n0.65\\n0.6\\n0.55\\n0.5\\n0.45\\n0.4\\n0.35\\n\\nOriginal Model\\nRefit Model\\n0.2\\n\\n0.4\\n0.6\\nRecall\\n\\n0.8\\n\\n1\\n\\nFigure 7: The figure on the left shows the those lines with the top 100 scores that actually\\ncontain the specified word. The first of each set of two rows (in red) is the results from\\nRound 1. The second (in green) is the results for Round 2. Almost all search words in our\\ncorpus show a significant improvement. The numbers to the right (x/y) mean that out of\\ny lines that actually contained the search word in our document, x of them made it into\\nthe top ten. On the right are average precision/recall curves for 21 high frequency words\\nunder the model with our original templates (Rnd 1) and after refitting with new extracted\\ntemplates (Rnd 2). Extracting new templates vastly improves our search quality\\nAn entire word is much different. Given a dictionary, we expect many word images to have\\na single likely transcription even if many characters are locally ambiguous. We show that\\nwe can identify these high confidence regions even with a poorly tuned character model. By\\nextracting new templates only from these regions of the document, we overcome the noise\\nproblem and significantly improve our character models. We demonstrate this improvement\\nfor the task of search where the refitted models have drastically better search responses than\\nwith the original. Our method is indifferent to the form of the actual character emission\\nmodel. There is a rich literature in character prediction from isolated image windows, and\\nwe expect that incorporating more powerful character models should provide even greater\\nreturns and help us in learning less regular scripts.\\nFinding high confidence regions to extract good training examples is a broadly applicable concept. We believe this work should extend to other problems, most notably speech\\nrecognition. Looked at more abstractly, our use of language model in this work is actually encoding spatial constraints. The probability of a character given an image window\\ndepends not only on the identify of surrounding characters but also on their spatial configuration. Integrating context into recognition problems is an area of intense research in\\nthe computer vision community, and we are investigating extending the idea of confidence\\nregions to more general object recognition problems.\\n\\nReferences\\n[1] Early Manuscripts at Oxford University. Bodleian library ms. auct. f. 2.13. http://image.ox.ac.uk/.\\n[2] J. Edwards, Y.W. Teh, D. Forsyth, R. Bock, M. Maire, and G. Vesom. Making latin manuscripts\\nsearchable using ghmm?s. In NIPS 17, pages 385?392. 2005.\\n[3] G. Kopec and M. Lomelin. Document-specific character template estimation. In Proceedings,\\nDocument Image Recognition III, SPIE, 1996.\\n[4] V. Lavrenko, T. Rath, and R. Manmatha. Holistic word recognition for handwritten historical\\ndocuments. In dial, pages 278?287, 2004.\\n[5] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document\\nrecognition. Proceedings of the IEEE, 86(11):2278?2324, 1998.\\n[6] M. Mohri, F. Pereira, and M. Riley. Weighted finite state transducers in speech recognition. ISCA\\nITRW Automatic Speech Recognition, pages 97?106, 2000.\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile(\"papers.zip\", \"r\") as zip_ref:\n",
        "    # Extract the file to a temporary directory\n",
        "    zip_ref.extractall(\"temp\")\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "papers = pd.read_csv(\"temp/papers.csv\")\n",
        "\n",
        "# Print head\n",
        "papers.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the columns\n",
        "papers = papers.drop(columns=['id', 'event_type', 'pdf_name'], axis=1).sample(100)\n",
        "\n",
        "# Print out the first rows of papers\n",
        "papers.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0fglSUUGZ0E9",
        "outputId": "756bb16d-1bcc-47e0-ea06-1c27687e3af3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      year                                              title  \\\n",
              "2394  2007        Continuous Time Particle Filtering for fMRI   \n",
              "4828  2014  Nonparametric Bayesian inference on multivaria...   \n",
              "5638  2016  Relevant sparse codes with variational informa...   \n",
              "1654  2003  Extending Q-Learning to General Adaptive Multi...   \n",
              "2206  2006        Learning to be Bayesian without Supervision   \n",
              "\n",
              "                                               abstract  \\\n",
              "2394  We construct a biologically motivated stochast...   \n",
              "4828  We develop a model by choosing the maximum ent...   \n",
              "5638  In many applications, it is desirable to extra...   \n",
              "1654                                   Abstract Missing   \n",
              "2206                                   Abstract Missing   \n",
              "\n",
              "                                             paper_text  \n",
              "2394  Continuous Time Particle Filtering for fMRI\\n\\...  \n",
              "4828  Nonparametric Bayesian inference on multivaria...  \n",
              "5638  Relevant sparse codes with variational informa...  \n",
              "1654  Extending Q-Learning to General Adaptive\\nMult...  \n",
              "2206  Learning to be Bayesian without Supervision\\n\\...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b5a1484f-570d-4f40-aab6-2a22dd62ae5d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2394</th>\n",
              "      <td>2007</td>\n",
              "      <td>Continuous Time Particle Filtering for fMRI</td>\n",
              "      <td>We construct a biologically motivated stochast...</td>\n",
              "      <td>Continuous Time Particle Filtering for fMRI\\n\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4828</th>\n",
              "      <td>2014</td>\n",
              "      <td>Nonparametric Bayesian inference on multivaria...</td>\n",
              "      <td>We develop a model by choosing the maximum ent...</td>\n",
              "      <td>Nonparametric Bayesian inference on multivaria...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5638</th>\n",
              "      <td>2016</td>\n",
              "      <td>Relevant sparse codes with variational informa...</td>\n",
              "      <td>In many applications, it is desirable to extra...</td>\n",
              "      <td>Relevant sparse codes with variational informa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1654</th>\n",
              "      <td>2003</td>\n",
              "      <td>Extending Q-Learning to General Adaptive Multi...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Extending Q-Learning to General Adaptive\\nMult...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2206</th>\n",
              "      <td>2006</td>\n",
              "      <td>Learning to be Bayesian without Supervision</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Learning to be Bayesian without Supervision\\n\\...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b5a1484f-570d-4f40-aab6-2a22dd62ae5d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b5a1484f-570d-4f40-aab6-2a22dd62ae5d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b5a1484f-570d-4f40-aab6-2a22dd62ae5d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-89c3d5b0-243f-44ac-bcf1-40f36dd20c49\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-89c3d5b0-243f-44ac-bcf1-40f36dd20c49')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-89c3d5b0-243f-44ac-bcf1-40f36dd20c49 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "papers",
              "summary": "{\n  \"name\": \"papers\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9,\n        \"min\": 1987,\n        \"max\": 2017,\n        \"num_unique_values\": 31,\n        \"samples\": [\n          1991,\n          2013,\n          2011\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Adaptive Averaging in Accelerated Descent Dynamics\",\n          \"Max-Margin Deep Generative Models\",\n          \"Causal Categorization with Bayes Nets\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 56,\n        \"samples\": [\n          \"We construct a biologically motivated stochastic differential model of the neural and hemodynamic activity underlying the observed Blood Oxygen Level Dependent (BOLD) signal in Functional Magnetic Resonance Imaging (fMRI). The model poses a difficult parameter estimation problem, both theoretically due to the nonlinearity and divergence of the differential system, and computationally due to its time and space complexity. We adapt a particle filter and smoother to the task, and discuss some of the practical approaches used to tackle the difficulties, including use of sparse matrices and parallelisation. Results demonstrate the tractability of the approach in its application to an effective connectivity study.\",\n          \"We study the scalability of consensus-based distributed optimization algorithms by considering two questions: How many processors should we use for a given problem, and how often should they communicate when communication is not free? Central to our analysis is a problem-specific value $r$ which quantifies the communication/computation tradeoff. We show that organizing the communication among nodes as a $k$-regular expander graph~\\\\cite{kRegExpanders} yields speedups, while when all pairs of nodes communicate (as in a complete graph), there is an optimal number of processors that depends on $r$. Surprisingly, a speedup can be obtained, in terms of the time to reach a fixed level of accuracy, by communicating less and less frequently as the computation progresses. Experiments on a real cluster solving metric learning and non-smooth convex minimization tasks demonstrate strong agreement between theory and practice.\",\n          \"Gaussian process regression generally does not scale to beyond a few thousands data points without applying some sort of kernel approximation method. Most approximations focus on the high eigenvalue part of the spectrum of the kernel matrix, $K$, which leads to bad performance when the length scale of the kernel is small. In this paper we introduce Multiresolution Kernel Approximation (MKA), the first true broad bandwidth kernel approximation algorithm.  Important points about MKA are that it is memory efficient, and it is a direct method,  which means that it also makes it easy to approximate $K^{-1}$ and $\\\\mathop{\\\\textrm{det}}(K)$.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Adaptive Averaging in Accelerated Descent Dynamics\\n\\nWalid Krichene ?\\nUC Berkeley\\n\\nAlexandre M. Bayen\\nUC Berkeley\\n\\nPeter L. Bartlett\\nUC Berkeley and QUT\\n\\nwalid@eecs.berkeley.edu\\n\\nbayen@berkeley.edu\\n\\nbartlett@cs.berkeley.edu\\n\\nAbstract\\nWe study accelerated descent dynamics for constrained convex optimization. This\\ndynamics can be described naturally as a coupling of a dual variable accumulating\\ngradients at a given rate ?(t), and a primal variable obtained as the weighted average\\nof the mirrored dual trajectory, with weights w(t). Using a Lyapunov argument,\\nwe give sufficient conditions on ? and w to achieve a desired convergence rate. As\\nan example, we show that the replicator dynamics (an example of mirror descent\\non the simplex) can be accelerated using a simple averaging scheme.\\nWe then propose an adaptive averaging heuristic which adaptively computes the\\nweights to speed up the decrease of the Lyapunov function. We provide guarantees\\non adaptive averaging in continuous-time, prove that it preserves the quadratic\\nconvergence rate of accelerated first-order methods in discrete-time, and give\\nnumerical experiments to compare it with existing heuristics, such as adaptive\\nrestarting. The experiments indicate that adaptive averaging performs at least as\\nwell as adaptive restarting, with significant improvements in some cases.\\n\\n1\\n\\nIntroduction\\n\\nWe study the problem of minimizing a convex function f over a feasible set X , a closed convex subset\\nof E = Rn . We will assume that f is differentiable, that its gradient ?f is a Lipschitz function with\\nLipschitz constant L, and that the set of minimizers S = arg minx?X f (x) is non-empty. We will\\nfocus on the study of continuous-time, first-order dynamics for optimization. First-order methods\\nhave seen a resurgence of interest due to the significant increase in both size and dimensionality of the\\ndata sets typically encountered in machine learning and other applications, which makes higher-order\\nmethods computationally intractable in most cases. Continuous-time dynamics for optimization\\nhave been studied for a long time, e.g. [6, 9, 5], and more recently [20, 2, 1, 3, 11, 23], in which a\\nconnection is made between Nesterov?s accelerated methods [14, 15] and a family of continuous-time\\nODEs. Many optimization algorithms can be interpreted as a discretization of a continuous-time\\nprocess, and studying the continuous-time dynamics is useful for many reasons: The analysis is\\noften simpler in continuous-time, it can help guide the design and analysis of new algorithms, and\\nit provides intuition and insight into the discrete process. For example, Su et al. show in [20] that\\nNesterov?s original method [14] is a discretization of a second-order ODE, and use this interpretation\\nto propose a restarting heuristic which empirically speeds up the convergence. In [11], we generalize\\nthis approach to the proximal version of Nesterov?s method [15] which applies to constrained convex\\nproblems, and show that the continuous-time ODE can be interpreted as coupled dynamics of a dual\\nvariable Z(t) which evolves in the dual space E ? , and a primal variable X(t) which is obtained as\\nthe weighted average of a non-linear transformation of the dual trajectory. More precisely,\\n?\\n?\\nZ(t)\\n= ? rt ?f (X(t))\\n?\\n?\\n?\\n?\\n\\nR t r?1\\n?\\n?? ? (Z(? ))d?\\n\\nX(t) = 0 R t ? r?1 d?\\n?\\n0\\n?\\n?\\n?\\nX(0) = ?? ? (Z(0)) = x0 ,\\n?\\n\\nWalid Krichene is currently affiliated with Google. walidk@google.com\\n\\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\\n\\n\\fwhere r ? 2 is a fixed parameter, the initial condition x0 is a point in the feasible set X , and ?? ? is\\na Lipschitz function that maps from the dual space E ? to the feasible set X , which we refer to as the\\nmirror map (such a function can be constructed using standard results from convex analysis, by taking\\nthe convex conjugate of a strongly convex function ? with domain X ; see the supplementary material\\nfor a brief review of the definition and basic properties of mirror maps). Using a Lyapunov argument,\\nwe show that the solution trajectories of this ODE exhibit a quadratic convergence rate, i.e. if f ? is the\\nminimum of f over the feasible set, then f (X(t)) ? f ? ? C/t2 for a constant C which depends on\\nthe initial conditions. This formalized an interesting connection between acceleration and averaging,\\nwhich had been observed in [8] in the special case of unconstrained quadratic minimization.\\nA natural question that arises is whether different averaging schemes can be used to achieve the same\\nrate, or perhaps faster rates. In this article, we provide a positive answer. We study a broad family of\\nAccelerated Mirror Descent (AMD) dynamics, given by\\n?\\n?\\nZ(t)\\n= ??(t)?f (X(t))\\n?\\n?\\n?\\n?\\nR\\nRt\\nX(t0 )W (t0 )+ tt w(? )?? ? (Z(? ))d?\\n0\\nAMDw,?\\nX(t) =\\n, with W (t) = 0 w(? )d?\\nW (t)\\n?\\n?\\n?\\n?\\nX(t0 ) = ?? ? (Z(t0 )) = x0 ,\\n\\n(1)\\n\\nparameterized by two positive, continuous weight functions w and ?, where w is used in the averaging\\nand ? determines the rate at which Z accumulates gradients. This is illustrated in Figure 1. In our\\nformulation we choose to initialize the ODE at t0 > 0 instead of 0 (to guarantee existence and\\nuniqueness of a solution, as discussed in Section 2). We give a unified study of this ODE using an\\nappropriate Lyapunov function, given by\\nLr (X, Z, t) = r(t)(f (X) ? f ? ) + D?? (Z, z ? ),\\n\\n(2)\\n\\nwhere D?? is the Bregman divergence associated with ? ? (a non-negative function defined on\\nE ? ? E ? ), and r(t) is a desired convergence rate (a non-negative function defined on R+ ). By\\nconstruction, Lr is a non-negative function on X ? E ? ? R+ . If t 7? Lr (X(t), Z(t), t) is a\\nnon-increasing function for all solution trajectories (X(t), Z(t)), then Lr is said to be a Lyapunov\\nfunction for the ODE, in reference to Aleksandr Mikhailovich Lyapunov [12]. We give in Theorem 2\\na sufficient condition on ?, w and r for Lr to be a Lyapunov function for AMDw,? , and show that\\nunder these conditions, f (X(t)) converges to f ? at the rate 1/r(t).\\nE\\n\\n??\\n\\nX\\n\\nE?\\n\\n?? ? (Z(t))\\n\\nZ(t)\\n\\nX(t)\\n\\n??(t)?f (X(t))\\n\\n?? ?\\n\\nFigure 1: Illustration of AMDw,? . The dual variable Z evolves in the dual space E ? , and accumulates\\nnegative gradients at a rate ?(t), and the primal variable X(t) (green solid line) is obtained by\\naveraging the mirrored trajectory {?? ? (Z(? )), ? ? [t0 , t]} (green dashed line), with weights w(? ).\\nIn Section 3, we give an equivalent formulation of AMDw,? written purely in the primal space. We\\ngive several examples of these dynamics for simple constraint sets. In particular, when the feasible\\nset is the probability simplex, we derive an accelerated version of the replicator dynamics, an ODE\\nthat plays an important role in evolutionary game theory [22] and viability theory [4].\\nMany heuristics have been developed to empirically speed up the convergence of accelerated methods.\\nMost of these heuristics consist in restarting the ODE (or the algorithm in discrete time) whenever\\na simple condition is met. For example, a gradient restart heuristic is proposed in [17], in which\\nthe algorithm is restarted whenever the trajectory forms an acute angle with the gradient (which\\nintuitively indicates that the trajectory is not making progress), and a speed restarting heuristic\\n?\\nis proposed in [20], in which the ODE is restarted whenever the speed kX(t)k\\ndecreases (which\\nintuitively indicates that progress is slowing). These heuristics are known to empirically improve\\n2\\n\\n\\fthe speed of convergence, but provide few guarantees. For example, the gradient restart in [17]\\nis only studied for unconstrained quadratic problems, and the speed restart in [20] is only studied\\nfor unconstrained strongly convex problems. In particular, it is not guaranteed (to our knowledge)\\nthat these heuristics preserve the original convergence rate of the non-restarted method, when the\\nobjective function is not strongly convex. In Section 4, we propose a new heuristic that provides such\\nguarantees, and that is based on a simple idea for adaptively computing the weights w(t) along the\\nsolution trajectories. The heuristic simply decreases the time derivative of the Lyapunov function\\nLr (X(t), Z(t), t) whenever possible. Thus it preserves the 1/r(t) convergence rate. Other adaptive\\nmethods have been applied to convex optimization, such as Adagrad [7] and Adam [10], which adapt\\nthe learning rate in first-order methods, by maintaining moment estimates of the observed gradients.\\nThey are particularly well suited to problems with sparse gradients. While these methods are similar\\nin spirit to adaptive averaging, they are not designed for accelerated methods. In Section 5, we give\\nnumerical experiments in which we compare the performance of adaptive averaging and restarting.\\nThe experiments indicate that adaptive averaging compares favorably in all of the examples, and\\ngives a significant improvement in some cases. We conclude with a brief discussion in Section 6.\\n\\n2\\n\\nAccelerated mirror descent with generalized averaging\\n\\nWe start by giving an equivalent form of AMDw,? , which we use to briefly discuss existence\\nand uniqueness of a solution. Writing the second equation as X(t)W (t) ? X(t0 )W (t0 ) =\\nRt\\nw(? )?? ? (Z(? ))d? , then taking the time-derivative, we have\\nt0\\n?\\nX(t)W\\n(t) + X(t)w(t) = w(t)?? ? (Z(t)).\\nThus the ODE is equivalent to\\n? ?\\nZ(t) = ??(t)?f (X(t))\\n?\\n?\\n?\\n0\\nw(t)\\n?\\nAMDw,?\\nX(t)\\n= W\\n(?? ? (Z(t)) ? X(t))\\n(t)\\n?\\n?\\n?\\nX(t0 ) = ?? ? (Z(t0 )) = x0 .\\n\\nThe following theorem guarantees existence and uniqueness of the solution.\\nTheorem 1. Suppose that W (t0 ) > 0. Then AMDw,? has a unique maximal (i.e. defined on a\\nmaximal interval) solution (X(t), Z(t)) that is C 1 ([t0 , +?)). Furthermore, for all t ? t0 , X(t)\\nbelongs to the feasible set X .\\nProof. Recall that, by assumption, ?f and ?? ? are both Lipschitz, and w, ? are continuous. Furthermore, W (t) is non-decreasing and continuous, as the integral of a non-negative function, thus\\nw(t)/W (t) ? w(t)/W (t0 ). This guarantees that on any finite interval [t0 , T ), the functions ?(t) and\\nw(t)\\n?\\nw(t)/W (t) are bounded. Therefore, ??(t)?f (X) and W\\n(t) (?? (Z) ? X) are Lipschitz functions\\nof (X, Z), uniformly in t ? [t0 , T ). By the Cauchy-Lipschitz theorem (e.g. Theorem 2.5 in [21]),\\nthere exists a unique C 1 solution defined on [t0 , T ). Since T is arbitrary, this defines a unique solution\\non all of [t0 , +?). Indeed, any two solutions defined on [t0 , T1 ) and [t0 , T2 ) with T2 > T1 coincide\\non [t0 , T1 ). Finally, feasibility of the solution follows from the fact that X is convex and X(t) is the\\nweighted average of points in X , specifically, x0 and the set {?? ? (Z(? )), ? ? [t0 , t]}.\\nNote that in general, it is important to initialize the ODE at t0 and not 0, since W (0) = 0 and\\nw(t)/W (t) can diverge at 0, in which case one cannot apply the Cauchy-Lipschitz theorem. It is\\npossible however to prove existence and uniqueness with t0 = 0 for some choices of w, by taking\\na sequence of Lipschitz ODEs that approximate the original one, as is done in [20], but this is a\\ntechnicality and does not matter for practical purposes.\\nWe now move to our main result for this section. Suppose that r is an increasing, positive differentiable\\nfunction on [t0 , +?), and consider the candidate Lyapunov function Lr defined in (2), where the\\nBregman divergence term is given by\\nD?? (z, y) := ? ? (z) ? ? ? (y) ? h?? ? (y), z ? yi ,\\n\\nand z ? is a point in the dual space such that ?? ? (z ? ) = x? belongs to the set of minimizers S. Let\\n(X(t), Z(t)) be the unique maximal solution trajectory of AMDw,? .\\n3\\n\\n\\fTaking the derivative of t 7? Lr (X(t), Z(t), t) = r(t)(f (X(t)) ? f ? ) + D?? (Z(t), z ? ), we have\\n\\nD\\nE D\\nE\\nd\\n?\\n?\\nLr (X(t), Z(t), t) = r0 (t)(f (X(t)) ? f ? ) + r(t) ?f (X(t)), X(t)\\n+ Z(t),\\n?? ? (Z(t)) ? ?? ? (z ? )\\ndt\\n\\u001d\\nD\\nE \\u001c\\nW (t) ?\\n0\\n?\\n?\\n?\\n= r (t)(f (X(t)) ? f ) + r(t) ?f (X(t)), X(t) + ??(t)?f (X(t)), X(t) +\\nX(t) ? x\\nw(t)\\n\\u0013\\nD\\nE\\u0012\\n?(t)W (t)\\n?\\n? (f (X(t)) ? f ? )(r0 (t) ? ?(t)) + ?f (X(t)), X(t)\\nr(t) ?\\n,\\n(3)\\nw(t)\\n\\nwhere we used the expressions for Z? and ?? ? (Z) from AMD0w,? in the second equality, and\\nconvexity of f in the last inequality. Equipped with this bound, it becomes straightforward to give\\nsufficient conditions for Lr to be a Lyapunov function.\\nTheorem 2. Suppose that for all t ? [t0 , +?),\\n0\\n1. ?(t)\\nD ? r (t) and E \\u0010\\n?\\n2. ?f (X(t)), X(t)\\nr(t) ?\\n\\n?(t)W (t)\\nw(t)\\n\\n\\u0011\\n\\n? 0.\\n\\nThen Lr is a Lyapunov function for AMDw,? , and for all t ? t0 , f (X(t)) ? f ? ?\\n\\nLr (X(t0 ),Z(t0 ),t0 )\\n.\\nr(t)\\n\\nd\\nProof. The two conditions, combined with inequality (3), imply that dt\\nLr (X(t), Z(t), t) ? 0, thus\\nLr is a Lyapunov function. Finally, since D?? is non-negative, and Lr is decreasing, we have\\n\\nf (X(t)) ? f ? ?\\n\\nLr (X(t), Z(t), t)\\nLr (X(t0 ), Z(t0 ), t0 )\\n?\\n.\\nr(t)\\nr(t)\\n\\nwhich proves the claim.\\nNote that the second condition depends on the solution trajectory X(t), and may be hard to check a\\npriori. However, we give one special case in which the condition trivially holds.\\nCorollary 1. Suppose that for all t ? [t0 , +?), ?(t) =\\n\\nw(t)r(t)\\nW (t) ,\\n?\\n\\nw(t)\\nr 0 (t)\\nW (t) ? r(t) . Then\\nLr (X(t0 ),Z(t0 ),t0 )\\n.\\nr(t)\\n\\nand\\n\\nLyapunov function for AMDw,? , and for all t ? t0 , f (X(t)) ? f ?\\n\\nLr is a\\n\\nNext, we describe a method to construct weight functions w, ? that satisfy the conditions of Corolw(t)\\nr 0 (t)\\nlary 1, given a desired rate r. Of course, it suffices to construct w that satisfies W\\n(t) ? r(t) , then\\nto set ?(t) =\\n\\nw(t)r(t)\\nW (t) .\\n\\nWe can reparameterize the weight function by writing\\n\\nintegrating from t0 to t, we have\\n\\nW (t)\\nW (t0 )\\n\\n=e\\n\\nRt\\nt0\\n\\na(? )d?\\n\\nw(t) = w(t0 )\\n\\nw(t)\\nW (t)\\n\\n= a(t). Then\\n\\n, and\\n\\na(t) Rtt a(? )d?\\ne 0\\n.\\na(t0 )\\n\\n(4)\\n\\nTherefore the conditions of the corollary are satisfied whenever w(t) is of the form (4) and a : R+ ?\\n0\\n(t)\\nR+ is a continuous, positive function with a(t) ? rr(t)\\n. Note that the expression of w is defined up\\nto the constant w(t0 ), which reflects the fact that the condition of the corollary is scale-invariant (if\\nthe condition holds for a function w, then it holds for ?w for all ? > 0).\\nExample 1. Let r(t) = t2 . Then r0 (t)/r(t) = 2/t, and we can take a(t) = ?t with ? ? 2. Then\\nRt\\n\\na(? )d?\\n\\na(t)\\n?/t ? ln(t/t0 )\\ne t0\\ne\\n= (t/t0 )??1 and ?(t) = w(t)r(t)\\nw(t) = a(t\\n= ?/t\\nW (t) = ?t, and we recover\\n0)\\n0\\nthe weighting scheme used in [11].\\nExample 2. More generally, if r(t) = tp , p ? 1, then r0 (t)/r(t) = p/t, and we can take a(t) = ?t\\np?1\\nwith ? ? p. Then w(t) = (t/t0 )??1 , and ?(t) = w(t)r(t)\\n.\\nW (t) = ?t\\n\\nWe also exhibit in the following a second energy function that is guaranteed to decrease under the\\nsame conditions. This energy function, unlike the Lyapunov function Lr , does not guarantee a\\nspecific convergence rate. However, it captures a natural measure of energy in the system. To define\\nthis energy function, we will use the following characterization of the inverse mirror map: By duality\\nof the subdifferentials (e.g. Theorem 23.5 in [18]), we have for a pair of convex conjugate functions ?\\nand ? ? that x ? ?? ? (x? ) if and only if x? ? ??(x). To simplify the discussion, we will assume that\\n? is also differentiable, so that (?? ? )?1 = ?? (this assumption can be relaxed). In what follows,\\n? = ??(X) and Z? = ?? ? (Z).\\nwe will denote by X\\n4\\n\\n\\f? = ??(X).\\nTheorem 3. Let (X(t), Z(t)) be the unique maximal solution of AMDw,? , and let X\\nConsider the energy function\\nEr (t) = f (X(t)) +\\n\\n1\\n?\\nD?? (Z(t), X(t)).\\nr(t)\\n\\n(5)\\n\\nThen if w, ? satisfy condition (2) of Theorem 2, Er is a decreasing function of time.\\nProof. To make the notation more concise,\\nwe omit the\\u000b explicit dependence on time in this proof.\\n? = ? ? (Z) ? ? ? (X)\\n? ? X, Z ? X\\n? . Taking the time-derivative , we have\\nWe have D?? (Z, X)\\nE\\nE D\\nE D\\nD\\nE D\\nd\\n??\\n?? ? X,\\n? Z ?X\\n? ? X, Z? ? X\\n? = ?? ? (Z), Z? ? ?? ? (X),\\n? X\\nD?? (Z, X)\\ndt\\nD\\nE D\\nE\\n? Z ?X\\n? .\\n= ?? ? (Z) ? X, Z? ? X,\\n\\nD\\nE\\n? Z ?X\\n? =\\n? and X,\\nUsing the second equation in AMD0w,? , we have ?? ? (Z) ? X = a1 X,\\n\\n\\n\\u000b\\n?\\n? Z ?X\\n?\\na ?? ? (Z) ? ?? ? (X),\\nCombining, we have\\nD\\nE ? 0 by monotonicity of ?? .\\n?\\nd\\n?\\n?\\n?\\nD? (Z, X) ? ? X, ?f (X) , and we can finally bound the derivative of Er :\\ndt\\n\\na\\n\\nD\\nE 1 d\\n0\\nd\\n? ? r D?? (Z, X)\\n?\\nEr (t) = ?f (X), X? +\\nD?? (Z, X)\\n2\\ndt\\nr\\ndt\\nr\\nD\\nE\\u0010\\n? \\u0011\\n? ?f (X), X?\\n1?\\n.\\nar\\n\\nTherefore condition (2) of Theorem 2 implies that\\n\\nd\\ndt Er (t)\\n\\n? 0.\\n\\nThis energy function can be interpreted, loosely speaking, as the sum of a potential energy given by\\n1\\n? Indeed, when the problem is unconstrained,\\nf (X), and a kinetic energy given by r(t)\\nD?? (Z, X):\\n? =\\nthen one can take ? ? (z) = 12 kzk2 , in which case ?? ? = ?? = I, the identity, and D?? (Z, X)\\n?\\n1 ?\\nkZ ? Xk2 = 1 k X k2 , a quantity proportional to the kinetic energy.\\n2\\n\\n3\\n\\n2\\n\\na\\n\\nPrimal Representation and Example Dynamics\\n\\nAn equivalent primal representation can be obtained by rewriting the equations in terms of Z? =\\n?? ? (Z) and its derivatives (Z? is a primal variable that remains in X , since ?? ? maps into X ).\\nIn this section, we assume that ? ? is twice differentiable on E ? . Taking the time derivative of\\n? = ?? ? (Z(t)), we have\\nZ(t)\\n??\\n?\\n?\\nZ(t)\\n= ?2 ? ? (Z(t))Z(t)\\n= ??(t)?2 ? ? ? ??(Z(t))?f\\n(X(t)),\\n2\\n\\n?\\n\\nwhere ?2 ? ? (z) is the Hessian of ? ? at z, defined as ?2 ? ? (z)ij = ??z?j ?z(z)\\n. Then using the averaging\\ni\\nexpression for X, we can write AMDw,? in the following primal form\\n\\u0012\\n\\u0013\\n?\\nR\\n? )d?\\nx0 W (t0 )+ tt w(? )Z(?\\n? Z(t)\\n?\\n2 ?\\n0\\n?\\n?\\n=\\n??(t)?\\n?\\n?\\n??(\\nZ(t))?f\\nW (t)\\nAMDpw,?\\n? ?\\nZ(t0 ) = x0 .\\n\\n(6)\\n\\nA similar derivation can be made for the mirror descent ODE without acceleration, which can be\\nwritten as follows [11] (see also the original derivation of Nemirovski and Yudin in Chapter 3 in [13])\\n?\\n?\\n?\\n= ??f (X(t))\\n? Z(t)\\nMD\\nX(t) = ?? ? (Z(t))\\n?\\n? X(t ) = x .\\n0\\n0\\n\\nNote that this can be interpreted as a limit case of AMD?,w with ?(t) ? 1 and w(t) a Dirac function\\n?\\n?\\nat t. Taking the time derivative of X(t) = ?? ? (Z(t)), we have X(t)\\n= ?2 ? ? (Z(t))Z(t),\\nwhich\\nleads to the primal form of the mirror descent ODE\\n(\\nMD\\n\\np\\n\\n?\\nX(t)\\n= ??2 ? ? ? ??(X(t))?f (X(t))\\nX(t0 ) = x0 .\\n\\n5\\n\\n(7)\\n\\n\\fThe operator ?2 ? ? ? ?? appears in both primal representations (6) and (7), and multiplies the\\ngradient of f . It can be thought of as a transformation of the gradient which ensures that the primal\\ntrajectory remains in the feasible set, this is illustrated in the supplementary material. For some\\nchoices of ?, ?2 ? ? ? ?? has a simple expression. We give two examples below.\\n\\nWe also observe that in its primal form, AMDpw,? is a generalization of the ODE family studied\\nd\\n?\\nin [23], which can be written as dt\\n??(X(t) + e??(t) X(t))\\n= ?e?(t)+?(t) ?f (X(t)), for which\\n??(t)\\nthey prove the convergence rate O(e\\n). This corresponds to setting, in our notation, a(t) = e?(t) ,\\n?(t)\\nr(t) = e\\nand taking ?(t) = a(t)r(t) (which corresponds to the condition of Corollary 1).\\nn\\nPositive-orthant-constrained dynamics\\northant\\n+ , and consider\\nP Suppose that X is the positive\\nP zR\\n?\\ni ?1\\nthe negative entropy function ?(x) = i xi ln xi . Then its dual is ? (z) = i e\\n, and we have\\n??(x)i = 1 + ln xi and ?2 ? ? (z)i,j = ?ij ezi ?1 , where ?ij is 1 if i = j and 0 otherwise. Thus for all\\nx ? Rn+ , ?2 ? ? ? ??(x) = diag(x). Therefore, the primal forms (7) and (6), reduce to, respectively,\\n(\\n\\u001a\\n?i, X? i = ?Xi ?f (X)i\\n?i, Z?? i = ??(t)Z?i ?f (X)i\\n? 0 ) = x0\\nX(0) = x0\\nZ(t\\n\\nwhere for the second ODE we write X compactly to denote the weighted average given by the second\\nequation of AMDw,? . When f is affine, the mirror descent ODE lead to Lotka-Volterra equation\\nwhich has applications in economics and ecology. For the mirror descent ODE, one can verify that\\nthe solution remains in the positive orthant since X? tends to 0 as Xi approaches the boundary of the\\nfeasible set. Similarly for the accelerated version, Z?? tends to 0 as Z? approaches the boundary, thus Z?\\nremains feasible, and so does X by convexity.\\nSimplex-constrained dynamics:\\nPn the replicator equation. Now suppose that X is the n-simplex,\\nn\\nX\\n=\\n?\\n=\\n{x\\n?\\nR\\n:\\n+\\ni=1 xi = 1}. Consider the distance-generating function ?(x) =\\nPn\\nx\\nln\\nx\\n+\\n?\\n(x),\\nwhere\\ni\\ni\\nX\\ni=1\\nPn ?X (?) is the convex indicator function of the feasible set. Then its\\nconjugate is ? ? (z) = ln ( i=1 ezi ), defined on E ? , and we have ??(x)i = 1 + ln xi , ?? ? (z)i =\\nzi\\nPe z\\nk\\nke\\n\\n?j x\\nPi i\\nk xk\\n\\n(\\n\\n, and ?2 ? ? (z)ij =\\n\\n?\\n\\n(\\n\\nxi xj\\nP\\n2\\nk xk )\\n\\n? j ez i\\nPi z\\nk\\nke\\n\\n?\\n\\nezi ezj\\nP z 2.\\nk)\\nke\\n\\n(\\n\\nThen it is simple to calculate ?2 ? ? ? ??(x)ij =\\n\\n= ?ij xi ? xi xj . Therefore, the primal forms (7) and (6) reduce to, respectively,\\n\\n?i, X? i + Xi (?f (X)i ? hX, ?f (X)i) = 0\\nX(0) = x0\\n\\n(\\n\\n\\n\\n\\u000b\\u0001\\n? ?f (X) = 0\\n?i, Z?? i + ?(t)Z?i ?f (X)i ? Z,\\n?\\nZ(0)\\n= x0 .\\n\\nThe first ODE is known as the replicator dynamics [19], and has many applications in evolutionary\\ngame theory [22] and viability theory [4], among others. See the supplementary material for additional\\ndiscussion on the interpretation and applications of the replicator dynamics. This example shows that\\nthe replicator dynamics can be accelerated simply by performing the original replicator update on the\\n? in which (i) the gradient of the objective function is scaled by ?(t) at time t, and (ii) the\\nvariable Z,\\ngradient is evaluated at X(t), the weighted average of the Z? trajectory.\\n\\n4\\n\\nAdaptive Averaging Heuristic\\n\\nIn this section, we propose an adaptive averaging heuristic for Dadaptively computing\\nE \\u0010 the weights\\n\\u0011 w.\\n?(t)\\n?(t)\\n?\\nNote that in Corollary 1, we simply set a(t) = r(t) so that ?f (X(t)), X(t) r(t) ? a(t) is\\nidentically zero (thus trivially satisfying condition (2) of Theorem 2). However, from the bound (3),\\nif this term is negative, then this helps further decrease the Lyapunov function Lr (as well as the\\nenergy function Er ). A simple strategy is then to adaptively choose a(t) as follows\\nD\\nE\\n(\\n?\\na(t) = ?(t)\\nif ?f (X(t)), X(t)\\n> 0,\\nr(t)\\n(8)\\n?(t)\\na(t) ? r(t) otherwise.\\nIf we further have ?(t) ? r0 (t), then the conditions of Theorem 2 and Theorem 3 are satisfied, which\\nguarantee that Lr is a Lyapunov function and that the energy Er decreases. In particular, such a\\nheuristic would preserve the convergence rate r(t) by Theorem 2.\\n6\\n\\n\\fWe now propose a discrete version of the heuristic when r(t) = t2 . We consider the quadratic rate\\nin particular since in this case the discretization proposed by [11] preserves the quadratic rate, and\\ncorresponds to a first-order accelerated method2 for which many heuristics have been developed,\\nsuch as the restarting heuristics [17, 20] discussed in the introduction. To satisfy condition (1) of\\n?\\nTheorem 2, we choose ?(t) = ?t with ? ? 2. Note that in this case, ?(t)\\nr(t) = t . In the supplementary\\n?\\nmaterial, we propose a discretization of the heuristic (8), using the correspondance t = k s, for a\\nstep size s. The resulting algorithm is summarized in Algorithm 1, where ? ? is a smooth distance\\ngenerating function, and R is a regularizer assumed to be strongly convex and smooth. We give a\\nbound on the convergence rate of Algorithm 1 in the supplementary material. The proof relies on a\\ndiscrete counterpart of the Lyapunov function Lr .\\n?\\nThe algorithm keeps ak = ak?1 whenever f (?\\nx(k+1) ) ? f (?\\nx(k) ), and sets ak to k?\\notherwise. This\\ns\\nresults in a non-increasing sequence ak . It is worth observing that in continuous time, from the\\nexpression (4), a constant a(t) over an interval [t1 , t2 ] corresponds to an exponential increase in\\nthe weight w(t) over that interval, while a(t) = ?t corresponds to a polynomial increase w(t) =\\n(t/t0 )??1 . Intuitively, adaptive averaging increases the weights w(t) on portions of the trajectory\\nwhich make progress.\\n\\nAlgorithm 1 Accelerated mirror descent with adaptive averaging\\n1: Initialize x\\n?(0) = x0 , z?(0) = x0 , a1 = ??s\\n2: for k ? N do\\nD\\nE\\n3:\\nz?(k+1) = arg minz??X ?ks ?f (x(k) ), z? + D? (?\\nz , z?(k) ).\\nD\\nE\\n4:\\nx\\n?(k+1) = arg minx??X ?s ?f (x(k) ), x\\n? + R(?\\nx, x(k) )\\n6:\\n\\nx(k+1) = ?k+1 z?(k+1) + (1 ? ?k+1 )?\\nx(k+1) , with ?k =\\n\\u0010\\n\\u0011\\nmax\\nak = min ak?1 , ?k?s\\n\\n7:\\n8:\\n\\nif f (?\\nx(k+1) ) ? f (?\\nx(k) ) > 0 then\\n?\\n?\\nak = k s\\n\\n5:\\n\\n5\\n\\n?\\nsa\\n? k .\\n1+ sak\\n\\nNumerical Experiments\\n\\nIn this section, we compare our adaptive averaging heuristic (in its discrete version given in Algorithm 1) to existing restarting heuristics. We consider simplex-constrained problems and take\\nthe distance generating function ? to be the entropy function, so that the resulting algorithm is a\\ndiscretization of the accelerated replicator ODE studied in Section 3. We perform the experiments in\\nR3 so that we can visualize the solution trajectories (the supplementary material contains additional\\nexperiments in higher dimension). We consider different objective functions: A strongly convex\\nquadratic given by f (x) = (x ? s)T A(x ? s) for a positive definite matrix A, a weakly convex\\nquadratic, a linear function f (x) = cT x, and the Kullback-Leibler divergence, f (x) = DKL (x? , x).\\nWe compare the following methods:\\n1. The original accelerated mirror descent method (in which the weights follow a predetermined\\n?\\nschedule given by ak = k?\\n),\\ns\\n2. Our adaptive averaging, in which ak is computed adaptively following Algorithm 1,\\n3. The gradient restarting\\nheuristic in [17], in \\u000bwhich the algorithm is restarted from the current\\n\\n\\npoint whenever ?f (x(k) ), x(k+1) ? x(k) > 0,\\n4. The speed restarting heuristic in [20], in which the algorithm is restarted from the current\\npoint whenever kx(k+1) ? x(k) k ? kx(k) ? x(k?1) k.\\nThe results are shown in Figure 2. Each subfigure is divided into four plots: Clockwise from the top\\nleft, we show the value of the objective function, the trajectory on the simplex, the value of the energy\\nfunction Er and the value of the Lyapunov function Lr .\\n2\\nFor faster rates r(t) = tp , p > 2, it is possible to discretize the ODE and preserve the convergence rate, as\\nproposed by Wibisono et al. [23], however this discretization results in a higher-order method such as Nesterov?s\\ncubic accelerated Newton method [16].\\n\\n7\\n\\n\\fThe experiments show that adaptive averaging compares favorably to the restarting heuristics on\\nall these examples, with a significant improvement in the strongly convex case. Additionally, the\\nexperiments confirm that under the adaptive averaging heuristic, the Lyapunov function is decreasing.\\nThis is not the case for the restarting heuristics as can be seen on the weakly convex example. It is\\ninteresting to observe, however, that the energy function Er is non-increasing for all the methods\\nin our experiments. If we interpret the energy as the sum of a potential and a kinetic term, then this\\ncould be explained intuitively by the fact that restarting keeps the potential energy constant, and\\ndecreases the kinetic energy (since the velocity is reset to zero). It is also worth observing that even\\nthough the Lyapunov function Lr is non-decreasing, it will not necessarily converge to 0 when there\\nis more than one minimizer (its limit will depend on the choice of z ? in the definition of Lr ).\\nFinally, we observe that the methods have a different qualitative behavior: The original accelerated\\nmethod typically exhibits oscillations around the set of minimizers. The heuristics alleviate these\\noscillations in different ways: Intuitively, adaptive averaging acts by increasing the weights on\\nportions of the trajectory which make the most progress, while the restarting heuristics reset the\\nvelocity to zero whenever the algorithm detects that the trajectory is moving in a bad direction. The\\nspeed restarting heuristic seems to be more conservative in that it restarts more frequently.\\n\\n(a) Strongly convex quadratic.\\n\\n(b) Weakly convex function.\\n\\n(c) Linear function.\\n\\n(d) KL divergence.\\n\\nFigure 2: Examples of accelerated descent with adaptive averaging and restarting.\\n\\n6\\n\\nConclusion\\n\\nMotivated by the averaging formulation of accelerated mirror descent, we studied a family of ODEs\\nwith a generalized averaging scheme, and gave simple sufficient conditions on the weight functions to\\nguarantee a given convergence rate in continuous time. We showed as an example how the replicator\\nODE can be accelerated by averaging. Our adaptive averaging heuristic preserves the convergence\\nrate (since it preserves the Lyapunov function), and it seems to perform at least as well as other\\nheuristics for first-order accelerated methods, and in some cases considerably better. This encourages\\nfurther investigation into the performance of this adaptive averaging, both theoretically (by attempting\\nto prove faster rates, e.g. for strongly convex functions), and numerically, by testing it on other\\nmethods, such as the higher-order accelerated methods proposed in [23].\\n8\\n\\n\\fReferences\\n[1] H. Attouch and J. Peypouquet. The rate of convergence of nesterov?s accelerated forwardbackward method is actually faster than 1/k 2 . SIAM Journal on Optimization, 26(3):1824?1834,\\n2016.\\n[2] H. Attouch, J. Peypouquet, and P. Redont. Fast convergence of an inertial gradient-like system\\nwith vanishing viscosity. CoRR, abs/1507.04782, 2015.\\n[3] H. Attouch, J. Peypouquet, and P. Redont. Fast convex optimization via inertial dynamics with\\nhessian driven damping. CoRR, abs/1601.07113, 2016.\\n[4] J.-P. Aubin. Viability Theory. Birkhauser Boston Inc., Cambridge, MA, USA, 1991.\\n[5] A. Bloch, editor. Hamiltonian and gradient flows, algorithms, and control. American Mathematical Society, 1994.\\n[6] A. A. Brown and M. C. Bartholomew-Biggs. Some effective methods for unconstrained\\noptimization based on the solution of systems of ordinary differential equations. Journal of\\nOptimization Theory and Applications, 62(2):211?224, 1989.\\n[7] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and\\nstochastic optimization. J. Mach. Learn. Res., 12:2121?2159, July 2011.\\n[8] N. Flammarion and F. R. Bach. From averaging to acceleration, there is only a step-size. In\\n28th Conference on Learning Theory, COLT, pages 658?695, 2015.\\n[9] U. Helmke and J. Moore. Optimization and dynamical systems. Communications and control\\nengineering series. Springer-Verlag, 1994.\\n[10] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Proceedings of the\\n3rd International Conference on Learning Representations (ICLR), 2014.\\n[11] W. Krichene, A. Bayen, and P. Bartlett. Accelerated mirror descent in continuous and discrete\\ntime. In NIPS, 2015.\\n[12] A. Lyapunov. General Problem of the Stability Of Motion. Control Theory and Applications\\nSeries. Taylor & Francis, 1992.\\n[13] A. S. Nemirovsky and D. B. Yudin. Problem Complexity and Method Efficiency in Optimization.\\nWiley-Interscience series in discrete mathematics. Wiley, 1983.\\n[14] Y. Nesterov. A method of solving a convex programming problem with convergence rate o(1/k2).\\nSoviet Mathematics Doklady, 27(2):372?376, 1983.\\n[15] Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming, 103\\n(1):127?152, 2005.\\n[16] Y. Nesterov. Accelerating the cubic regularization of newton?s method on convex problems.\\nMathematical Programming, 112(1):159?181, 2008.\\n[17] B. O?Donoghue and E. Cand?s. Adaptive restart for accelerated gradient schemes. Foundations\\nof Computational Mathematics, 15(3):715?732, 2015. ISSN 1615-3375.\\n[18] R. Rockafellar. Convex Analysis. Princeton University Press, 1970.\\n[19] K. Sigmund. Complexity, Language, and Life: Mathematical Approaches, chapter A Survey of\\nReplicator Equations, pages 88?104. Springer Berlin Heidelberg, Berlin, Heidelberg, 1986.\\n[20] W. Su, S. Boyd, and E. Cand?s. A differential equation for modeling Nesterov?s accelerated\\ngradient method: Theory and insights. In NIPS, 2014.\\n[21] G. Teschl. Ordinary differential equations and dynamical systems, volume 140. American\\nMathematical Soc., 2012.\\n[22] J. W. Weibull. Evolutionary game theory. MIT press, 1997.\\n[23] A. Wibisono, A. C. Wilson, and M. I. Jordan. A variational perspective on accelerated methods\\nin optimization. CoRR, abs/1603.04245, 2016.\\n9\\n\\n\\f\",\n          \"Max-Margin Deep Generative Models\\n\\nChongxuan Li? , Jun Zhu? , Tianlin Shi? , Bo Zhang?\\n?\\nDept. of Comp. Sci. & Tech., State Key Lab of Intell. Tech. & Sys., TNList Lab,\\nCenter for Bio-Inspired Computing Research, Tsinghua University, Beijing, 100084, China\\n?\\nDept. of Comp. Sci., Stanford University, Stanford, CA 94305, USA\\n{licx14@mails., dcszj@, dcszb@}tsinghua.edu.cn; stl501@gmail.com\\n\\nAbstract\\nDeep generative models (DGMs) are effective on learning multilayered representations of complex data and performing inference of input data by exploring the\\ngenerative ability. However, little work has been done on examining or empowering the discriminative ability of DGMs on making accurate predictions. This paper presents max-margin deep generative models (mmDGMs), which explore the\\nstrongly discriminative principle of max-margin learning to improve the discriminative power of DGMs, while retaining the generative capability. We develop an\\nefficient doubly stochastic subgradient algorithm for the piecewise linear objective. Empirical results on MNIST and SVHN datasets demonstrate that (1) maxmargin learning can significantly improve the prediction performance of DGMs\\nand meanwhile retain the generative ability; and (2) mmDGMs are competitive to\\nthe state-of-the-art fully discriminative networks by employing deep convolutional\\nneural networks (CNNs) as both recognition and generative models.\\n\\n1\\n\\nIntroduction\\n\\nMax-margin learning has been effective on learning discriminative models, with many examples\\nsuch as univariate-output support vector machines (SVMs) [5] and multivariate-output max-margin\\nMarkov networks (or structured SVMs) [30, 1, 31]. However, the ever-increasing size of complex\\ndata makes it hard to construct such a fully discriminative model, which has only single layer of\\nadjustable weights, due to the facts that: (1) the manually constructed features may not well capture\\nthe underlying high-order statistics; and (2) a fully discriminative approach cannot reconstruct the\\ninput data when noise or missing values are present.\\nTo address the first challenge, previous work has considered incorporating latent variables into\\na max-margin model, including partially observed maximum entropy discrimination Markov networks [37], structured latent SVMs [32] and max-margin min-entropy models [20]. All this work\\nhas primarily focused on a shallow structure of latent variables. To improve the flexibility, learning SVMs with a deep latent structure has been presented in [29]. However, these methods do not\\naddress the second challenge, which requires a generative model to describe the inputs. The recent work on learning max-margin generative models includes max-margin Harmoniums [4], maxmargin topic models [34, 35], and nonparametric Bayesian latent SVMs [36] which can infer the\\ndimension of latent features from data. However, these methods only consider the shallow structure\\nof latent variables, which may not be flexible enough to describe complex data.\\nMuch work has been done on learning generative models with a deep structure of nonlinear hidden\\nvariables, including deep belief networks [25, 16, 23], autoregressive models [13, 9], and stochastic\\nvariations of neural networks [3]. For such models, inference is a challenging problem, but fortunately there exists much recent progress on stochastic variational inference algorithms [12, 24].\\nHowever, the primary focus of deep generative models (DGMs) has been on unsupervised learning,\\n1\\n\\n\\fwith the goals of learning latent representations and generating input samples. Though the latent\\nrepresentations can be used with a downstream classifier to make predictions, it is often beneficial\\nto learn a joint model that considers both input and response variables. One recent attempt is the\\nconditional generative models [11], which treat labels as conditions of a DGM to describe input\\ndata. This conditional DGM is learned in a semi-supervised setting, which is not exclusive to ours.\\nIn this paper, we revisit the max-margin principle and present a max-margin deep generative model\\n(mmDGM), which learns multi-layer representations that are good for both classification and input inference. Our mmDGM conjoins the flexibility of DGMs on describing input data and the\\nstrong discriminative ability of max-margin learning on making accurate predictions. We formulate\\nmmDGM as solving a variational inference problem of a DGM regularized by a set of max-margin\\nposterior constraints, which bias the model to learn representations that are good for prediction. We\\ndefine the max-margin posterior constraints as a linear functional of the target variational distribution of the latent presentations. Then, we develop a doubly stochastic subgradient descent algorithm,\\nwhich generalizes the Pagesos algorithm [28] to consider nontrivial latent variables. For the variational distribution, we build a recognition model to capture the nonlinearity, similar as in [12, 24].\\nWe consider two types of networks used as our recognition and generative models: multiple layer\\nperceptrons (MLPs) as in [12, 24] and convolutional neural networks (CNNs) [14]. Though CNNs\\nhave shown promising results in various domains, especially for image classification, little work has\\nbeen done to take advantage of CNN to generate images. The recent work [6] presents a type of\\nCNN to map manual features including class labels to RBG chair images by applying unpooling,\\nconvolution and rectification sequentially; but it is a deterministic mapping and there is no random\\ngeneration. Generative Adversarial Nets [7] employs a single such layer together with MLPs in a\\nminimax two-player game framework with primary goal of generating images. We propose to stack\\nthis structure to form a highly non-trivial deep generative network to generate images from latent\\nvariables learned automatically by a recognition model using standard CNN. We present the detailed\\nnetwork structures in experiments part. Empirical results on MNIST [14] and SVHN [22] datasets\\ndemonstrate that mmDGM can significantly improve the prediction performance, which is competitive to the state-of-the-art methods [33, 17, 8, 15], while retaining the capability of generating input\\nsamples and completing their missing values.\\n\\n2\\n\\nBasics of Deep Generative Models\\n\\nWe start from a general setting, where we have N i.i.d. data X = {xn }N\\nn=1 . A deep generative\\nmodel (DGM) assumes that each xn ? RD is generated from a vector of latent variables zn ? RK ,\\nwhich itself follows some distribution. The joint probability of a DGM is as follows:\\np(X, Z|?, ?) =\\n\\nN\\nY\\n\\np(zn |?)p(xn |zn , ?),\\n\\n(1)\\n\\nn=1\\n\\nwhere p(zn |?) is the prior of the latent variables and p(xn |zn , ?) is the likelihood model for generating observations. For notation simplicity, we define ? = (?, ?). Depending on the structure\\nof z, various DGMs have been developed, such as the deep belief networks [25, 16], deep sigmoid\\nnetworks [21], deep latent Gaussian models [24], and deep autoregressive models [9]. In this paper,\\nwe focus on the directed DGMs, which can be easily sampled from via an ancestral sampler.\\nHowever, in most cases learning DGMs is challenging due to the intractability of posterior inference.\\nThe state-of-the-art methods resort to stochastic variational methods under the maximum likelihood\\n? = argmax log p(X|?). Specifically, let q(Z) be the variational\\nestimation (MLE) framework, ?\\n?\\ndistribution that approximates the true posterior p(Z|X, ?). A variational upper bound of the per\\nsample negative log-likelihood (NLL) ? log p(xn |?, ?) is:\\nL(?, q(zn ); xn ) , KL(q(zn )||p(zn |?)) ? Eq(zn ) [log p(xn |zn , ?)],\\n\\n(2)\\n\\nwhere KL(q||p) is the Kullback-Leibler (KL) divergence between distributions q and p. Then,\\nP\\nL(?, q(Z); X) , n L(?, q(zn ); xn ) upper bounds the full negative log-likelihood ? log p(X|?).\\nIt is important to notice that if we do not make restricting assumption on the variational distribution\\nq, the lower bound is tight by simply setting q(Z) = p(Z|X, ?). That is, the MLE is equivalent to\\nsolving the variational problem: min?,q(Z) L(?, q(Z); X). However, since the true posterior is intractable except a handful of special cases, we must resort to approximation methods. One common\\n2\\n\\n\\fassumption is that the variational distribution is of some parametric form, q? (Z), and then we optimize the variational bound w.r.t the variational parameters ?. For DGMs, another challenge arises\\nthat the variational bound is often intractable to compute analytically. To address this challenge, the\\nearly work further bounds the intractable parts with tractable ones by introducing more variational\\nparameters [26]. However, this technique increases the gap between the bound being optimized and\\nthe log-likelihood, potentially resulting in poorer estimates. Much recent progress [12, 24, 21] has\\nbeen made on hybrid Monte Carlo and variational methods, which approximates the intractable expectations and their gradients over the parameters (?, ?) via some unbiased Monte Carlo estimates.\\nFurthermore, to handle large-scale datasets, stochastic optimization of the variational objective can\\nbe used with a suitable learning rate annealing scheme. It is important to notice that variance reduction is a key part of these methods in order to have fast and stable convergence.\\nMost work on directed DGMs has been focusing on the generative capability on inferring the observations, such as filling in missing values [12, 24, 21], while little work has been done on investigating\\nthe predictive power, except the semi-supervised DGMs [11] which builds a DGM conditioned on\\nthe class labels and learns the parameters via MLE. Below, we present max-margin deep generative\\nmodels, which explore the discriminative max-margin principle to improve the predictive ability of\\nthe latent representations, while retaining the generative capability.\\n\\n3\\n\\nMax-margin Deep Generative Models\\n\\nWe consider supervised learning, where the training data is a pair (x, y) with input features x ? RD\\nand the ground truth label y. Without loss of generality, we consider the multi-class classification,\\nwhere y ? C = {1, . . . , M }. A max-margin deep generative model (mmDGM) consists of two\\ncomponents: (1) a deep generative model to describe input features; and (2) a max-margin classifier\\nto consider supervision. For the generative model, we can in theory adopt any DGM that defines a\\njoint distribution over (X, Z) as in Eq. (1). For the max-margin classifier, instead of fitting the input\\nfeatures into a conventional SVM, we define the linear classifier on the latent representations, whose\\nlearning will be regularized by the supervision signal as we shall see. Specifically, if the latent\\nrepresentation z is given, we define the latent discriminant function F (y, z, ?; x) = ? > f (y, z),\\nwhere f (y, z) is an M K-dimensional vector that concatenates M subvectors, with the yth being z\\nand all others being zero, and ? is the corresponding weight vector.\\nWe consider the case that ? is a random vector, following some prior distribution p0 (?). Then\\nour goal is to infer the posterior distribution p(?, Z|X, Y), which is typically approximated by a\\nvariational distribution q(?, Z) for computational tractability. Notice that this posterior is different\\nfrom the one in the vanilla DGM. We expect that the supervision information will bias the learned\\nrepresentations to be more powerful on predicting the labels at testing. To account for the\\n\\u0002 uncertainty\\n\\u0003\\nof (?, Z), we take the expectation and define the discriminant function F (y; x) = Eq ? > f (y, z) ,\\nand the final prediction rule that maps inputs to outputs is:\\ny? = argmax F (y; x).\\n\\n(3)\\n\\ny?C\\n\\nNote that different from the conditional DGM [11], which puts the class labels upstream, the above\\nclassifier is a downstream model, in the sense that the supervision signal is determined by conditioning on the latent representations.\\n3.1\\n\\nThe Learning Problem\\n\\nWe want to jointly learn the parameters ? and infer the posterior distribution q(?, Z). Based on the\\nequivalent variational formulation of MLE, we define the joint learning problem as solving:\\nmin\\n\\nL(?, q(?, Z); X) + C\\n\\n?,q(?,Z),?\\n\\n?n\\n\\n(4)\\n\\nn=1\\n\\n\\u001a\\n?n, y ? C, s.t. :\\n\\nN\\nX\\n\\nEq [? > ?fn (y)] ? ?ln (y) ? ?n\\n?n ? 0,\\n\\nwhere ?fn (y) = f (yn , zn ) ? f (y, zn ) is the difference of the feature vectors; ?ln (y) is the loss\\nfunction that measures the cost to predict y if the true label is yn ; and C is a nonnegative regularization parameter balancing the two components. In the objective, the variational bound is defined\\n3\\n\\n\\fas L(?, q(?, Z); X) = KL(q(?, Z)||p0 (?, Z|?)) ? Eq [log p(X|Z, ?)], and the margin constraints\\nare from the classifier (3). If we ignore the constraints (e.g., setting C at 0), the solution of q(?, Z)\\nwill be exactly the Bayesian posterior, and the problem is equivalent to do MLE for ?.\\nBy absorbing the slack variables, we can rewrite the problem in an unconstrained form:\\nmin L(?, q(?, Z); X) + CR(q(?, Z; X)),\\n\\n(5)\\n\\n?,q(?,Z)\\n\\nPN\\nwhere the hinge loss is: R(q(?, Z); X) = n=1 maxy?C (?ln (y) ? Eq [? > ?fn (y)]). Due to the\\nconvexity of max function, it is easy to verify P\\nthat the hinge loss is an upper bound of the training error of classifier (3), that is, R(q(?, Z); X) ? n ?ln (?\\nyn ). Furthermore, the hinge loss is a convex\\nfunctional over the variational distribution because of the linearity of the expectation operator. These\\nproperties render the hinge loss as a good surrogate to optimize over. Previous work has explored\\nthis idea to learn discriminative topic models [34], but with a restriction on the shallow structure of\\nhidden variables. Our work presents a significant extension to learn deep generative models, which\\npose new challenges on the learning and inference.\\n3.2\\n\\nThe Doubly Stochastic Subgradient Algorithm\\n\\nThe variational formulation of problem (5) naturally suggests that we can develop a variational\\nalgorithm to address the intractability of the true posterior. We now present a new algorithm to\\nsolve problem (5). Our method is a doubly stochastic generalization of the Pegasos (i.e., Primal\\nEstimated sub-GrAdient SOlver for SVM) algorithm [28] for the classic SVMs with fully observed\\ninput features, with the new extension of dealing with a highly nontrivial structure of latent variables.\\nFirst, we make the structured mean-field (SMF) assumption that q(?, Z) = q(?)q? (Z). Under the\\nassumption, we have the discriminant function as Eq [? > ?fn (y)] = Eq(?) [? > ]Eq? (z(n) ) [?fn (y)].\\nMoreover, we can solve for the optimal solution of q(?) in some analytical form. In fact,\\nby the calculus\\nof variations, we can\\n\\u0011 show that given the other parts the solution is q(?) ?\\n\\u0010 P\\n>\\ny\\np0 (?) exp ?\\nn,y ?n Eq? [?fn (y)] , where ? are the Lagrange multipliers (See [34] for de2\\ntails). If the prior is normal,\\nP p0 (?) = N (0, ? I), we have the normal posterior: q(?) =\\nN (?, ? 2 I), where ? = ? 2 n,y ?ny Eq? [?fn (y)]. Therefore, even though we did not make a parametric form assumption of q(?), the above results show that the optimal posterior distribution of ?\\nis Gaussian. Since we only use the expectation in the optimization problem and in prediction, we\\ncan directly solve for the mean parameter ? instead of q(?). Further, in this case we can verify that\\n2\\nKL(q(?)||p0 (?)) = ||?||\\n2? 2 and then the equivalent objective function in terms of ? can be written\\nas:\\n||?||2\\nmin L(?, ?; X) +\\n+ CR(?, ?; X),\\n(6)\\n?,?,?\\n2? 2\\nPN\\nwhere R(?, ?; X) =\\nn=1 `(?, ?; xn ) is the total hinge loss, and the per-sample hinge-loss is\\n`(?, ?; xn ) = maxy?C (?ln (y) ? ?> Eq? [?fn (y)]). Below, we present a doubly stochastic subgradient descent algorithm to solve this problem.\\n\\nThe first stochasticity arises from a stochastic estimate of the objective by random mini-batches.\\nSpecifically, the batch learning needs to scan the full dataset to compute subgradients, which is\\noften too expensive to deal with large-scale datasets. One effective technique is to do stochastic\\nsubgradient descent [28], where at each iteration we randomly draw a mini-batch of the training\\ndata and then do the variational updates over the small mini-batch. Formally, given a mini batch of\\nsize m, we get an unbiased estimate of the objective:\\nm\\nm\\nN X\\n||?||2\\nNC X\\nL?m :=\\nL(?, ?; xn ) +\\n+\\n`(?, ?; xn ).\\nm n=1\\n2? 2\\nm n=1\\n\\nThe second stochasticity arises from a stochastic estimate of the per-sample variational bound\\nand its subgradient, whose intractability calls for another Monte Carlo estimator. Formally, let\\nzln ? q? (z|xn , yn ) be a set of samples from the variational distribution, where we explicitly put the\\nconditions. Then, an estimate of the per-sample variational bound and the per-sample hinge-loss is\\n\\u0010\\n\\u0011\\nX\\nX\\n? ?; xn )=max ?ln (y)? 1 ?>?fn (y, zl ) ,\\n? ?; xn )= 1 log p(xn , zln |?)?log q? (zln ); `(?,\\nL(?,\\nn\\ny\\nL\\nL\\nl\\n\\nl\\n\\n4\\n\\n\\fwhere ?fn (y, zln ) = f (yn , zln ) ? f (y, zln ). Note that L? is an unbiased estimate of L, while `? is a\\nbiased estimate of `. Nevertheless, we can still show that `? is an upper bound estimate of ` under\\nexpectation. Furthermore, this biasedness does not affect our estimate of the gradient. In fact,\\nby using the equality ?? q? (z) = q? (z)?? log q? (z), we can construct an unbiased Monte Carlo\\nestimate of ?? (L(?, ?; xn ) + `(?, ?; xn )) as:\\nL\\n\\ng? =\\n\\n\\u0011\\n1 X\\u0010\\nlog p(zln , xn ) ? log q? (zln ) + C?> ?fn (?\\nyn , zln ) ?? log q? (zln ),\\nL\\n\\n(7)\\n\\nl=1\\n\\nwhere the last term roots\\nfrom the hinge loss with the loss-augmented prediction y?n =\\nP\\nargmaxy (?ln (y) + L1 l ?> f (y, zln )). For ? and ?, the estimates of the gradient ?? L(?, ?; xn )\\nand the subgradient ?? `(?, ?; xn ) are easier, which are:\\n\\u0001\\n1X\\n1X\\ng? =\\n?? log p(xn , zln |?), g? =\\nf (?\\nyn , zln ) ? f (yn , zln ) .\\nL\\nL\\nl\\n\\nl\\n\\nNotice that the sampling and the gradient\\nnot the underlying model.\\n\\n?? log q? (zln )\\n\\nonly depend on the variational distribution,\\n\\nThe above estimates consider the gen- Algorithm 1 Doubly Stochastic Subgradient Algorithm\\nInitialize ?, ?, and ?\\neral case where the variational bound is\\nrepeat\\nintractable. In some cases, we can comdraw a random mini-batch of m data points\\npute the KL-divergence term analytidraw random samples from noise distribution p(\\u000f)\\ncally, e.g., when the prior and the vari? ?, ?; Xm , \\u000f)\\national distribution are both Gaussian.\\ncompute subgradient g = ??,?,? L(?,\\nIn such cases, we only need to estimate\\nupdate parameters (?, ?, ?) using subgradient g.\\nthe rest intractable part by sampling,\\nuntil Converge\\nwhich often reduces the variance [12].\\nreturn ?, ?, and ?\\nSimilarly, we could use the expectation\\nof the features directly, if it can be computed analytically, in the computation of subgradients (e.g.,\\ng? and g? ) instead of sampling, which again can lead to variance reduction.\\nWith the above estimates of subgradients, we can use stochastic optimization methods such as\\nSGD [28] and AdaM [10] to update the parameters, as outlined in Alg. 1. Overall, our algorithm is\\na doubly stochastic generalization of Pegasos to deal with the highly nontrivial latent variables.\\nNow, the remaining question is how to define an appropriate variational distribution q? (z) to obtain\\na robust estimate of the subgradients as well as the objective. Two types of methods have been developed for unsupervised DGMs, namely, variance reduction [21] and auto-encoding variational Bayes\\n(AVB) [12]. Though both methods can be used for our models, we focus on the AVB approach. For\\ncontinuous variables Z, under certain mild conditions we can reparameterize the variational distribution q? (z) using some simple variables \\u000f. Specifically, we can draw samples \\u000f from some simple\\ndistribution p(\\u000f) and do the transformation z = g? (\\u000f, x, y) to get the sample of the distribution\\nq(z|x, y). We refer the readers to [12] for more details. In our experiments, we consider the special\\nGaussian case, where we assume that the variational distribution is a multivariate Gaussian with a\\ndiagonal covariance matrix:\\nq? (z|x, y) = N (?(x, y; ?), ? 2 (x, y; ?)),\\n\\n(8)\\n\\nwhose mean and variance are functions of the input data. This defines our recognition model. Then,\\nthe reparameterization trick is as follows: we first draw standard normal variables \\u000fl ? N (0, I) and\\nthen do the transformation zln = ?(xn , yn ; ?) + ?(xn , yn ; ?) \\f \\u000fl to get a sample. For simplicity,\\nwe assume that both the mean and variance are function of x only. However, it is worth to emphasize\\nthat although the recognition model is unsupervised, the parameters ? are learned in a supervised\\nmanner because the subgradient (7) depends on the hinge loss. Further details of the experimental\\nsettings are presented in Sec. 4.1.\\n\\n4\\n\\nExperiments\\n\\nWe now present experimental results on the widely adopted MNIST [14] and SVHN [22] datasets.\\nThough mmDGMs are applicable to any DGMs that define a joint distribution of X and Z, we\\n5\\n\\n\\fconcentrate on the Variational Auto-encoder (VA) [12], which is unsupervised. We denote our\\nmmDGM with VA by MMVA. In our experiments, we consider two types of recognition models:\\nmultiple layer perceptrons (MLPs) and convolutional neural networks (CNNs). We implement all\\nexperiments based on Theano [2]. 1\\n4.1\\n\\nArchitectures and Settings\\n\\nIn the MLP case, we follow the settings in [11] to compare both generative and discriminative\\ncapacity of VA and MMVA. In the CNN case, we use standard convolutional nets [14] with convolution and max-pooling operation as the recognition model to obtain more competitive classification\\nresults. For the generative model, we use unconvnets [6] with a ?symmetric? structure as the recognition model, to reconstruct the input images approximately. More specifically, the top-down generative model has the same structure as the bottom-up recognition model but replacing max-pooling\\nwith unpooling operation [6] and applies unpooling, convolution and rectification in order. The total\\nnumber of parameters in the convolutional network is comparable with previous work [8, 17, 15].\\nFor simplicity, we do not involve mlpconv layers [17, 15] and contrast normalization layers in our\\nrecognition model, but they are not exclusive to our model. We illustrate details of the network\\narchitectures in appendix A.\\nIn both settings, the mean and variance of the latent z are transformed from the last layer of the\\nrecognition model through a linear operation. It should be noticed that we could use not only the\\nexpectation of z but also the activation of any layer in the recognition model as features. The only\\ntheoretical difference is from where we add a hinge loss regularization to the gradient and backpropagate it to previous layers. In all of the experiments, the mean of z has the same nonlinearity\\nbut typically much lower dimension than the activation of the last layer in the recognition model,\\nand hence often leads to a worse performance. In the MLP case, we concatenate the activations of\\n2 layers as the features used in the supervised tasks. In the CNN case, we use the activations of the\\nlast layer as the features. We use AdaM [10] to optimize parameters in all of the models. Although it\\nis an adaptive gradient-based optimization method, we decay the global learning rate by factor three\\nperiodically after sufficient number of epochs to ensure a stable convergence.\\nWe denote our mmDGM with MLPs by MMVA. To perform classification using VA, we first learn\\nthe feature representations by VA, and then build a linear SVM classifier on these features using the\\nPegasos stochastic subgradient algorithm [28]. This baseline will be denoted by VA+Pegasos. The\\ncorresponding models with CNNs are denoted by CMMVA and CVA+Pegasos respectively.\\n4.2\\n\\nResults on the MNIST dataset\\n\\nWe present both the prediction performance and the results on generating samples of MMVA and\\nVA+Pegasos with both kinds of recognition models on the MNIST [14] dataset, which consists of\\nimages of 10 different classes (0 to 9) of size 28?28 with 50,000 training samples, 10,000 validating\\nsamples and 10,000 testing samples.\\nTable 1: Error rates (%) on MNIST dataset.\\nM ODEL\\nE RROR R ATE\\n4.2.1 Predictive Performance\\nVA+Pegasos\\n1.04\\nIn the MLP case, we only use 50,000 trainVA+Class-conditionVA\\n0.96\\ning data, and the parameters for classification are\\nMMVA\\n0.90\\noptimized according to the validation set. We\\nCVA+Pegasos\\n1.35\\nCMMVA\\n0.45\\nchoose C = 15 for MMVA and initialize it with\\nStochastic Pooling [33]\\n0.47\\nan unsupervised pre-training procedure in classiNetwork in Network [17]\\n0.47\\nfication. First three rows in Table 1 compare\\nMaxout Network [8]\\n0.45\\nVA+Pegasos, VA+Class-condtionVA and MMVA,\\nDSN\\n[15]\\n0.39\\nwhere VA+Class-condtionVA refers to the best fully\\nsupervised model in [11]. Our model outperforms the baseline significantly. We further use the\\nt-SNE algorithm [19] to embed the features learned by VA and MMVA on 2D plane, which again\\ndemonstrates the stronger discriminative ability of MMVA (See Appendix B for details).\\nIn the CNN case, we use 60,000 training data. Table 2 shows the effect of C on classification error\\nrate and variational lower bound. Typically, as C gets lager, CMMVA learns more discriminative\\nfeatures and leads to a worse estimation of data likelihood. However, if C is too small, the supervision is not enough to lead to predictive features. Nevertheless, C = 103 is quite a good trade-off\\n1\\n\\nThe source code is available at https://github.com/zhenxuan00/mmdgm.\\n\\n6\\n\\n\\f(a) VA\\n\\n(b) MMVA\\n\\n(c) CVA\\n\\n(d) CMMVA\\n\\nFigure 1: (a-b): randomly generated images by VA and MMVA, 3000 epochs; (c-d): randomly\\ngenerated images by CVA and CMMVA, 600 epochs.\\nbetween the classification performance and generative performance and this is the default setting\\nof CMMVA on MNIST throughout this paper. In this setting, the classification performance of our\\nCMMVA model is comparable to the recent state-of-the-art fully discriminative networks (without\\ndata augmentation), shown in the last four rows of Table 1.\\nTable 2: Effects of C on MNIST dataset\\n4.2.2 Generative Performance\\nwith a CNN recognition model.\\nWe further investigate the generative capability of MMVA C E RROR R ATE (%) L OWER B OUND\\non generating samples. Fig. 1 illustrates the images ran- 0\\n1.35\\n-93.17\\ndomly sampled from VA and MMVA models where we 1\\n1.86\\n-95.86\\noutput the expectation of the gray value at each pixel to 10\\n0.88\\n-95.90\\nget a smooth visualization. We do not pre-train our model 102\\n0.54\\n-96.35\\nin all settings when generating data to prove that MMVA 103\\n0.45\\n-99.62\\n(CMMVA) remains the generative capability of DGMs.\\n104\\n0.43\\n-112.12\\n4.3\\n\\nResults on the SVHN (Street View House Numbers) dataset\\n\\nSVHN [22] is a large dataset consisting of color images of size 32 ? 32. The task is to recognize\\ncenter digits in natural scene images, which is significantly harder than classification of hand-written\\ndigits. We follow the work [27, 8] to split the dataset into 598,388 training data, 6000 validating\\ndata and 26, 032 testing data and preprocess the data by Local Contrast Normalization (LCN).\\nWe only consider the CNN recognition model here. The network structure is similar to that in\\nMNIST. We set C = 104 for our CMMVA model on SVHN by default.\\nTable 3 shows the predictive performance.\\nIn\\nthis more challenging problem, we observe a\\nlarger improvement by CMMVA as compared to\\nCVA+Pegasos, suggesting that DGMs benefit a lot\\nfrom max-margin learning on image classification.\\nWe also compare CMMVA with state-of-the-art results. To the best of our knowledge, there is no competitive generative models to classify digits on SVHN\\ndataset with full labels.\\n\\nTable 3: Error rates (%) on SVHN dataset.\\nM ODEL\\nE RROR R ATE\\nCVA+Pegasos\\n25.3\\nCMMVA\\n3.09\\nCNN [27]\\n4.9\\nStochastic Pooling [33]\\n2.80\\nMaxout Network [8]\\n2.47\\nNetwork in Network [17]\\n2.35\\nDSN [15]\\n1.92\\n\\nWe further compare the generative capability of CMMVA and CVA to examine the benefits from\\njointly training of DGMs and max-margin classifiers. Though CVA gives a tighter lower bound\\nof data likelihood and reconstructs data more elaborately, it fails to learn the pattern of digits in a\\ncomplex scenario and could not generate meaningful images. Visualization of random samples from\\nCVA and CMMVA is shown in Fig. 2. In this scenario, the hinge loss regularization on recognition\\nmodel is useful for generating main objects to be classified in images.\\n4.4\\n\\nMissing Data Imputation and Classification\\n\\nFinally, we test all models on the task of missing data imputation. For MNIST, we consider two types\\nof missing values [18]: (1) Rand-Drop: each pixel is missing randomly with a pre-fixed probability;\\nand (2) Rect: a rectangle located at the center of the image is missing. Given the perturbed images,\\nwe uniformly initialize the missing values between 0 and 1, and then iteratively do the following\\nsteps: (1) using the recognition model to sample the hidden variables; (2) predicting the missing\\nvalues to generate images; and (3) using the refined images as the input of the next round. For\\nSVHN, we do the same procedure as in MNIST but initialize the missing values with Guassian\\n7\\n\\n\\f(a) Training data\\n\\n(b) CVA\\n\\n(c) CMMVA (C = 103 ) (d) CMMVA (C = 104 )\\n\\nFigure 2: (a): training data after LCN preprocessing; (b): random samples from CVA; (c-d):\\nrandom samples from CMMVA when C = 103 and C = 104 respectively.\\nrandom variables as the input distribution changes. Visualization results on MNIST and SVHN are\\npresented in Appendix C and Appendix D respectively.\\nIntuitively, generative models with CNNs\\nTable 4: MSE on MNIST data with missing values in\\ncould be more powerful on learning patthe testing procedure.\\nterns and high-level structures, while\\nVA\\nMMVA CVA CMMVA\\ngenerative models with MLPs lean more N OISE T YPE\\nR\\nAND -D ROP (0.2) 0.0109 0.0110 0.0111 0.0147\\nto reconstruct the pixels in detail. This\\nR AND -D ROP (0.4) 0.0127 0.0127 0.0127 0.0161\\nconforms to the MSE results shown in R AND -D ROP (0.6) 0.0168 0.0165 0.0175 0.0203\\nTable 4: CVA and CMMVA outperform R AND -D ROP (0.8) 0.0379 0.0358 0.0453 0.0449\\nVA and MMVA with a missing rectan- R ECT (6 ? 6)\\n0.0637 0.0645 0.0585 0.0597\\ngle, while VA and MMVA outperform R ECT (8 ? 8)\\n0.0850 0.0841 0.0754 0.0724\\nCVA and CMMVA with random miss- R ECT (10 ? 10) 0.1100 0.1079 0.0978 0.0884\\ning values. Compared with the baseline, R ECT (12 ? 12) 0.1450 0.1342 0.1299 0.1090\\nmmDGMs also make more accurate completion when large patches are missing. All of the models infer missing values for 100 iterations.\\nWe also compare the classification performance of CVA, CNN and CMMVA with Rect missing\\nvalues in testing procedure in Appendix E. CMMVA outperforms both CVA and CNN.\\nOverall, mmDGMs have comparable capability of inferring missing values and prefer to learn highlevel patterns instead of local details.\\n\\n5\\n\\nConclusions\\n\\nWe propose max-margin deep generative models (mmDGMs), which conjoin the predictive power\\nof max-margin principle and the generative ability of deep generative models. We develop a doubly\\nstochastic subgradient algorithm to learn all parameters jointly and consider two types of recognition\\nmodels with MLPs and CNNs respectively. In both cases, we present extensive results to demonstrate that mmDGMs can significantly improve the prediction performance of deep generative models, while retaining the strong generative ability on generating input samples as well as completing\\nmissing values. In fact, by employing CNNs in both recognition and generative models, we achieve\\nlow error rates on MNIST and SVHN datasets, which are competitive to the state-of-the-art fully\\ndiscriminative networks.\\nAcknowledgments\\nThe work was supported by the National Basic Research Program (973 Program) of China (Nos.\\n2013CB329403, 2012CB316301), National NSF of China (Nos. 61322308, 61332007), Tsinghua TNList Lab\\nBig Data Initiative, and Tsinghua Initiative Scientific Research Program (Nos. 20121088071, 20141080934).\\n\\nReferences\\n[1] Y. Altun, I. Tsochantaridis, and T. Hofmann. Hidden Markov support vector machines. In ICML, 2003.\\n[2] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. Goodfellow, A. Bergeron, N. Bouchard, D. WardeFarley, and Y. Bengio. Theano: new features and speed improvements. In Deep Learning and Unsupervised Feature Learning NIPS Workshop, 2012.\\n[3] Y. Bengio, E. Laufer, G. Alain, and J. Yosinski. Deep generative stochastic networks trainable by backprop. In ICML, 2014.\\n[4] N. Chen, J. Zhu, F. Sun, and E. P. Xing. Large-margin predictive latent subspace learning for multi-view\\ndata analysis. IEEE Trans. on PAMI, 34(12):2365?2378, 2012.\\n\\n8\\n\\n\\f[5] C. Cortes and V. Vapnik. Support-vector networks. Journal of Machine Learning, 20(3):273?297, 1995.\\n[6] A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning to generate chairs with convolutional neural\\nnetworks. arXiv:1411.5928, 2014.\\n[7] I. J. Goodfellow, J. P. Abadie, M. Mirza, B. Xu, D. W. Farley, S.ozair, A. Courville, and Y. Bengio.\\nGenerative adversarial nets. In NIPS, 2014.\\n[8] I. J. Goodfellow, D.Warde-Farley, M. Mirza, A. C. Courville, and Y. Bengio. Maxout networks. In ICML,\\n2013.\\n[9] K. Gregor, I. Danihelka, A. Mnih, C. Blundell, and D. Wierstra. Deep autoregressive networks. In ICML,\\n2014.\\n[10] D. P. Kingma and J. L. Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[11] D. P. Kingma, D. J. Rezende, S. Mohamed, and M. Welling. Semi-supervised learning with deep generative models. In NIPS, 2014.\\n[12] D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In ICLR, 2014.\\n[13] H. Larochelle and I. Murray. The neural autoregressive distribution estimator. In AISTATS, 2011.\\n[14] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.\\nIn Proceedings of the IEEE, 1998.\\n[15] C. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-supervised nets. In AISTATS, 2015.\\n[16] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In ICML, 2009.\\n[17] M. Lin, Q. Chen, and S. Yan. Network in network. In ICLR, 2014.\\n[18] R. J. Little and D. B. Rubin. Statistical analysis with missing data. JMLR, 539, 1987.\\n[19] L. V. Matten and G. Hinton. Visualizing data using t-SNE. JMLR, 9:2579?2605, 2008.\\n[20] K. Miller, M. P. Kumar, B. Packer, D. Goodman, and D. Koller. Max-margin min-entropy models. In\\nAISTATS, 2012.\\n[21] A. Mnih and K. Gregor. Neural variational inference and learning in belief networks. In ICML, 2014.\\n[22] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with\\nunsupervised feature learning. NIPS Workshop on Deep Learning and Unsupervised Feature Learning,\\n2011.\\n[23] M. Ranzato, J. Susskind, V. Mnih, and G. E. Hinton. On deep generative models with applications to\\nrecognition. In CVPR, 2011.\\n[24] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in\\ndeep generative models. In ICML, 2014.\\n[25] R. Salakhutdinov and G. E. Hinton. Deep Boltzmann machines. In AISTATS, 2009.\\n[26] L. Saul, T. Jaakkola, and M. Jordan. Mean field theory for sigmoid belief networks. Journal of AI\\nResearch, 4:61?76, 1996.\\n[27] P. Sermanet, S. Chintala, and Y. Lecun. Convolutional neural networks applied to house numbers digit\\nclassification. In ICPR, 2012.\\n[28] S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter. Pegasos: Primal estimated sub-gradient solver for\\nSVM. Mathematical Programming, Series B, 2011.\\n[29] Y. Tang. Deep learning using linear support vector machines. In Challenges on Representation Learning\\nWorkshop, ICML, 2013.\\n[30] B. Taskar, C. Guestrin, and D. Koller. Max-margin Markov networks. In NIPS, 2003.\\n[31] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support vector machine learning for interdependent and structured output spaces. In ICML, 2004.\\n[32] C. J. Yu and T. Joachims. Learning structural SVMs with latent variables. In ICML, 2009.\\n[33] M. D. Zeiler and R. Fergus. Stochastic pooling for regularization of deep convolutional neural networks.\\nIn ICLR, 2013.\\n[34] J. Zhu, A. Ahmed, and E. P. Xing. MedLDA: Maximum margin supervised topic models. JMLR, 13:2237?\\n2278, 2012.\\n[35] J. Zhu, N. Chen, H. Perkins, and B. Zhang. Gibbs max-margin topic models with data augmentation.\\nJMLR, 15:1073?1110, 2014.\\n[36] J. Zhu, N. Chen, and E. P. Xing. Bayesian inference with posterior regularization and applications to\\ninfinite latent SVMs. JMLR, 15:1799?1847, 2014.\\n[37] J. Zhu, E.P. Xing, and B. Zhang. Partially observed maximum entropy discrimination Markov networks.\\nIn NIPS, 2008.\\n\\n9\\n\\n\\f\",\n          \"Causal Categorization with Bayes Nets\\n\\nBob Rehder\\nDepartment of Psychology\\nNew York University\\nNew York, NY 10012\\nbob .rehder@nyu.edu\\n\\nAbstract\\nA theory of categorization is presented in which knowledge of\\ncausal relationships between category features is represented as a\\nBayesian network. Referred to as causal-model theory, this theory\\npredicts that objects are classified as category members to the\\nextent they are likely to have been produced by a categorys causal\\nmodel. On this view, people have models of the world that lead\\nthem to expect a certain distribution of features in category\\nmembers (e.g., correlations between feature pairs that are directly\\nconnected by causal relationships), and consider exemplars good\\ncategory members when they manifest those expectations. These\\nexpectations include sensitivity to higher-order feature interactions\\nthat emerge from the asymmetries inherent in causal relationships.\\nResearch on the topic of categorization has traditionally focused on the problem of\\nlearning new categories given observations of category members. In contrast, the\\ntheory-based view of categories emphasizes the influence of the prior theoretical\\nknowledge that learners often contribute to their representations of categories [1].\\nHowever, in contrast to models accounting for the effects of empirical observations,\\nthere have been few models developed to account for the effects of prior knowledge.\\nThe purpose of this article is to present a model of categorization referred to as\\ncausal-model theory or CMT [2, 3]. According to CMT, people 's know ledge of\\nmany categories includes not only features, but also an explicit representation of the\\ncausal mechanisms that people believe link the features of many categories.\\nIn this article I apply CMT to the problem of establishing objects category\\nmembership. In the psychological literature one standard view of categorization is\\nthat objects are placed in a category to the extent they have features that have often\\nbeen observed in members of that category. For example, an object that has most of\\nthe features of birds (e.g., wings, fly, build nests in trees, etc.) and few features of\\nother categories is thought to be a bird. This view of categorization is formalized by\\nprototype models in which classification is a function of the similarity (i.e. , number\\nof shared features) between a mental representation of a category prototype and a\\nto-be-classified object. However , a well-known difficulty with prototype models is\\nthat a features contribution to category membership is independent of the presence\\nor absence of other features. In contrast , consideration of a categorys theoretical\\nknowledge is likely to influence which combinations of features make for\\nacceptable category members. For example , people believe that birds have nests in\\ntrees because they can fly , and in light of this knowledge an animal that doesnt fly\\n\\n\\fand yet still builds nests in trees might be considered a less plausible bird than an\\nanimal that builds nests on the ground and doesnt fly (e.g., an ostrich) even though\\nthe latter animal has fewer features typical of birds.\\nTo assess whether knowledge in fact influences which feature combinations make\\nfor good category members , in the following experiment undergraduates were taught\\nnovel categories whose four binary features exhibited either a common-cause or a\\ncommon-effect schema (Figure 1). In the common-cause schema, one category\\nfeature (PI) is described as causing the three other features (F 2, F 3, and F4). In the\\ncommon-effect schema one feature (F4) is described as being caused by the three\\nothers (F I, F 2, and F3). CMT assumes that people represent causal knowledge such\\nas that in Figure 1 as a kind of Bayesian network [4] in which nodes are variables\\nrepresenting binary category features and directed edges are causal relationships\\nrepresenting the presence of probabilistic causal mechanisms between features.\\nSpecifically , CMT assumes that when a cause feature is present it enables the\\noperation of a causal mechanism that will, with some probability m , bring about the\\npresence of the effect feature. CMT also allow for the possibility that effect features\\nhave potential background causes that are not explicitly represented in the network,\\nas represented by parameter b which is the probability that an effect will be present\\neven when its network causes are absent. Finally, each cause node has a parameter c\\nthat represents the probability that a cause feature will be present.\\n\\n~\\n\\nCommon-Cause\\nSchema\\n\\n~\\n?\\nCommon-Effect\\nSchema\\n\\nFigure 1.\\n\\n...(~~) @ .....\\n:\\n\\n~~:f?\\\"\\\"\\\"\\\"?1\\n\\\"....@/ ?\\\"\\\"\\\"::?\\n?'.\\nF\\n\\nCommon-Cause\\nCorrelations\\n\\n3\\n\\n.?\\n\\nCommon-Effect\\nCorrelations\\n\\nFigure 2.\\n\\nThe central prediction of CMT is that an object is considered to be a category\\nmember to the extent that its features were likely to have been generated by a\\ncategory's causal mechanisms. For example, Table 1 presents the likelihoods that\\nthe causal models of Figure 1 will generate the sixteen possible combinations of F I,\\nF 2, F 3, and F 4. Each likelihood equation can be derived by the application of simple\\nBoolean algebra operations. For example, the probability of exemplar 1101 (F I, F 2,\\nF4 present, F3 absent) being generated by a common-cause model is the probability\\nthat F I is present [c], times the probability that F2 was brought about by F I or its\\nbackground cause [1- (lmj(l-b)], times the probability that F3 was brought about\\nby neither F I nor its background cause [(l-m )(l-b)], times the probability that F 4\\nwas brought about by F I or its background cause [1- (lmj(l-b)]. Likewise , the\\nprobability of exemplar 1011 (F I, F 3, F 4 present, F2 absent) being generated by a\\ncommon-effect model is the probability that FI and F3 are present [c 2 ], times the\\nprobability that F2 is absent [1-?], times the probability that F4 was brought about\\nby F I, F 3, or its background cause [1- (lmj(l-m )(l-b)] . Note that these likelihoods\\nassume that the causal mechanisms in each model operate independently and with\\nthe same probability m, restrictions that can be relaxed in other applications.\\nThis formalization of categorization offered by CMT implies that peoples\\ntheoretical knowledge leads them to expect a certain distribution of features in\\ncategory members , and that they use this information when assigning category\\nmembership. Thus , to gain insight into the categorization performance predicted by\\nCMT , we can examine the statistical properties of category features that one can\\n\\n\\fexpect to be generated by a causal model. For example , dotted lines in Figure 2\\nrepresent the features correlations that are generated from the causal schemas of\\nFigure 1. As one would expect, pairs of features directly linked by causal\\nrelationships are correlated in the common-cause schema F I is correlated with its\\neffects and in the common-effect schema F4 is correlated with its causes. Thus,\\nCMT predicts that combinations of features serve as evidence for category\\nmembership to the extent that they preserve these expected correlations (i.e. , both\\ncause and effect present or both absent) , and against category membership to the\\nextent that they break those correlations (one present and the other absent).\\nTable 1: Likelihoods Equations and Observed and Predicted Values\\nCommon Cause Schema\\nLikelihood\\nObserved Predicted\\ne'b ,3\\n60 .0\\n61.7\\ne 'b ,2 b\\n44 .9\\n45 .7\\ne'b ,2 b\\n46.1\\n45 .7\\ne 'b ,2 b\\n42 .8\\n45 .7\\ne m ,3 b ,3\\n44.5\\n44.1\\ne 'b 'b 2\\n41.0\\n40.1\\ne'b 'b 2\\n40.8\\n40.1\\ne 'b 'b 2\\n42 .7\\n40.1\\ne m ,2 b ,2 (1- m 'b ')\\n55.1\\n52 .7\\nem ,2 b ,2 (1- m 'b ')\\n52 .6\\n52 .7\\nem ,2 b ,2 (1- m 'b ')\\n54 .3\\n52 .7\\ne 'b 3\\n39.4\\n38.1\\nem 'b '(1-m 'b ,)2\\n64 .2\\n65 .6\\ne m 'b '(1-m 'b ,)2\\n65 .3\\n65 .6\\nem 'b '(1-m 'b ,)2\\n62 .0\\n65 .6\\ne (1-m 'b ,)3\\n1111\\n90 .8\\n89 .6\\nNote . e'=l- c . m '=l-m . b'=l-b.\\n\\nExemElar\\n0000\\n0001\\n0010\\n0100\\n1000\\n0011\\n0101\\n0110\\n1001\\n1010\\n1100\\n0111\\n1011\\n1101\\n1110\\n\\nCommon Effect Schema\\nControl\\nLikelihood\\nObserved Predicted Observed\\ne ,3 b ,\\n70 .0\\n69 .3\\n70 .7\\ne ,3 b\\n26 .3\\n27 .8\\n67.0\\nee,2 m 'b '\\n43.4\\n47 .7\\n65.6\\nee ,2 m 'b '\\n47 .3\\n47 .7\\n66.0\\nee,2 m 'b '\\n48.0\\n47 .7\\n67.0\\nee ,2 (1-m 'b ')\\n56 .3\\n56.5\\n67.1\\nee,2 (1-m 'b ')\\n56.5\\n56.5\\n66.5\\ne 2e 'm ,2 b ,\\n38 .3\\n39 .2\\n65.6\\nee,2 (1-m 'b ')\\n57.7\\n56.5\\n68.0\\ne 2e 'm ,2 b ,\\n43 .0\\n39 .2\\n67.6\\ne 2e 'm ,2 b ,\\n41.9\\n39 .2\\n69 .9\\ne 2e'(1-m ,2 b ,)\\n71.0\\n74.4\\n67.6\\ne 2e '(1-m ,2 b ,)\\n75 .7\\n74.4\\n67 .2\\ne 2e'(1-m ,2 b ,)\\n74 .7\\n74.4\\n70 .2\\ne 3m ,3 b ,\\n33 .8\\n35 .8\\n72 .2\\ne 3(1-m ,3 b ,)\\n91.0\\n90 .0\\n75.6\\n\\nCausal networks not only predict pairwise correlations between directly connected\\nfeatures. Figure 2 indicates that as a result of the asymmetries inherent in causal\\nrelationships there is an important disanalogy between the common-cause and\\ncommon-effect schemas: Although the common-cause schema implies that the three\\neffects (F 2 , F 3 , F 4) will be correlated (albeit more weakly than directly connected\\nfeatures) , the common-effect schema does not imply that the three causes (F I , F 2 ,\\nF 3 ) will be correlated. This asymmetry between common-cause and common-effect\\nschemas has been the focus of considerable investigation in the philosophical and\\npsychological literatures [3 , 5]. Use of these schemas in the following experiment\\nenables a test of whether categorizers are sensitive the pattern of correlations\\nbetween features directly-connected by causal laws, and also those that arise due to\\nthe asymmetries inherent in causal relationships shown in Figure 2. Moreover , I will\\nshow that CMT predicts, and humans exhibit, sensitivity to interactions among\\nfeatures of a higher-order than the pairwise interactions shown in Figure 2.\\n\\nMethod\\nSix novel categories were used in which the description of causal relationships\\nbetween features consisted of one sentence indicating the cause and effect feature ,\\nand then one or two sentences describing the mechanism responsible for the causal\\nrelationship. For example , one of the novel categories , Lake Victoria Shrimp , was\\ndescribed as having four binary features (e.g. , A high quantity of ACh\\nneurotransmitter. , Long-lasting flight response. , Accelerated sleep cycle. , etc.)\\n\\n\\fand causal relationships among those features (e.g. , \\\"A high quantity of ACh\\nneurotransmitter causes a long-lasting flight response. The duration of the electrical\\nsignal to the muscles is longer because of the excess amount of neurotransmitter. \\\").\\nParticipants first studied several computer screens of information about their\\nassigned category at their own pace. All participants were first presented with the\\nfour features. Participants in the common-cause condition were\\ncategorys\\nadditionally instructed on the common-cause causal relationships (F 1-;' F 2 , F 1-;' F 3 ,\\nF 1-;' F 4) , and participants in the common-effect condition were instructed on the\\ncommon-effect relationships (F 1-;.F4 , F 2 -;.F4 , F 3 -;.F4 ). When ready , participants\\ntook a multiple-choice test that tested them on the knowledge they had just studied.\\nParticipants were required to retake the test until they committed 0 errors.\\nParticipants then performed a classification task in which they rated on a 0-100\\nscale the category membership of 16 exemplars , consisting of all possible objects\\nthat can be formed from four binary features. For example , those participants\\nassigned to learn the Lake Victoria Shrimp category were asked to classify a shrimp\\nthat possessed \\\"High amounts of the ACh neurotransmitter ,\\\" \\\"A normal flight\\nresponse ,\\\" \\\"Accelerated sleep cycle ,\\\" and \\\"Normal body weight.\\\" The order of the\\ntest exemplars was randomized for each participant.\\nOne hundred and eight University of Illinois undergraduates received course credit\\nfor participating in this experiment. They were randomly assigned in equal numbers\\nto the three conditions , and to one of the six experimental categories.\\n\\nResults\\nCategorization ratings for the 16 test exemplars averaged over partIclpants in the\\ncommon-cause , common-effect, and control conditions are presented in Table 1.\\nThe presence of causal knowledge had a large effect on the ratings. For instance,\\nexemplars 0111 and 0001 were given lower ratings in the common-cause and\\ncommon-effect conditions , respectively (39.4 and 26.3) than in the control condition\\n(67.6 and 67.0) presumably because in these exemplars correlations are broken\\n(effect features are present even though their causes are absent). In contrast,\\nexemplar 1111 received a significantly higher rating in the common-cause and\\ncommon-effect conditions than in the control condition (90.8 and 9l.0 vs. 75.6) ,\\npresumably because in both conditions all correlations are preserved.\\nTo confirm that causal schemas induced a sensitivity to interactions between\\nfeatures, categorization ratings were analyzed by performing a multiple regression\\nfor each participant. Four predictor variables (f1 , f2, f3 , f4) were coded as -1 if the\\nfeature was absent , and + 1 if it was present. An additional six predictor variables\\nwere formed from the multiplicative interaction between pairs of features: f12 , f13 ,\\nf14 , f24 , f34 , and f23. For those feature pairs connected by a causal relationship the\\ntwo-way interaction terms represent whether the causal relationship is confirmed\\n(+ 1, cause and effect both present or both absent) or violated (-1 , one present and\\none absent). Finally , the four three-way interactions (f123 , f124 , f134, and f234) , and\\nthe single four-way interaction (f1234) were also included as predictors.\\nRegression weights averaged over participants are presented in Figure 3 as a\\nfunction of causal schema condition. Figure 3 indicates that the interaction terms\\ncorresponding to those feature pairs assigned causal relationships had significantly\\npositive weights in both the common-cause condition (f12 , f13 , f14) , and the\\ncommon-effect condition (f14 , f24 , f34). That is , as predicted (Figure 2) an exemplar\\nwas rated a better category member when it preserved expected correlations (cause\\nand effect feature either both present or both absent) , and a worse member when it\\nbroke those correlations (one absent and the other present).\\n\\n\\f12\\n\\n(a) Common Cause vs. Control\\n\\n10\\n\\n.l:\\nOf)\\n'0:;\\n\\n~\\n\\n6\\n\\n~\\n\\n4\\n\\na\\n\\n'\\\"\\n\\\"''\\\"\\\"\\n\\n?\\n\\n8\\n\\nControl Observed\\n\\nE9\\n\\n0\\n\\n(b) Common Effect vs. Control\\n\\n10\\n\\n.l:\\nOf)\\n\\nCC Predicted\\n\\n~\\n\\n2\\n\\n(2)\\n12\\n\\n'0:;\\n\\nCC Observed\\n\\n?\\n\\n8\\n\\nCE Observed\\n\\n~\\n\\n6\\n\\nControl Observed\\n\\n~\\n\\na\\n\\n4\\n\\nCE Predicted\\n\\n'\\\"\\n\\n\\\"''\\\"\\\"\\n\\n2\\n0\\n(2)\\n\\nfl\\n\\nf2\\n\\nf3\\n\\nf4\\n\\nfl2\\n\\nfl3\\n\\nfl4\\n\\nf24\\n\\nf34\\n\\nf23 fl23 f124 f134 f234 f1234\\n\\nRegression Term\\nFigure 3\\n\\nIn addition, it was shown earlier (Figure 2) that because of their common-cause the\\nthree effect features in a common-cause schema will be correlated, albeit more\\nweakly than directly-linked features. Consistent with this prediction, in this\\ncondition the three two-way interaction terms between the effect features (f24, f34,\\nf23) are greater than those interactions in the control condition. In contrast, the\\ncommon-effect schema does not imply that the three cause features will be\\ncorrelated, and in fact in that condition the interactions between the cause attributes\\n(f12, f13, f23) did not differ from those in the control condition (Figure 3).\\nFigure 3 also reveals higher-order interactions among features in the common-effect\\ncondition: Weights on interaction terms f124, f134, f234, and f1234 (- 1.6,2.0 , -2.0,\\nand 2.2) were significantly different from those in the control condition. These\\nhigher-order interactions arose because a common-effect schema requires only one\\ncause feature to explain the presence of the common effect. Figures 7b presents\\nthe logarithm of the ratings in the common-effect condition for those test exemplars\\nin which the common effect is present as a function of the number of cause features\\npresent. Ratings increased\\nmore with the introduction\\n4.5\\nof the first cause as\\ncompared to subsequent\\nbO 4.0\\ncauses. That is, participants\\n'ill\\nconsidered the presence of\\n~ 3.5\\nat\\nleast\\none\\ncause\\nOf)\\n0\\nexplaining the presence of\\n.....l 3.0\\nObserved\\nObserved\\nthe common-effect to be\\n(CE Present)\\n(CC Present)\\nsufficient grounds to grant\\n2.5\\nPred icted\\nan exemplar a relatively\\nhigh category membership\\no\\n2\\n3\\no\\n2\\n3\\nrating in a common-effect\\n# of Effects\\n# of Causes\\ncategory.\\nIn\\ncontrast ,\\nFigure 4\\nFigure 7a shows a linear\\n\\n.=\\n\\n?\\n\\n\\fincrease in (the logarithm of) categorization ratings for those exemplars in which\\nthe common cause is present as a function of the number of effect features. In the\\npresence of the common cause each additional effect produced a constant increment\\nto log categorization ratings.\\nFinally , Figure 3 also indicates that the simple feature weights differed as a function\\nof causal schema. In the common-cause condition, the common-cause (f1) carried\\ngreater weight than the three effects (f2, f3 , f4). In contrast, in the common-effect\\ncondition it was the common-effect (f4) that had greater weight than the three\\ncauses (f1 , f2, f3). That is , causal networks promote the importance of not only\\nspecific feature combinations , but the importance of individual features as well.\\n\\nModel Fitting\\nTo assess whether CMT accounts for the patterns of classification found in this\\nexperiment, the causal models of Figure 1 were fitted to the category membership\\nratings of each participant in the common-cause and common-effect conditions,\\nrespectively. That is , the ratings were predicted from the equation ,\\nRating (X) = K ? Likelihood (X; c, m , b)\\nwhere Likelihood (X; c, m , b) is the likelihood of exemplar X as a function of c, m ,\\nand b. The likelihood equations for the common-cause and common-effect models\\nshown in Table 1 were used for common-cause and common-effect participants ,\\nrespectively. K is a scaling constant that brings the likelihood into the range 0-100.\\nFor each participant, the values for parameters K , c, m, and b that minimized the\\nsquared deviation between the predicted and observed ratings was computed. The\\nbest fitting values for parameters K , c, m , and b averaged over participants were\\n846 , .578 , .214 , and .437 in the common-cause condition , and 876 , .522 , .325 , and\\n.280 in the common-effect condition. The predicted ratings for each exemplar are\\npresented in Table 1. The significantly positive estimate for m in both conditions\\nindicates that participants categorization performance was consistent with them\\nassuming the presence of a probabilistic causal mechanisms linking category\\nfeatures. Ratings predicted by CMT did not differ from observed ratings according\\nto chi-square tests: )(\\\\16)=3.0 for common cause, )(\\\\16)=5.3 for common-effect.\\nTo demonstrate that CMT predicts participants\\nsensitivity to particular\\ncombinations of features when categorizing , each participants predicted ratings\\nwere subjected to the same regressions that were performed on the observed ratings.\\nThe resulting regression weights averaged over participants are presented in Figure\\n3 superimposed on the weights from the observed data. First, Figure 3 indicates that\\nCMT reproduces participants sensitivity to agreement between pairs of features\\ndirectly connected by causal relationships (f12 , f13 , f14 in the common-cause\\ncondition , and f14 , f24 , f34 in the common-effect condition). That is , according to\\nboth CMT and human participants , category membership ratings increase when\\npairs of features confirm causal laws , and decrease when they violate those laws.\\nSecond, Figure 3 indicates that CMT accounts for the interactions between the\\neffect features in the common-cause condition (f12, f13 , f23) and also for the higherorder feature interactions in the common-effect condition (f124 , f134, f234 , f1234) ,\\nindicating that that CMT is also sensitive to the asymmetries inherent in causal\\nrelationships. The predictions of CMT superimposed on the observed data in Figure\\n4 confirm that CMT , like the human participants , requires only one cause feature to\\nexplain the presence of a common effect (nonlinear increase in ratings in Figure\\n4b) whereas CMT predicts a linear increase in log ratings as one adds effect features\\nto a common cause (Figure 4a). Finally , CMT also accounts for the larger weight\\ngiven to the common cause and common-effect features (Figure 3).\\n\\n\\fDiscussion\\nThe current results support CMTs claims that people have a representation of the\\nprobabilistic causal mechanisms that link category features, and that they classify by\\nevaluating whether an objects combination of features was likely to have been\\ngenerated by those mechanisms. That is , people have models of the world that lead\\nthem to expect a certain distribution of features in category members , and consider\\nexemplars good category members to the extent they manifest those expectations.\\nOne way this effect manifested itself is in terms of the importance of preserved\\ncorrelations between features directly connected by causal relationships. An\\nalternative model that accounts for this particular result assumes that the feature\\nspace is expanded to include configural cues encoding the confirmation or violation\\nof each causal relationship [6]. However , such a model treats causal links as\\nsymmetric and does not consider interactions among links. As a result , it does not fit\\nthe common effect data as well as CMT (Figure 4b) , because it is unable to account\\nfor categorizers sensitivity to the higher-order feature interactions that emerge as a\\nresult of causal asymmetries in a complex network.\\nCMT diverges from traditional models of categorization by emphasizing the\\nknowledge people possess as opposed to the examples they observe. Indeed , the\\ncurrent experiment differed from many categorization studies in not providing\\nexamples of category members. As a result , CMT is applicable to the many realworld categories about which people know far more than they have observed first\\nhand (e.g., scientific concepts). Of course, for many other categories people observe\\ncategory members , and the nature of the interactions between knowledge and\\nobservations is an open question of considerable interest. Using the same materials\\nas in the current study, the effects of knowledge and observations have been\\northogonally manipulated with the finding that observations had little effect on\\nclassification performance as compared to the theories [7]. Thus , theories may often\\ndominate categorization decisions even when observations are available.\\n\\nAcknowledgments\\nSupport for this research was provided by funds from the National Science\\nFoundation (Grants Number SBR-98l6458 and SBR 97-20304) and from the\\nNational Institute of Mental Health (Grant Number ROl MH58362).\\n\\nReferences\\n[1] Murphy, G . L. , & Medin, D . L. (1985). The role of theories in conceptual coherence .\\nPsychological Review , 92, 289-316.\\n[2] Rehder, B. (1999). A causal model theory of categorization . In Proceedin gs of the 21st\\nAnnual Meeting of the Cognitive Science Society (pp. 595-600). Vancouver.\\n[3] Waldmann , M .R ., Holyoak , K.J ., & Fratianne, A. (1995). Causal models and the acquisition\\nof category structure. Journal of Experimental Psychology: General, 124 , 181-206 .\\n[4] Pearl , J. (1988). Probabilistic reasoning in intelligent systems: Networks of plausible\\ninference. San Mateo , CA: Morgan Kaufman.\\n[5] Salmon, W. C. (1984). Scientific explanation and the causa l structure of the world.\\nPrinceton , NJ: Princeton University Press.\\n[6] Gluck, M. A. , & Bower, G. H . (1988). Evaluating an adaptive network model of human\\nlearning. Journal of Memory and Language, 27,166-195.\\n[7] Rehder, B., & Hastie , R. (2001). Causal knowledge and categories: The effects of causal beliefs\\non categorization , induction, and similarity. Journal of Experimental Psychology: General, 130 ,\\n323-360.\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the regular expression library\n",
        "import re\n",
        "\n",
        "# Remove punctuation\n",
        "papers['paper_text_processed'] = \\\n",
        "papers['paper_text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
        "\n",
        "# Convert the titles to lowercase\n",
        "papers['paper_text_processed'] = \\\n",
        "papers['paper_text_processed'].map(lambda x: x.lower())\n",
        "\n",
        "# Print out the first rows of papers\n",
        "papers['paper_text_processed'].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "D1nbSUoiZ3Pm",
        "outputId": "7e603098-f7f2-4d64-c6f5-3a566a200f83"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2394    continuous time particle filtering for fmri\\n\\...\n",
              "4828    nonparametric bayesian inference on multivaria...\n",
              "5638    relevant sparse codes with variational informa...\n",
              "1654    extending q-learning to general adaptive\\nmult...\n",
              "2206    learning to be bayesian without supervision\\n\\...\n",
              "Name: paper_text_processed, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_text_processed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2394</th>\n",
              "      <td>continuous time particle filtering for fmri\\n\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4828</th>\n",
              "      <td>nonparametric bayesian inference on multivaria...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5638</th>\n",
              "      <td>relevant sparse codes with variational informa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1654</th>\n",
              "      <td>extending q-learning to general adaptive\\nmult...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2206</th>\n",
              "      <td>learning to be bayesian without supervision\\n\\...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the wordcloud library\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Join the different processed titles together.\n",
        "long_string = ','.join(list(papers['paper_text_processed'].values))\n",
        "\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
        "\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "Lhz9hc_xZ6rF",
        "outputId": "45048f15-beec-4909-f7b5-15bedc5f7732"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=400x200>"
            ],
            "image/png": "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\n",
            "image/jpeg": "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\n"
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U gensim"
      ],
      "metadata": {
        "id": "9IkGrUARbLoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim"
      ],
      "metadata": {
        "id": "gV7U6APOb_oV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import simple_preprocess\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(simple_preprocess(str(sentence), deacc=True))\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc))\n",
        "             if word not in stop_words] for doc in texts]\n",
        "\n",
        "\n",
        "data = papers.paper_text_processed.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "# remove stop words\n",
        "data_words = remove_stopwords(data_words)\n",
        "\n",
        "print(data_words[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-3hASV1Z_6o",
        "outputId": "6de60079-9829-4824-f93b-cd070c4141c1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['continuous', 'time', 'particle', 'filtering', 'fmri', 'lawrence', 'murray', 'school', 'informatics', 'university', 'edinburgh', 'lawrencemurray', 'edacuk', 'amos', 'storkey', 'school', 'informatics', 'university', 'edinburgh', 'astorkey', 'edacuk', 'abstract', 'construct', 'biologically', 'motivated', 'stochastic', 'differential', 'model', 'neural', 'hemodynamic']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.corpora as corpora\n",
        "\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_words)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_words\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "print(corpus[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QywvToTb4cU",
        "outputId": "753415a1-4bf0-47fc-cb64-eca17481466d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 1), (1, 2), (2, 1), (3, 2), (4, 1), (5, 1), (6, 2), (7, 1), (8, 1), (9, 4), (10, 1), (11, 1), (12, 23), (13, 4), (14, 1), (15, 2), (16, 1), (17, 1), (18, 2), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# number of topics\n",
        "num_topics = 10\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=id2word,\n",
        "                                       num_topics=num_topics)\n",
        "\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9yHJwq4b9kS",
        "outputId": "4afb050f-70b6-4c84-deea-837cc5f237cd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  '0.006*\"model\" + 0.005*\"data\" + 0.005*\"learning\" + 0.004*\"function\" + '\n",
            "  '0.004*\"algorithm\" + 0.003*\"figure\" + 0.003*\"neural\" + 0.003*\"using\" + '\n",
            "  '0.003*\"set\" + 0.003*\"two\"'),\n",
            " (1,\n",
            "  '0.007*\"learning\" + 0.005*\"model\" + 0.004*\"distribution\" + 0.004*\"data\" + '\n",
            "  '0.004*\"two\" + 0.003*\"function\" + 0.003*\"algorithm\" + 0.003*\"figure\" + '\n",
            "  '0.003*\"time\" + 0.003*\"set\"'),\n",
            " (2,\n",
            "  '0.007*\"model\" + 0.005*\"data\" + 0.004*\"learning\" + 0.004*\"algorithm\" + '\n",
            "  '0.004*\"time\" + 0.004*\"one\" + 0.004*\"using\" + 0.004*\"set\" + 0.003*\"function\" '\n",
            "  '+ 0.003*\"problem\"'),\n",
            " (3,\n",
            "  '0.007*\"model\" + 0.007*\"data\" + 0.006*\"learning\" + 0.005*\"one\" + '\n",
            "  '0.004*\"algorithm\" + 0.004*\"set\" + 0.004*\"models\" + 0.004*\"time\" + '\n",
            "  '0.004*\"neural\" + 0.003*\"using\"'),\n",
            " (4,\n",
            "  '0.007*\"model\" + 0.006*\"learning\" + 0.005*\"algorithm\" + 0.005*\"using\" + '\n",
            "  '0.005*\"time\" + 0.004*\"data\" + 0.003*\"results\" + 0.003*\"problem\" + '\n",
            "  '0.003*\"one\" + 0.003*\"models\"'),\n",
            " (5,\n",
            "  '0.006*\"model\" + 0.005*\"learning\" + 0.005*\"models\" + 0.004*\"data\" + '\n",
            "  '0.004*\"figure\" + 0.004*\"function\" + 0.004*\"using\" + 0.003*\"features\" + '\n",
            "  '0.003*\"time\" + 0.003*\"algorithm\"'),\n",
            " (6,\n",
            "  '0.008*\"data\" + 0.006*\"learning\" + 0.006*\"algorithm\" + 0.005*\"model\" + '\n",
            "  '0.004*\"set\" + 0.004*\"using\" + 0.004*\"function\" + 0.003*\"distribution\" + '\n",
            "  '0.003*\"training\" + 0.003*\"figure\"'),\n",
            " (7,\n",
            "  '0.007*\"model\" + 0.007*\"data\" + 0.006*\"learning\" + 0.004*\"algorithm\" + '\n",
            "  '0.004*\"function\" + 0.004*\"two\" + 0.004*\"image\" + 0.003*\"one\" + '\n",
            "  '0.003*\"network\" + 0.003*\"neural\"'),\n",
            " (8,\n",
            "  '0.007*\"learning\" + 0.006*\"data\" + 0.005*\"algorithm\" + 0.005*\"model\" + '\n",
            "  '0.004*\"problem\" + 0.004*\"set\" + 0.004*\"using\" + 0.004*\"given\" + 0.004*\"one\" '\n",
            "  '+ 0.004*\"image\"'),\n",
            " (9,\n",
            "  '0.009*\"model\" + 0.006*\"data\" + 0.006*\"learning\" + 0.004*\"algorithm\" + '\n",
            "  '0.004*\"distribution\" + 0.004*\"function\" + 0.003*\"two\" + 0.003*\"figure\" + '\n",
            "  '0.003*\"used\" + 0.003*\"set\"')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "on6sR5yYcUu8",
        "outputId": "0703752a-9204-4436-db31-4f8749b12f91"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (1.13.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (2.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (3.1.6)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (2.10.2)\n",
            "Collecting funcy (from pyLDAvis)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (1.6.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (4.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (75.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyLDAvis) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyLDAvis) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim->pyLDAvis) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->pyLDAvis) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim->pyLDAvis) (1.17.2)\n",
            "Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-2.0 pyLDAvis-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pickle\n",
        "\n",
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "LDAvis_data_filepath = os.path.join('results/outputLDAvis_'+str(num_topics))\n",
        "\n",
        "# # this is a bit time consuming - make the if statement True\n",
        "# # if you want to execute visualization prep yourself\n",
        "if 1 == 1:\n",
        "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
        "    with open(LDAvis_data_filepath, 'wb') as f:\n",
        "        pickle.dump(LDAvis_prepared, f)\n",
        "\n",
        "# load the pre-prepared pyLDAvis data from disk\n",
        "with open(LDAvis_data_filepath, 'rb') as f:\n",
        "    LDAvis_prepared = pickle.load(f)\n",
        "\n",
        "pyLDAvis.save_html(LDAvis_prepared, 'results/outputLDAvis_'+ str(num_topics) +'.html')\n",
        "\n",
        "LDAvis_prepared"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 861
        },
        "id": "RysvTPAxcH46",
        "outputId": "f9c862b8-9f62-4b79-bc0e-a1ef9e1b4b96"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
              "topic                                                \n",
              "8      0.004020 -0.000695       1        1  21.436968\n",
              "6      0.004452  0.004344       2        1  13.508419\n",
              "3     -0.000258 -0.007382       3        1  12.684576\n",
              "2     -0.001647 -0.006506       4        1  11.691858\n",
              "9      0.004305  0.000976       5        1  10.372429\n",
              "7     -0.001070  0.004956       6        1   9.332729\n",
              "1      0.000649  0.004164       7        1   6.990640\n",
              "4      0.003841 -0.001409       8        1   6.364289\n",
              "5     -0.013209  0.001953       9        1   5.535981\n",
              "0     -0.001083 -0.000400      10        1   2.082111, topic_info=            Term         Freq        Total Category  logprob  loglift\n",
              "493        model  1463.000000  1463.000000  Default  30.0000  30.0000\n",
              "437     learning  1406.000000  1406.000000  Default  29.0000  29.0000\n",
              "337     function   798.000000   798.000000  Default  28.0000  28.0000\n",
              "170         data  1382.000000  1382.000000  Default  27.0000  27.0000\n",
              "305       figure   667.000000   667.000000  Default  26.0000  26.0000\n",
              "..           ...          ...          ...      ...      ...      ...\n",
              "554          one    11.686611   797.820761  Topic10  -6.0297  -0.3517\n",
              "398  information    10.067554   562.201641  Topic10  -6.1788  -0.1508\n",
              "313        first     9.866942   533.307140  Topic10  -6.1989  -0.1181\n",
              "539       number    10.117710   622.740085  Topic10  -6.1738  -0.2481\n",
              "811         time    10.364890   797.269560  Topic10  -6.1497  -0.4710\n",
              "\n",
              "[848 rows x 6 columns], token_table=       Topic      Freq     Term\n",
              "term                           \n",
              "10740      1  0.393528  abalone\n",
              "10740      2  0.196764  abalone\n",
              "10740      5  0.196764  abalone\n",
              "9424       1  0.074397   absent\n",
              "9424       2  0.148794   absent\n",
              "...      ...       ...      ...\n",
              "6309       6  0.153686       zk\n",
              "6309       7  0.038421       zk\n",
              "6309       8  0.076843       zk\n",
              "6309       9  0.038421       zk\n",
              "6309      10  0.019211       zk\n",
              "\n",
              "[3634 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[9, 7, 4, 3, 10, 8, 2, 5, 6, 1])"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el28481335360893575844802989362\" style=\"background-color:white;\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el28481335360893575844802989362_data = {\"mdsDat\": {\"x\": [0.004019506472826627, 0.004452345834872732, -0.0002581923583941617, -0.00164732348822906, 0.004305251236615758, -0.0010695558172843336, 0.0006486992949415247, 0.0038413058398789023, -0.013208742384599697, -0.0010832946306282912], \"y\": [-0.0006945159200970406, 0.004344057948783106, -0.007381969164375094, -0.00650559730367312, 0.000975741410931089, 0.004955517553843674, 0.00416350917826025, -0.001409182382002279, 0.0019528395643050294, -0.00040040088597562937], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [21.436968024546452, 13.508419195278638, 12.684575863939717, 11.691857670861816, 10.37242929971036, 9.332728601247394, 6.990639644446875, 6.364288951491752, 5.535981291628354, 2.082111456848635]}, \"tinfo\": {\"Term\": [\"model\", \"learning\", \"function\", \"data\", \"figure\", \"distribution\", \"using\", \"models\", \"two\", \"neural\", \"algorithm\", \"results\", \"time\", \"training\", \"problem\", \"different\", \"features\", \"used\", \"image\", \"network\", \"set\", \"one\", \"also\", \"case\", \"information\", \"networks\", \"log\", \"linear\", \"probability\", \"first\", \"latents\", \"boring\", \"rendered\", \"hints\", \"recipes\", \"flagged\", \"ambig\", \"lightness\", \"interleave\", \"compression\", \"hint\", \"relu\", \"deepmind\", \"astrophysics\", \"recipe\", \"occlusions\", \"classes\", \"regularizing\", \"gilchrist\", \"generations\", \"paint\", \"surface\", \"abalone\", \"subbands\", \"omniglot\", \"lowlik\", \"bls\", \"encoders\", \"teacher\", \"eigenvalue\", \"decay\", \"variational\", \"polynomial\", \"image\", \"shape\", \"zk\", \"bits\", \"problem\", \"given\", \"stereo\", \"derivatives\", \"subband\", \"show\", \"stochastic\", \"learning\", \"gaussian\", \"py\", \"accuracy\", \"xi\", \"high\", \"set\", \"algorithm\", \"matrix\", \"methods\", \"section\", \"case\", \"bound\", \"approximation\", \"regression\", \"data\", \"kernel\", \"information\", \"using\", \"one\", \"approach\", \"network\", \"log\", \"probability\", \"model\", \"first\", \"time\", \"function\", \"models\", \"different\", \"training\", \"number\", \"distribution\", \"results\", \"figure\", \"used\", \"neural\", \"two\", \"romma\", \"mistake\", \"cytometry\", \"wtll\", \"misno\", \"universal\", \"iiwtll\", \"sanger\", \"epx\", \"motor\", \"iiwt\", \"freund\", \"pxy\", \"voted\", \"gretton\", \"polyhedron\", \"terence\", \"primitives\", \"percep\", \"bkx\", \"pbx\", \"perceptron\", \"iiwtl\", \"men\", \"sensory\", \"borgwardt\", \"iixtll\", \"liitll\", \"deduce\", \"ytxtlllxtll\", \"trial\", \"px\", \"lmnn\", \"bcm\", \"pen\", \"wt\", \"xij\", \"isolet\", \"circles\", \"yt\", \"vector\", \"fields\", \"data\", \"algorithm\", \"sample\", \"training\", \"xu\", \"classification\", \"distributions\", \"group\", \"test\", \"learning\", \"optimal\", \"number\", \"different\", \"distribution\", \"first\", \"set\", \"points\", \"figure\", \"using\", \"function\", \"based\", \"algorithms\", \"task\", \"error\", \"kernel\", \"model\", \"two\", \"input\", \"log\", \"also\", \"linear\", \"results\", \"given\", \"time\", \"neural\", \"one\", \"networks\", \"methods\", \"network\", \"problem\", \"used\", \"willett\", \"mmi\", \"ndb\", \"xlw\", \"lwk\", \"transfonnation\", \"nbd\", \"lw\", \"outperforming\", \"rigoll\", \"linkage\", \"microclustering\", \"syriasizes\", \"cn\", \"chaperones\", \"syria\", \"xiw\", \"pdfs\", \"iwk\", \"incorporation\", \"italy\", \"nltcs\", \"triphone\", \"lwi\", \"gerhard\", \"negbin\", \"steorts\", \"streams\", \"lhe\", \"experiences\", \"entity\", \"pyp\", \"dp\", \"mlp\", \"nbnb\", \"discriminant\", \"alignment\", \"splines\", \"extraction\", \"net\", \"rad\", \"one\", \"neural\", \"models\", \"cluster\", \"matrix\", \"convex\", \"model\", \"mixture\", \"network\", \"feedback\", \"data\", \"set\", \"training\", \"time\", \"number\", \"experiments\", \"system\", \"rate\", \"systems\", \"learning\", \"based\", \"algorithms\", \"algorithm\", \"using\", \"random\", \"input\", \"two\", \"also\", \"function\", \"figure\", \"first\", \"approach\", \"methods\", \"log\", \"used\", \"error\", \"problem\", \"results\", \"given\", \"information\", \"ucto\", \"ysa\", \"mcts\", \"dng\", \"atari\", \"thompson\", \"mabs\", \"trpo\", \"epical\", \"mdps\", \"qvalue\", \"xiaoping\", \"timesteps\", \"xsa\", \"normalgamma\", \"cvar\", \"rearing\", \"wsas\", \"orthant\", \"ormalgamma\", \"mujoco\", \"optimistic\", \"vthd\", \"sas\", \"forwards\", \"xa\", \"smoothing\", \"accumulated\", \"laplace\", \"trust\", \"horizon\", \"critic\", \"synapse\", \"split\", \"uct\", \"actor\", \"adwords\", \"cluster\", \"policy\", \"risk\", \"acktr\", \"cells\", \"state\", \"optimization\", \"dip\", \"episode\", \"model\", \"assumption\", \"search\", \"gradient\", \"coherent\", \"time\", \"space\", \"one\", \"proposed\", \"continuous\", \"input\", \"using\", \"data\", \"problem\", \"function\", \"distribution\", \"set\", \"neural\", \"algorithm\", \"two\", \"used\", \"results\", \"information\", \"figure\", \"based\", \"learning\", \"number\", \"also\", \"models\", \"output\", \"given\", \"case\", \"network\", \"different\", \"networks\", \"image\", \"pyp\", \"jutten\", \"sublinearly\", \"nbd\", \"nbnb\", \"pflat\", \"projectivity\", \"employees\", \"microclustering\", \"herault\", \"pik\", \"genk\", \"disparities\", \"percentile\", \"boyan\", \"roles\", \"robustness\", \"klingner\", \"phillips\", \"income\", \"lstd\", \"rights\", \"siegel\", \"disparity\", \"italy\", \"td\", \"gam\", \"entity\", \"lambda\", \"anhedonia\", \"cn\", \"ndb\", \"estimators\", \"breakdown\", \"robust\", \"pi\", \"planning\", \"entropy\", \"dyna\", \"nk\", \"model\", \"distribution\", \"point\", \"prior\", \"used\", \"networks\", \"object\", \"function\", \"process\", \"two\", \"figure\", \"linear\", \"data\", \"random\", \"estimator\", \"learning\", \"results\", \"also\", \"task\", \"example\", \"xi\", \"information\", \"algorithm\", \"models\", \"based\", \"log\", \"number\", \"set\", \"problem\", \"time\", \"one\", \"case\", \"using\", \"given\", \"network\", \"different\", \"approach\", \"first\", \"plit\", \"summaries\", \"submodular\", \"parse\", \"ajor\", \"modular\", \"facility\", \"polymatroid\", \"synthesizing\", \"jegelka\", \"ubmodular\", \"subband\", \"rouge\", \"sumit\", \"hamming\", \"turtle\", \"sh\", \"denotation\", \"sos\", \"mis\", \"knapsack\", \"lambertian\", \"nion\", \"tense\", \"morphology\", \"matroid\", \"rvm\", \"iyer\", \"inflections\", \"bskraibd\", \"monotone\", \"recipes\", \"stereo\", \"hardness\", \"shape\", \"lighting\", \"firing\", \"image\", \"programs\", \"synthesis\", \"distance\", \"retina\", \"left\", \"bayesian\", \"texture\", \"go\", \"likelihood\", \"two\", \"model\", \"data\", \"features\", \"gaussian\", \"object\", \"learning\", \"network\", \"neural\", \"non\", \"images\", \"function\", \"prior\", \"xi\", \"approximation\", \"space\", \"algorithm\", \"following\", \"one\", \"based\", \"variables\", \"time\", \"models\", \"information\", \"distribution\", \"given\", \"used\", \"using\", \"training\", \"also\", \"set\", \"problem\", \"input\", \"results\", \"number\", \"figure\", \"matrix\", \"intrator\", \"behavioural\", \"tid\", \"rats\", \"psychiatric\", \"cooper\", \"bdia\", \"economic\", \"healthy\", \"comon\", \"kurtosis\", \"dopamine\", \"shock\", \"stock\", \"qbcm\", \"subjects\", \"talking\", \"operationnelle\", \"buy\", \"dayan\", \"whitened\", \"anns\", \"seligman\", \"resonsiveness\", \"interview\", \"risks\", \"veiel\", \"yoked\", \"stocks\", \"textons\", \"inf\", \"hh\", \"miller\", \"helplessness\", \"minimax\", \"mtl\", \"hdp\", \"polynomials\", \"mdd\", \"outcome\", \"reward\", \"equation\", \"linsker\", \"distribution\", \"measure\", \"hyper\", \"second\", \"learning\", \"qd\", \"control\", \"information\", \"two\", \"feedback\", \"activity\", \"figure\", \"linear\", \"also\", \"yi\", \"parameters\", \"probability\", \"order\", \"well\", \"transition\", \"variables\", \"feature\", \"function\", \"number\", \"case\", \"model\", \"time\", \"input\", \"first\", \"neural\", \"problem\", \"set\", \"results\", \"models\", \"network\", \"using\", \"data\", \"algorithm\", \"training\", \"different\", \"networks\", \"one\", \"given\", \"tomorrows\", \"dyna\", \"counterfactual\", \"intercept\", \"vy\", \"invalid\", \"knew\", \"deploy\", \"iterated\", \"fulfilling\", \"iiwt\", \"broadcasting\", \"stock\", \"beforehand\", \"promoting\", \"imbens\", \"live\", \"bf\", \"explanatory\", \"agg\", \"imaginary\", \"pxy\", \"style\", \"percep\", \"studden\", \"regulatory\", \"nonstandard\", \"counterfactuals\", \"todays\", \"dipankar\", \"sutton\", \"xf\", \"mistake\", \"predictor\", \"mtl\", \"feedback\", \"planning\", \"yi\", \"predictions\", \"control\", \"pitch\", \"romma\", \"px\", \"segments\", \"using\", \"time\", \"wt\", \"different\", \"estimation\", \"results\", \"algorithm\", \"matrix\", \"learning\", \"model\", \"step\", \"first\", \"linear\", \"error\", \"case\", \"test\", \"task\", \"problem\", \"multi\", \"algorithms\", \"models\", \"used\", \"distribution\", \"data\", \"one\", \"log\", \"training\", \"network\", \"based\", \"set\", \"two\", \"function\", \"figure\", \"also\", \"neural\", \"number\", \"schema\", \"cmt\", \"participants\", \"rehder\", \"shrimp\", \"asymmetries\", \"instructed\", \"absent\", \"schemas\", \"nests\", \"victoria\", \"rating\", \"birds\", \"manifest\", \"lmj\", \"exemplar\", \"ratings\", \"differed\", \"exemplars\", \"participant\", \"undergraduates\", \"causal\", \"cause\", \"laws\", \"ach\", \"cmts\", \"sbr\", \"promote\", \"retake\", \"categorizers\", \"common\", \"category\", \"vestibular\", \"dilated\", \"ilated\", \"rnn\", \"effect\", \"features\", \"procedural\", \"models\", \"relationships\", \"figure\", \"model\", \"fibers\", \"large\", \"firing\", \"layer\", \"performance\", \"function\", \"feature\", \"using\", \"kernel\", \"learning\", \"time\", \"also\", \"probability\", \"two\", \"neural\", \"one\", \"given\", \"data\", \"results\", \"problem\", \"output\", \"algorithm\", \"used\", \"distribution\", \"network\", \"number\", \"training\", \"set\", \"information\", \"methods\", \"silicon\", \"ccd\", \"face\", \"bochum\", \"adaptivity\", \"receptors\", \"eeckman\", \"faces\", \"malsburg\", \"livermore\", \"venugopal\", \"symplectic\", \"prototype\", \"edgeworth\", \"boahen\", \"alopex\", \"retina\", \"distortions\", \"optimised\", \"lnp\", \"fapp\", \"galleries\", \"rotated\", \"drastic\", \"cog\", \"brunelli\", \"eu\", \"deformation\", \"mosis\", \"retinas\", \"conserving\", \"lades\", \"percent\", \"buhmann\", \"submodular\", \"balls\", \"diffusivity\", \"camera\", \"cc\", \"si\", \"bipolar\", \"label\", \"partition\", \"unique\", \"distance\", \"function\", \"dip\", \"figure\", \"second\", \"recognition\", \"neural\", \"values\", \"term\", \"likelihood\", \"training\", \"et\", \"output\", \"different\", \"estimation\", \"model\", \"bound\", \"two\", \"data\", \"xi\", \"section\", \"mean\", \"distribution\", \"networks\", \"problem\", \"results\", \"log\", \"learning\", \"order\", \"set\", \"using\", \"image\", \"algorithm\", \"used\", \"case\", \"given\", \"based\", \"network\", \"gradient\", \"models\", \"one\", \"information\", \"first\", \"number\", \"time\"], \"Freq\": [1463.0, 1406.0, 798.0, 1382.0, 667.0, 709.0, 838.0, 693.0, 678.0, 620.0, 1074.0, 594.0, 797.0, 559.0, 705.0, 493.0, 426.0, 566.0, 538.0, 627.0, 825.0, 797.0, 561.0, 454.0, 562.0, 478.0, 494.0, 490.0, 438.0, 533.0, 2.624031867245082, 1.7568602377178144, 3.876960878225637, 7.342031786085146, 27.64324602971859, 2.534654840483284, 6.883051566469484, 2.4834998492823286, 5.496855774885667, 40.4387982934811, 2.9078770998452503, 8.550498395756657, 2.3542832579629525, 1.6082054664609517, 7.916357176232279, 1.5549272410663508, 41.924477211797765, 2.2756637449528334, 1.165057912446318, 2.603066384849589, 1.1235951797333346, 8.250763651197895, 1.8978879559185464, 2.621105689620084, 4.389768504527077, 7.850635874368528, 7.663775985026835, 6.073860311126358, 2.9437385108232506, 34.09874531738527, 31.852543993586085, 77.84298121507912, 49.013267843204666, 176.82003311326676, 81.34725191584961, 17.978611415265117, 15.256702308149888, 208.8756712244654, 183.15947778785954, 18.31686779842764, 16.224079240536533, 9.935350612310122, 89.85420581980212, 86.1478393144121, 344.7086401158624, 118.64940026427885, 20.97169056346415, 50.49932222034379, 107.51619048166492, 76.1907353746677, 205.4895332576931, 256.7606765745951, 135.0316058797387, 119.03517401748313, 97.20088136209831, 116.88302076290842, 91.02294801289794, 87.54308504612047, 69.97773591777022, 300.89896378278826, 105.74700522599336, 136.9300421074067, 191.00952427919282, 178.17819282702388, 108.2312387116279, 144.4394863255705, 119.76383190212667, 107.58258557203564, 244.90809933029325, 121.8844698998833, 157.0810160666551, 154.6414534492921, 136.43161906533217, 112.33929376744062, 119.31684380819486, 125.50577897405319, 132.11501846264753, 121.28984790432234, 124.92207730021198, 116.51079845371459, 118.39087503601793, 112.66704312464145, 16.142065929266604, 9.531266469710847, 5.898622574546004, 1.4009862282617227, 2.0487222342601537, 9.124345192933694, 1.316392916400079, 3.6859123624257744, 0.9413645272994704, 10.607439351164174, 3.5610559097509085, 0.9728382174120316, 11.356467106523752, 2.6160993166970603, 1.2188329080002318, 1.5497312106430488, 1.877521582317237, 11.019295169022636, 1.5318288267354205, 0.8885921162206928, 3.8447003188769, 9.862165918641779, 0.5936634749386042, 1.1630384802471119, 9.180141913263045, 0.8599449433443079, 1.1581125363266722, 0.8611566271972158, 1.1267225825609772, 0.8857621028057631, 9.473705382713508, 34.43154949957201, 20.17854254250913, 6.72947420967922, 2.912956988934985, 36.94978492688969, 4.12064671553178, 5.841424631623781, 25.282185880736012, 26.114686336695133, 68.45846846453766, 19.39389640578194, 242.7428426881056, 186.7793181802507, 60.443013049945954, 103.03516188237491, 19.04507818102332, 56.68611885926193, 47.39398826519782, 25.265312519670022, 50.84885296092262, 198.0434751633166, 57.14584825399267, 97.97069592085786, 81.17766672603135, 108.17516914775997, 86.17491068431026, 120.64441424191483, 60.49534614610431, 98.89918562661163, 117.64645081888123, 110.9446159082525, 83.9176071133902, 66.44541046316975, 56.33442666093914, 61.70860316482239, 65.05128935443413, 155.0165511699247, 90.84250052106736, 72.76298758861834, 72.80746723563708, 78.54321419031066, 72.18836894563808, 80.21430051657858, 81.83679156297644, 91.93582829194796, 76.32410310535128, 84.08196098641687, 66.53390023759495, 65.40014976259542, 69.3078000981601, 69.34321214766686, 66.2203931889889, 2.6443863261342835, 6.573523541724849, 17.198123399501647, 2.123964190070153, 1.6153971845298238, 1.26619250115367, 8.790624437497561, 2.1488683706900242, 0.9066119548954092, 2.118573684103765, 3.2386590397653205, 5.972445960138639, 1.7172110117973673, 19.601305680668215, 1.441937311498441, 1.975670380666154, 0.868287534885836, 1.7507854251567065, 1.1527989887450723, 3.116602829270163, 2.1914507845149203, 1.650229777286949, 0.8360473470997069, 0.8322688263040982, 0.8390893017545809, 2.2056286332545505, 1.9034733364989214, 4.103449903693267, 0.8303254778622022, 0.8074212093999882, 8.380995928037073, 17.458944666781385, 30.835478333547417, 11.118021888057767, 19.802600606662892, 7.944878162260608, 16.02199594133562, 2.98369499638069, 8.829764160770761, 9.42148241179009, 24.674502010543932, 140.39466357103038, 112.31158791715887, 122.43406257471824, 44.57473794124723, 92.8489611829563, 48.13177610768611, 216.85136821880897, 44.31134025322445, 102.78014693358318, 39.34967745286797, 193.20902926858818, 123.79219452522734, 89.15594448965412, 118.92733350102381, 95.63340379010785, 42.983415802603666, 50.75746573081796, 44.271102623384614, 53.35981256868834, 164.29355616112366, 80.08176307736396, 64.08091583036291, 125.69779521715597, 103.35634207740506, 63.49697764666089, 69.81744750242706, 86.11668422666015, 74.31520301487492, 90.31606403952699, 80.61642549214896, 70.36723088922001, 61.549568672674745, 63.57534492573262, 65.75870995129796, 69.26182928042192, 56.6024447385701, 72.70642114796067, 65.56114532432028, 64.69459996264368, 61.64056316891758, 1.9967485916114778, 1.9182378077531852, 14.914093150804874, 10.440461917999263, 6.170411379655429, 7.4826921932726025, 2.1637316376815496, 9.60928705390848, 8.991087255642517, 7.7980393969847075, 0.7483783348395223, 0.7306487507342351, 11.070803917440609, 2.427975516891935, 2.846157393968836, 5.681678654662034, 1.2103495825643058, 1.9252296138816953, 3.7749030356082858, 1.6532741312692396, 4.261866974138135, 0.9362086148599154, 2.1565804939346607, 2.80883766638251, 2.6824646796396814, 2.834165204299673, 14.987568627808896, 4.955711066554919, 0.9438278500660264, 4.879884615575117, 4.101040749516451, 14.224287860236759, 3.863933277126268, 20.357024921615896, 6.950225161378647, 12.23804027248857, 4.8237045127842055, 46.2961478439609, 33.427249659443454, 49.66988598233605, 14.491771868924737, 29.11300763627535, 78.61804922864455, 70.01855788230404, 16.716945719362062, 8.549791502504783, 194.33430905276413, 30.613167146324972, 19.285477652167554, 54.01408585889438, 12.8437391119029, 109.32596014996645, 48.564267845607375, 106.84460157021968, 39.10538042292664, 27.46342714149093, 66.69542680020699, 100.66405131765183, 147.53311929635055, 86.71424482821327, 95.19470482235938, 86.17782908074554, 96.35999420267318, 77.15610217338559, 111.44649105341969, 79.57675133467879, 69.58185771975367, 71.66687478810485, 68.5144742617425, 76.09659233517178, 66.00030046658227, 122.37491024010772, 71.6779931012219, 65.0794742815893, 71.52741012259767, 47.718958564041685, 63.76076995940157, 54.75537446342773, 57.70922642733587, 53.992230214263365, 53.06702909052815, 53.47838762677916, 14.688012794781729, 6.0556315728146295, 1.044185271882362, 6.126910374080077, 16.43047346597412, 7.390794464997506, 0.8064760182944296, 1.2521878661745727, 4.248645518214862, 5.353745073587188, 1.8626292253894619, 0.820255620692126, 1.984735223986123, 5.109624614744565, 2.1040368043809035, 1.0998909351999366, 5.3025138752658725, 0.5780064169801273, 0.7933289106930018, 1.1826536626487358, 6.513123350217931, 1.125077624376985, 0.9513854522971167, 17.33407950793006, 1.5097002977067429, 4.549160290428777, 0.5489233186897167, 5.834822763259698, 4.643264531511476, 3.6210170414975247, 12.504706243189977, 9.020931242612402, 19.635087479132263, 11.576411603909493, 16.011854505406678, 18.959914687203284, 12.27219742413346, 13.319145481871661, 19.434343876050445, 7.06224994565855, 208.5121953319608, 102.9471612154138, 46.046640764760035, 46.44175971827969, 78.3581857820744, 66.90757181282406, 29.640120772740232, 102.68667405153687, 43.400489554672596, 83.31917570401343, 81.88589919098726, 62.73808725201095, 146.53561702769304, 54.663114733799205, 26.98380739744294, 139.14961565156634, 69.99385631245774, 65.3778392272434, 43.32820805639642, 44.58948823330706, 49.06959199196888, 63.990117830654086, 103.00757089374555, 73.50885574859973, 59.930676450203755, 55.7020271326284, 65.00465004598647, 77.86378292510483, 70.15669881350341, 75.12397530771194, 74.13891562473985, 51.851383321839045, 72.47762130497065, 60.858903491932956, 59.86110167054362, 51.09488785914951, 49.28185393613321, 49.70153896641571, 2.271390912346308, 3.2596773433198876, 13.17987418758135, 1.9004277962788994, 1.8396343693201502, 4.4455826033843096, 1.4078545429568041, 2.189718033971448, 0.811407907640889, 0.9542774740389971, 0.9506657849489607, 6.021567074517852, 1.4908732615771654, 0.7809521938682993, 8.225959312959665, 1.7332133625848025, 11.942613488401967, 1.567005022481317, 5.26475471202724, 0.5639922371137717, 0.3664000571486561, 0.5923822674661231, 1.8265168222503712, 1.6744273711058089, 2.635053672669879, 0.72398301544521, 0.5187585733887868, 1.0573220009069244, 1.4651356705812013, 0.5526575284344081, 3.7842021460363817, 12.86816130422626, 10.224467295998851, 4.4844455025624494, 42.01314800447533, 4.2376690546751785, 11.807020202460265, 78.38045864063182, 10.694771066835692, 9.828275496162593, 23.744036820094312, 5.966667182888613, 24.249975184132616, 21.428987591449374, 10.778479574119011, 7.712902900123805, 36.271176746317074, 78.89438239737545, 149.96595884105673, 141.9663541134195, 51.8149096400123, 52.437268491353095, 25.950250544098473, 138.47514346373836, 69.93703232477358, 68.74120012678442, 42.20185517585488, 29.233891668256696, 81.10080899760533, 37.23321002668552, 45.787309171751765, 38.574416412916044, 36.34063521768098, 95.21790371405469, 37.98381666706024, 72.0198234541935, 53.440163870096896, 34.88557299581976, 68.02615395306827, 61.441010375717504, 53.35100927937221, 62.15164697047227, 56.21458478840361, 53.169952980914715, 67.559742142513, 52.49820684520659, 49.83677758642378, 61.27186500893727, 53.479753779599484, 45.02737145681626, 48.463239356291446, 48.95986945393601, 47.36875651567186, 42.28227930708422, 1.8727469882107037, 1.2699325339170022, 0.9756338827241384, 1.730265221934956, 2.0036312041417204, 1.8661258503465077, 3.0637324664292303, 2.266928211481146, 1.6345766817895067, 1.2613806263136662, 4.721503892984074, 0.5892188052294345, 1.0104461689059188, 3.3168317730092824, 1.0586487359315868, 7.765620431729272, 0.43035485991134526, 0.5835110279918065, 1.9138158866315416, 2.8528578625466965, 1.1527251770715405, 1.1274419881794935, 0.6719778083113788, 0.4103978975944538, 0.5376783817553845, 3.9744827454613643, 0.405671759094791, 0.9483183077551379, 2.322111874303206, 4.004913783621034, 5.7383843395883325, 3.613103015026026, 3.3598338139967634, 2.053423802877107, 9.16826188592705, 17.63509697936583, 6.840690847387837, 5.25483911059877, 3.775472753659017, 4.848418368502677, 13.463525757813997, 21.41019340258288, 6.777246660179801, 71.50195633127761, 18.05146746174724, 9.28531061925394, 24.47256078134967, 114.75529873393931, 6.199946270894182, 22.419975772994853, 49.843515817075435, 57.29760725137857, 21.48070054813616, 13.244316360044646, 54.35412042877094, 42.293577337749646, 46.88161964956901, 29.42746120774316, 36.66457749865371, 37.984666860327806, 32.25313050091295, 28.652655199292056, 12.342994036529934, 26.993488787448236, 27.820227447759624, 55.799320442495905, 46.259346586054725, 36.13585837966323, 84.88190161312815, 53.621554546396126, 37.34207480034036, 39.43110074435429, 43.74497327865028, 47.15637788831312, 52.11391310236587, 41.91131672528846, 46.21302899319739, 43.256888356082094, 48.449454981693776, 63.99726862156384, 55.19754481208299, 37.89512963820097, 35.66891629927722, 34.811740906020155, 43.969609807484346, 34.6481031283696, 0.8552908956367793, 20.611415647415512, 0.9879939423057779, 0.7950026129657861, 0.6005552914614403, 0.4438034504758558, 0.5900042627834837, 0.5760582773018255, 1.5675261326629477, 0.7052842785881233, 1.6701084494431577, 0.5454472301704489, 3.070271199118374, 0.2728935708471393, 0.4039182301094687, 0.40477737889829024, 1.2002360155226746, 1.2017764725143674, 0.6553751786356291, 0.7001739405115588, 0.7999143182834302, 5.061606839709288, 5.482720817930011, 0.7125184426237156, 0.2610157585082862, 1.3246298077994116, 0.2592992971225418, 0.39435558572661333, 0.668069637321414, 0.26080991599588127, 3.6341071890805927, 1.5756065651412487, 3.3470972179419056, 1.9421727906282948, 16.916518841269948, 26.98693141849563, 8.840565807111252, 38.59509811293928, 9.195910847882972, 25.48582337112351, 7.205619244403382, 5.0536057955277816, 13.635914452430272, 6.279352457143779, 74.11250702751217, 69.02923657975984, 14.168569095659647, 43.341559985148216, 23.829279303280952, 50.16708880266315, 80.87479480376611, 42.636330954333985, 95.58501977136497, 98.32251316019139, 28.22863782804968, 41.423621687243674, 38.43429781985383, 30.74471326069641, 35.2901047752338, 23.126780797831326, 26.827230248278457, 47.53074582496985, 22.50159936471528, 30.599556458497286, 44.65712364945361, 37.49514571796227, 42.712585195592105, 65.86688726964579, 45.70992782101471, 32.952605701432226, 34.908190685756104, 37.21506639913945, 33.94841447645872, 41.15474443513818, 36.67795132175436, 38.07631211910925, 35.20321531049861, 32.564553114426005, 33.06991114491238, 31.007756175060827, 4.60652567133378, 7.1358215576298845, 5.5715804569690475, 1.2851454452278552, 1.0127848605052074, 1.4032080356566512, 0.6940254778105772, 3.7337236441583554, 1.3749024177024953, 1.1177569763995805, 0.6565201018343181, 0.860476617588958, 0.8658772639525125, 0.6231309782180477, 0.8357669834022194, 1.8646001687048435, 5.516017744310053, 0.617207432558429, 2.0383977828714195, 1.0017964120153169, 0.5999184492030161, 14.329683950609086, 14.96166160885901, 0.9379724448053155, 0.740462045168422, 0.3689638925723174, 0.5432073405156697, 0.35543491264942, 0.3492697650600862, 0.5278261339379464, 26.111487973818573, 13.302959462063424, 6.639850518333905, 10.266076853105782, 8.02819451553343, 15.187684136674253, 17.244795053027712, 44.69793429193535, 6.6947638230449344, 60.952305558101294, 5.023195281280303, 50.955457906978324, 82.70092361809952, 3.978520375358152, 28.437471819738953, 8.24294710958504, 22.412766967077758, 29.518291088394566, 48.90420970126621, 24.837302978879148, 47.890916833125786, 29.079631052136012, 66.58629832799531, 43.83361139793236, 34.42389768168958, 28.618328926728758, 37.57115408639581, 35.43614728798768, 40.79645369255231, 34.73705061244919, 53.67902673644851, 32.426027201478206, 35.30948567947013, 24.31532029507483, 42.15536387581401, 30.700592374992727, 33.870190217019896, 31.167432507067346, 30.602880530970307, 29.1671808643801, 31.524823347763743, 28.01987244401508, 25.693783369954982, 1.695900641233289, 0.9129208125264286, 2.75120299714, 0.1441105747942048, 0.1441697457224114, 0.4652246295869872, 0.2361198310187855, 0.6076792487985812, 0.32001231649174355, 0.1791014948895578, 0.1318728112663229, 0.9198746775379407, 0.7861323370311506, 0.34304452520615253, 0.17788512506203213, 0.13250826710504449, 1.7985949409162825, 0.7281181836861628, 0.17964574728504187, 0.12756910240942806, 0.1283866206039887, 0.1308710398544429, 0.1658902998031382, 0.12965376894679229, 0.12694232167543906, 0.12715643421454675, 0.5864895192526525, 0.08541562891244286, 0.1277483555154657, 0.21428089974516998, 0.5759703805466615, 0.4183638693465454, 0.4693614000995523, 0.6612118889761315, 2.596073890899954, 0.4492038136661866, 0.2071183728458944, 1.073615484328969, 3.148598979333738, 2.1154911809899652, 0.6771330913725281, 5.8607280326450395, 2.808346601074837, 2.020357473602633, 5.08098318759626, 20.691658248915452, 2.830953747325587, 16.723738420060485, 6.7119560233821565, 6.569349881363665, 15.404139863996532, 7.624108055547842, 5.795836129056743, 7.437509980387973, 13.34422801184445, 7.631477265492254, 8.99109706561982, 11.923553566674906, 6.878838606753506, 28.076961272340327, 8.788698780155629, 15.068989507414658, 25.82382391837743, 9.699938283178284, 9.15085694629977, 6.612953395966467, 14.705128971227351, 10.88296955953894, 14.375034274866662, 12.695436752656857, 11.104404311078653, 22.763445418366324, 8.864502587374979, 15.175157241292334, 15.32897437487042, 11.30754551060353, 17.591339920080262, 11.484035657622677, 10.039013532765688, 11.439201868289244, 10.727300229261616, 11.359634302392593, 8.335400211046233, 11.156271472493449, 11.686611473539726, 10.067554097288776, 9.866942424025314, 10.117710407906287, 10.36489046741199], \"Total\": [1463.0, 1406.0, 798.0, 1382.0, 667.0, 709.0, 838.0, 693.0, 678.0, 620.0, 1074.0, 594.0, 797.0, 559.0, 705.0, 493.0, 426.0, 566.0, 538.0, 627.0, 825.0, 797.0, 561.0, 454.0, 562.0, 478.0, 494.0, 490.0, 438.0, 533.0, 6.111598035127078, 4.161112692143078, 9.28306145467249, 17.704128360485804, 67.00183110493275, 6.1828382274955, 16.80930175240012, 6.092586103024378, 13.509521036294892, 99.94555667845596, 7.242137494306655, 21.581550922590644, 5.96098443606453, 4.088813222203321, 20.264763153330946, 4.035667233131855, 108.97628694368825, 5.92057837318631, 3.0368435420504096, 6.891314192874321, 2.9907889053263106, 22.089071993746586, 5.082224381623292, 7.023993488164167, 11.846167500001314, 21.271640565231174, 20.883283044249072, 16.639948696919713, 8.068706471685697, 93.84928734301373, 90.38868969690112, 226.58935249133285, 142.21286657179783, 538.6032252608978, 244.58016958983632, 52.05429310774133, 43.93114518619235, 705.6476456090287, 616.7032009512208, 53.891924783525056, 47.720265921357786, 28.29801290576511, 310.12006075158115, 297.49325246104394, 1406.735403047381, 434.88695428238, 63.82036386705431, 169.467930155289, 392.91580963314334, 270.69294026463473, 825.3904222881107, 1074.7287990449652, 524.3892025712956, 458.28751038116116, 365.2364939723248, 454.07002882667916, 340.7390597789548, 327.01191027317134, 252.24459575543978, 1382.2529327229809, 411.4545856155579, 562.20164110415, 838.4955851578165, 797.8207608282154, 431.37731120752244, 627.0338153446484, 494.0836003707228, 438.34232260070274, 1463.5707816085678, 533.307140235973, 797.269560261874, 798.35582178036, 693.4487309695877, 493.65146862933886, 559.8408039596272, 622.7400849861555, 709.4968014498725, 594.3891336841618, 667.0254685271118, 566.6729524359934, 620.7345456548189, 678.0322394753802, 39.03115752018399, 23.25606263419675, 15.582625766656726, 3.8062139309814293, 5.877812578189478, 26.19501832761999, 3.781405605404888, 10.64108383937207, 2.7497347937794983, 31.02316328775163, 10.461257049071664, 2.859803577449451, 33.54419669571721, 7.740161549831565, 3.693328310159019, 4.726042552445843, 5.782878600304384, 34.004625485371704, 4.744004906651587, 2.767151967999082, 12.014148873428761, 30.875681181784863, 1.8727582881555573, 3.7215409385381055, 29.70781377714811, 2.7852281676062267, 3.754650734019728, 2.797637627921839, 3.6727341552870283, 2.8911169414262003, 31.336905506560004, 119.59653671754474, 70.1830893505526, 22.766796159929783, 9.695938339958234, 134.77769165829957, 14.066801407360659, 20.33513147098498, 98.07066384369627, 109.87301819615094, 326.85690750173603, 80.58279664946507, 1382.2529327229809, 1074.7287990449652, 301.7712161103872, 559.8408039596272, 80.74859459821818, 287.05277263443674, 241.6438695610594, 114.72441566988935, 266.5988050544309, 1406.735403047381, 317.77095577863327, 622.7400849861555, 493.65146862933886, 709.4968014498725, 533.307140235973, 825.3904222881107, 348.6694247480356, 667.0254685271118, 838.4955851578165, 798.35582178036, 547.9596081699066, 411.8927470279446, 330.41170925072674, 376.5249977280042, 411.4545856155579, 1463.5707816085678, 678.0322394753802, 487.2261684089253, 494.0836003707228, 561.3178426747953, 490.9039281198128, 594.3891336841618, 616.7032009512208, 797.269560261874, 620.7345456548189, 797.8207608282154, 478.50039261756524, 458.28751038116116, 627.0338153446484, 705.6476456090287, 566.6729524359934, 6.85480932338474, 17.15109401638846, 45.59576551108721, 5.849579283193119, 4.765464765379495, 3.8203717830985537, 26.629771501919464, 6.626287593919593, 2.8552704441045984, 6.706557945855618, 10.297921514704992, 19.121612062747904, 5.514998362748934, 63.07818946586466, 4.67022391155107, 6.440035882789489, 2.836702718736853, 5.723598188733519, 3.8272137806278166, 10.395869560677713, 7.3449363682257784, 5.550575316497999, 2.813119589634874, 2.8145135224259152, 2.842278010071463, 7.483203809346962, 6.472050663131665, 14.021287067167686, 2.851429125204045, 2.7740806449359443, 29.051633438425135, 61.610069775447734, 110.57594334076308, 39.08922439429131, 71.71498027104926, 28.294460164991033, 60.401327757815416, 10.419506730903677, 34.41846942781399, 37.89432591191294, 111.84517048422553, 797.8207608282154, 620.7345456548189, 693.4487309695877, 222.94285468348784, 524.3892025712956, 246.46130993351744, 1463.5707816085678, 231.35741621314295, 627.0338153446484, 205.84111300470474, 1382.2529327229809, 825.3904222881107, 559.8408039596272, 797.269560261874, 622.7400849861555, 235.01357572786011, 288.891454419062, 246.8581546860228, 314.972574371779, 1406.735403047381, 547.9596081699066, 411.8927470279446, 1074.7287990449652, 838.4955851578165, 420.52369375333853, 487.2261684089253, 678.0322394753802, 561.3178426747953, 798.35582178036, 667.0254685271118, 533.307140235973, 431.37731120752244, 458.28751038116116, 494.0836003707228, 566.6729524359934, 376.5249977280042, 705.6476456090287, 594.3891336841618, 616.7032009512208, 562.20164110415, 6.725718810993694, 6.552651740809111, 51.88139926022494, 36.33560976636293, 21.488260041849784, 26.064511363444307, 7.545006022602543, 34.65941658082609, 32.825680680563444, 28.67240111719336, 2.7778473710893885, 2.7656144586811124, 42.21384533501668, 9.405138900656546, 11.064465041980766, 22.090187382511658, 4.717478634489528, 7.511785134069483, 14.738513029347407, 6.470322461062254, 16.708817615018383, 3.6802351349272584, 8.49490722989371, 11.217781473676514, 10.722520808596045, 11.329251027276166, 60.015866784789495, 19.927804581219497, 3.804925077218562, 19.81949539168273, 16.66194083855039, 59.42491928999219, 15.709624427208189, 88.96238896840865, 29.41412229694631, 53.77968403197363, 19.99252486977174, 222.94285468348784, 157.23604046765828, 247.734939853903, 67.17149820560641, 145.51516142696417, 452.2321047191511, 410.9147130200588, 81.57662236060398, 38.28236553010365, 1463.5707816085678, 170.2964390799811, 99.56984262722813, 341.05724496114357, 62.0638901193205, 797.269560261874, 307.5679976406327, 797.8207608282154, 243.67810889286346, 158.51161160537836, 487.2261684089253, 838.4955851578165, 1382.2529327229809, 705.6476456090287, 798.35582178036, 709.4968014498725, 825.3904222881107, 620.7345456548189, 1074.7287990449652, 678.0322394753802, 566.6729524359934, 594.3891336841618, 562.20164110415, 667.0254685271118, 547.9596081699066, 1406.735403047381, 622.7400849861555, 561.3178426747953, 693.4487309695877, 344.4149964711902, 616.7032009512208, 454.07002882667916, 627.0338153446484, 493.65146862933886, 478.50039261756524, 538.6032252608978, 61.610069775447734, 25.558784735923386, 4.491374426658918, 26.629771501919464, 71.71498027104926, 32.97245520265232, 3.6201873702677627, 5.63283873858211, 19.121612062747904, 24.13368602898913, 8.441167399097846, 3.737310651525801, 9.04424207356909, 23.288225832517977, 9.751646800378891, 5.1402131551295325, 24.81265615423353, 2.7222630701443014, 3.7440867136499736, 5.647863239143882, 31.249079682267222, 5.456743725696745, 4.62372425042138, 84.25901130101111, 7.3449363682257784, 22.501917755896105, 2.7262785076399343, 29.051633438425135, 23.272197647879075, 18.200360463627863, 63.07818946586466, 45.59576551108721, 100.04215465786439, 59.07960141650927, 83.54487582011564, 101.73770261316243, 65.04006911579768, 72.03873055179756, 107.98409659808217, 37.1625582577875, 1463.5707816085678, 709.4968014498725, 296.99020514861127, 302.98432222863516, 566.6729524359934, 478.50039261756524, 191.22575787606667, 798.35582178036, 309.53557783414226, 678.0322394753802, 667.0254685271118, 490.9039281198128, 1382.2529327229809, 420.52369375333853, 178.8457120402037, 1406.735403047381, 594.3891336841618, 561.3178426747953, 330.41170925072674, 346.30889803606294, 392.91580963314334, 562.20164110415, 1074.7287990449652, 693.4487309695877, 547.9596081699066, 494.0836003707228, 622.7400849861555, 825.3904222881107, 705.6476456090287, 797.269560261874, 797.8207608282154, 454.07002882667916, 838.4955851578165, 616.7032009512208, 627.0338153446484, 493.65146862933886, 431.37731120752244, 533.307140235973, 9.062584309131147, 13.66695309531007, 57.95412572963354, 8.370649969530376, 8.107504373135054, 19.801154432419846, 6.382641100523172, 9.989497091538325, 3.7048655990983983, 4.371815594677619, 4.45666554822283, 28.29801290576511, 7.084975692923301, 3.715973495124942, 39.38395716216829, 8.31806265643812, 57.48138998560592, 7.5630588182204965, 25.421213531281406, 2.7376586890979464, 1.793388127749919, 2.903250536498636, 8.997077285581428, 8.28431335412336, 13.101385913448405, 3.621390350611231, 2.597442071418511, 5.323866173749496, 7.401970704336464, 2.8132678452536664, 19.334937938559154, 67.00183110493275, 53.891924783525056, 23.56453785214807, 244.58016958983632, 22.659025987785586, 69.72325530607023, 538.6032252608978, 63.25911259252861, 58.107835121252414, 155.02814896919705, 33.9131346509987, 161.93791043886418, 143.11496732336764, 66.58957979115523, 45.528429766802304, 271.298855277757, 678.0322394753802, 1463.5707816085678, 1382.2529327229809, 426.68328351014657, 434.88695428238, 191.22575787606667, 1406.735403047381, 627.0338153446484, 620.7345456548189, 349.31492419698975, 224.6357311121601, 798.35582178036, 302.98432222863516, 392.91580963314334, 327.01191027317134, 307.5679976406327, 1074.7287990449652, 329.1247444208505, 797.8207608282154, 547.9596081699066, 302.31950201574176, 797.269560261874, 693.4487309695877, 562.20164110415, 709.4968014498725, 616.7032009512208, 566.6729524359934, 838.4955851578165, 559.8408039596272, 561.3178426747953, 825.3904222881107, 705.6476456090287, 487.2261684089253, 594.3891336841618, 622.7400849861555, 667.0254685271118, 524.3892025712956, 9.590144429143235, 6.819680638214155, 5.274497722267746, 9.561081929343338, 11.176396801590897, 10.45390764510152, 17.194572973460982, 12.917837025129705, 9.422763312044307, 7.282872577870531, 27.508642860704743, 3.438365852262372, 5.955156815672053, 19.551821229744956, 6.249066474155188, 46.04233762738332, 2.5523183515933794, 3.4888173460331666, 11.508720079716822, 17.365650434191803, 7.087242380916841, 6.968256150610045, 4.232878827284551, 2.5867569210137336, 3.4016102015097482, 25.195985599988813, 2.5734103237678068, 6.03332882565626, 14.787516861832199, 25.534202014472495, 36.911650346965864, 23.19203837709113, 21.64599193696304, 13.200762441247209, 61.77416916224602, 125.61054655416604, 46.97360274891429, 36.11019595312894, 25.410285610420328, 33.619925937835454, 103.32089336427329, 173.84284180768702, 49.97419704513686, 709.4968014498725, 150.88753314148445, 71.3084368550348, 227.02397516182867, 1406.735403047381, 45.67372994380195, 211.26062442937655, 562.20164110415, 678.0322394753802, 205.84111300470474, 115.8450710602645, 667.0254685271118, 490.9039281198128, 561.3178426747953, 316.346143766295, 418.2661398761172, 438.34232260070274, 365.6192050211425, 314.2487286817492, 107.33228670597877, 302.31950201574176, 316.8804057393445, 798.35582178036, 622.7400849861555, 454.07002882667916, 1463.5707816085678, 797.269560261874, 487.2261684089253, 533.307140235973, 620.7345456548189, 705.6476456090287, 825.3904222881107, 594.3891336841618, 693.4487309695877, 627.0338153446484, 838.4955851578165, 1382.2529327229809, 1074.7287990449652, 559.8408039596272, 493.65146862933886, 478.50039261756524, 797.8207608282154, 616.7032009512208, 4.321618945590518, 107.98409659808217, 5.221850241022078, 4.327014640023879, 3.448242635474703, 2.6351515079377785, 3.546480961187467, 3.483599038779125, 9.640088007922554, 4.39247453010086, 10.461257049071664, 3.450477143242307, 19.551821229744956, 1.738355966140522, 2.584997902445167, 2.617793839161641, 7.861611702973427, 7.892379961320937, 4.315479808782482, 4.610624619398237, 5.276571705365374, 33.54419669571721, 36.441493645213484, 4.744004906651587, 1.7404366632245605, 8.845793964691536, 1.7317602037060673, 2.635606158355165, 4.472433819724787, 1.7539794379471239, 24.609820690909682, 10.774387608550775, 23.25606263419675, 13.34934092697215, 125.61054655416604, 205.84111300470474, 65.04006911579768, 316.346143766295, 71.15641056047586, 211.26062442937655, 55.6261774039122, 39.03115752018399, 119.59653671754474, 51.53588945354713, 838.4955851578165, 797.269560261874, 134.77769165829957, 493.65146862933886, 248.47345763408262, 594.3891336841618, 1074.7287990449652, 524.3892025712956, 1406.735403047381, 1463.5707816085678, 331.7412386244635, 533.307140235973, 490.9039281198128, 376.5249977280042, 454.07002882667916, 266.5988050544309, 330.41170925072674, 705.6476456090287, 272.0558803472993, 411.8927470279446, 693.4487309695877, 566.6729524359934, 709.4968014498725, 1382.2529327229809, 797.8207608282154, 494.0836003707228, 559.8408039596272, 627.0338153446484, 547.9596081699066, 825.3904222881107, 678.0322394753802, 798.35582178036, 667.0254685271118, 561.3178426747953, 620.7345456548189, 622.7400849861555, 13.639360983738037, 21.317112251421637, 17.08682301746565, 4.179794492236205, 3.3265711686516206, 5.009200197731028, 2.480806803643541, 13.441383264078192, 5.048027830684105, 4.183770688401277, 2.5070311846603826, 3.3554439349526475, 3.406633871708572, 2.519564746428411, 3.4136888961867786, 7.657699742532727, 22.702234626441246, 2.549367291091672, 8.450158751435811, 4.1757162642414976, 2.547242680902892, 63.16503636922325, 66.00884425281563, 4.2846986447147986, 3.3917107843697973, 1.702075019555924, 2.5350024984923607, 1.7056667654241842, 1.6981392188739024, 2.5784446356965858, 140.23552615688652, 78.75420876199767, 38.854854101275556, 66.46579555796889, 51.212695817462425, 105.9512685816983, 131.1266882724894, 426.68328351014657, 44.87079591924277, 693.4487309695877, 32.526582120024585, 667.0254685271118, 1463.5707816085678, 25.731533348301305, 371.59235183215304, 69.72325530607023, 271.430326128861, 398.97616360595714, 798.35582178036, 316.8804057393445, 838.4955851578165, 411.4545856155579, 1406.735403047381, 797.269560261874, 561.3178426747953, 438.34232260070274, 678.0322394753802, 620.7345456548189, 797.8207608282154, 616.7032009512208, 1382.2529327229809, 594.3891336841618, 705.6476456090287, 344.4149964711902, 1074.7287990449652, 566.6729524359934, 709.4968014498725, 627.0338153446484, 622.7400849861555, 559.8408039596272, 825.3904222881107, 562.20164110415, 458.28751038116116, 25.096555408264823, 13.968153445307513, 46.41581101666233, 2.4424981166764343, 2.4638211061938455, 8.025173434776482, 4.1071160588180025, 10.748627022109108, 5.777593236234469, 3.240435332735729, 2.432812300568715, 16.976119705337105, 14.573730863337762, 6.381041625183273, 3.323009800451197, 2.4771670630321894, 33.9131346509987, 13.794486077844093, 3.4111151499792323, 2.430709219549446, 2.4529224144333543, 2.5142046614509654, 3.2038240938583, 2.5048645918849544, 2.480188477304539, 2.4893134496428675, 11.487020143507136, 1.6758728478183418, 2.511047819972119, 4.2237972100398995, 11.434039978395258, 8.295033238489243, 9.423768712467599, 13.395249920763385, 57.95412572963354, 9.37443342045531, 4.11399523007995, 25.05896577810864, 82.75992094800739, 54.6910682631992, 15.33387212174147, 174.4762306111041, 76.79326355486177, 53.33036753995414, 155.02814896919705, 798.35582178036, 81.57662236060398, 667.0254685271118, 227.02397516182867, 221.6334004671562, 620.7345456548189, 272.2205497679542, 195.05460120562742, 271.298855277757, 559.8408039596272, 280.8472557153662, 344.4149964711902, 493.65146862933886, 248.47345763408262, 1463.5707816085678, 340.7390597789548, 678.0322394753802, 1382.2529327229809, 392.91580963314334, 365.2364939723248, 240.38902723333467, 709.4968014498725, 478.50039261756524, 705.6476456090287, 594.3891336841618, 494.0836003707228, 1406.735403047381, 365.6192050211425, 825.3904222881107, 838.4955851578165, 538.6032252608978, 1074.7287990449652, 566.6729524359934, 454.07002882667916, 616.7032009512208, 547.9596081699066, 627.0338153446484, 341.05724496114357, 693.4487309695877, 797.8207608282154, 562.20164110415, 533.307140235973, 622.7400849861555, 797.269560261874], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -9.8551, -10.2563, -9.4648, -8.8262, -7.5004, -9.8898, -8.8908, -9.9102, -9.1157, -7.12, -9.7524, -8.6738, -9.9636, -10.3447, -8.7509, -10.3784, -7.084, -9.9976, -10.6671, -9.8631, -10.7033, -8.7095, -10.1791, -9.8562, -9.3406, -8.7592, -8.7833, -9.0158, -9.7401, -7.2906, -7.3587, -6.4651, -6.9277, -5.6447, -6.4211, -7.9306, -8.0948, -5.4781, -5.6095, -7.912, -8.0333, -8.5237, -6.3216, -6.3638, -4.9771, -6.0437, -7.7767, -6.8979, -6.1422, -6.4866, -5.4944, -5.2717, -5.9143, -6.0404, -6.2431, -6.0587, -6.3087, -6.3477, -6.5717, -5.1131, -6.1588, -5.9004, -5.5675, -5.637, -6.1356, -5.847, -6.0343, -6.1416, -5.3189, -6.0168, -5.7631, -5.7787, -5.904, -6.0983, -6.038, -5.9875, -5.9362, -6.0216, -5.9921, -6.0618, -6.0458, -6.0954, -7.5766, -8.1034, -8.5833, -10.0208, -9.6408, -8.1471, -10.0831, -9.0535, -10.4185, -7.9965, -9.088, -10.3856, -7.9282, -9.3963, -10.1601, -9.9199, -9.7281, -7.9584, -9.9316, -10.4761, -9.0113, -8.0693, -10.8795, -10.207, -8.141, -10.5089, -10.2112, -10.5075, -10.2387, -10.4793, -8.1095, -6.8191, -7.3534, -8.4515, -9.2889, -6.7485, -8.942, -8.5931, -7.1279, -7.0955, -6.1318, -7.3931, -4.866, -5.1281, -6.2563, -5.723, -7.4112, -6.3205, -6.4995, -7.1286, -6.4292, -5.0695, -6.3124, -5.7734, -5.9614, -5.6743, -5.9016, -5.5652, -6.2555, -5.7639, -5.5903, -5.649, -5.9282, -6.1616, -6.3267, -6.2356, -6.1829, -5.3145, -5.8489, -6.0708, -6.0702, -5.9944, -6.0787, -5.9733, -5.9533, -5.8369, -6.023, -5.9262, -6.1603, -6.1775, -6.1195, -6.119, -6.165, -9.3227, -8.412, -7.4503, -9.5418, -9.8155, -10.0591, -8.1214, -9.5302, -10.3931, -9.5444, -9.1199, -8.5079, -9.7544, -7.3195, -9.9291, -9.6142, -10.4363, -9.735, -10.1529, -9.1584, -9.5105, -9.7942, -10.4742, -10.4787, -10.4705, -9.5041, -9.6514, -8.8833, -10.481, -10.509, -8.1691, -7.4352, -6.8664, -7.8865, -7.3093, -8.2226, -7.5211, -9.2019, -8.117, -8.0521, -7.0893, -5.3506, -5.5738, -5.4875, -6.4979, -5.7641, -6.4212, -4.9159, -6.5039, -5.6625, -6.6226, -5.0313, -5.4765, -5.8047, -5.5166, -5.7346, -6.5343, -6.368, -6.5048, -6.318, -5.1934, -5.9121, -6.135, -5.4612, -5.6569, -6.1441, -6.0492, -5.8394, -5.9868, -5.7918, -5.9054, -6.0414, -6.1753, -6.1429, -6.1091, -6.0572, -6.259, -6.0087, -6.1121, -6.1254, -6.1738, -9.5221, -9.5622, -7.5113, -7.8679, -8.3938, -8.201, -9.4418, -7.9509, -8.0174, -8.1597, -10.5035, -10.5274, -7.8093, -9.3265, -9.1676, -8.4764, -10.0227, -9.5586, -8.8852, -9.7108, -8.7639, -10.2795, -9.4451, -9.1808, -9.2269, -9.1719, -7.5064, -8.6131, -10.2714, -8.6285, -8.8024, -7.5587, -8.8619, -7.2002, -8.2748, -7.7091, -8.6401, -6.3785, -6.7042, -6.3082, -7.54, -6.8424, -5.849, -5.9648, -7.3972, -8.0677, -4.944, -6.7922, -7.2543, -6.2244, -7.6607, -5.5193, -6.3307, -5.5422, -6.5473, -6.9008, -6.0135, -5.6018, -5.2196, -5.751, -5.6577, -5.7572, -5.6455, -5.8678, -5.5001, -5.8369, -5.9711, -5.9416, -5.9866, -5.8816, -6.0239, -5.4065, -5.9414, -6.038, -5.9435, -6.3483, -6.0585, -6.2107, -6.1582, -6.2248, -6.242, -6.2343, -7.4068, -8.2929, -10.0506, -8.2812, -7.2947, -8.0936, -10.3089, -9.869, -8.6473, -8.4161, -9.4719, -10.292, -9.4084, -8.4627, -9.35, -9.9987, -8.4257, -10.642, -10.3254, -9.9261, -8.22, -9.976, -10.1437, -7.2412, -9.682, -8.5789, -10.6937, -8.33, -8.5584, -8.8071, -7.5678, -7.8943, -7.1165, -7.6449, -7.3205, -7.1515, -7.5865, -7.5047, -7.1268, -8.1391, -4.7539, -5.4596, -6.2642, -6.2557, -5.7326, -5.8906, -6.7047, -5.4622, -6.3234, -5.6712, -5.6885, -5.9549, -5.1066, -6.0927, -6.7986, -5.1583, -5.8455, -5.9137, -6.3251, -6.2964, -6.2006, -5.9351, -5.4591, -5.7965, -6.0007, -6.0738, -5.9194, -5.7389, -5.8431, -5.7747, -5.7879, -6.1455, -5.8106, -5.9853, -6.0018, -6.1602, -6.1963, -6.1878, -9.1678, -8.8066, -7.4095, -9.3462, -9.3787, -8.4963, -9.6462, -9.2045, -10.1972, -10.035, -10.0388, -8.1929, -9.5889, -10.2355, -7.8809, -9.4383, -7.5081, -9.5391, -8.3272, -10.561, -10.9923, -10.5118, -9.3858, -9.4728, -9.0193, -10.3112, -10.6446, -9.9325, -9.6063, -10.5813, -8.6574, -7.4335, -7.6635, -8.4876, -6.2503, -8.5442, -7.5195, -5.6267, -7.6185, -7.703, -6.8209, -8.2021, -6.7998, -6.9235, -7.6107, -7.9453, -6.3972, -5.6201, -4.9778, -5.0327, -6.0406, -6.0286, -6.7321, -5.0575, -5.7406, -5.7579, -6.2458, -6.6129, -5.5925, -6.371, -6.1642, -6.3357, -6.3953, -5.4321, -6.3511, -5.7113, -6.0097, -6.4362, -5.7683, -5.8702, -6.0113, -5.8587, -5.9591, -6.0147, -5.7752, -6.0275, -6.0795, -5.8729, -6.0089, -6.181, -6.1074, -6.0972, -6.1303, -6.2439, -9.0719, -9.4603, -9.724, -9.151, -9.0043, -9.0754, -8.5797, -8.8809, -9.2079, -9.4671, -8.1472, -10.2282, -9.6889, -8.5003, -9.6423, -7.6496, -10.5424, -10.238, -9.0502, -8.651, -9.5572, -9.5793, -10.0968, -10.5899, -10.3198, -8.3194, -10.6015, -9.7524, -8.8568, -8.3118, -7.9521, -8.4147, -8.4874, -8.9798, -7.4835, -6.8294, -7.7764, -8.0401, -8.3708, -8.1206, -7.0993, -6.6354, -7.7857, -5.4296, -6.8061, -7.4709, -6.5017, -4.9565, -7.8747, -6.5893, -5.7904, -5.651, -6.6321, -7.1157, -5.7038, -5.9546, -5.8517, -6.3174, -6.0975, -6.0621, -6.2257, -6.344, -7.1862, -6.4037, -6.3735, -5.6775, -5.865, -6.112, -5.258, -5.7173, -6.0792, -6.0247, -5.9209, -5.8458, -5.7459, -5.9637, -5.866, -5.9321, -5.8188, -5.5404, -5.6884, -6.0645, -6.125, -6.1493, -5.9158, -6.154, -9.7617, -6.5796, -9.6175, -9.8348, -10.1153, -10.4178, -10.133, -10.157, -9.1559, -9.9546, -9.0925, -10.2116, -8.4836, -10.9041, -10.512, -10.5098, -9.4229, -9.4216, -10.028, -9.9618, -9.8287, -7.9837, -7.9038, -9.9444, -10.9486, -9.3243, -10.9552, -10.5359, -10.0088, -10.9494, -8.3151, -9.1508, -8.3973, -8.9416, -6.7771, -6.3101, -7.4261, -5.9523, -7.3867, -6.3673, -7.6306, -7.9853, -6.9927, -7.7681, -5.2998, -5.3709, -6.9544, -5.8363, -6.4345, -5.6901, -5.2125, -5.8527, -5.0454, -5.0172, -6.2651, -5.8816, -5.9565, -6.1797, -6.0418, -6.4644, -6.316, -5.744, -6.4918, -6.1844, -5.8064, -5.9812, -5.8509, -5.4178, -5.7831, -6.1103, -6.0527, -5.9887, -6.0806, -5.8881, -6.0032, -5.9658, -6.0443, -6.1222, -6.1068, -6.1712, -7.9385, -7.5009, -7.7483, -9.2151, -9.4533, -9.1272, -9.8312, -8.1486, -9.1476, -9.3547, -9.8868, -9.6163, -9.61, -9.939, -9.6454, -8.8429, -7.7583, -9.9485, -8.7538, -9.4642, -9.9769, -6.8036, -6.7605, -9.53, -9.7665, -10.463, -10.0762, -10.5004, -10.5179, -10.105, -6.2036, -6.878, -7.5729, -7.1371, -7.383, -6.7455, -6.6185, -5.6661, -7.5647, -5.3559, -7.8519, -5.535, -5.0508, -8.0851, -6.1183, -7.3566, -6.3564, -6.081, -5.5761, -6.2536, -5.5971, -6.0959, -5.2675, -5.6856, -5.9272, -6.1119, -5.8397, -5.8982, -5.7574, -5.9182, -5.483, -5.987, -5.9018, -6.2749, -5.7246, -6.0417, -5.9434, -6.0266, -6.0449, -6.0929, -6.0152, -6.1331, -6.2197, -7.9599, -8.5792, -7.4761, -10.4253, -10.4249, -9.2533, -9.9315, -8.9862, -9.6275, -10.2079, -10.514, -8.5716, -8.7287, -9.558, -10.2147, -10.5092, -7.9011, -8.8054, -10.2049, -10.5472, -10.5408, -10.5216, -10.2845, -10.531, -10.5521, -10.5504, -9.0217, -10.9483, -10.5458, -10.0286, -9.0398, -9.3595, -9.2445, -8.9018, -7.5341, -9.2884, -10.0626, -8.4171, -7.3411, -7.7388, -8.878, -6.7198, -7.4555, -7.7848, -6.8626, -5.4584, -7.4475, -5.6713, -6.5842, -6.6057, -5.7535, -6.4568, -6.731, -6.4816, -5.897, -6.4558, -6.2919, -6.0096, -6.5596, -5.1531, -6.3146, -5.7755, -5.2368, -6.216, -6.2742, -6.5991, -5.7999, -6.1009, -5.8226, -5.9469, -6.0808, -5.3629, -6.306, -5.7684, -5.7584, -6.0626, -5.6207, -6.0471, -6.1816, -6.051, -6.1153, -6.058, -6.3676, -6.0761, -6.0297, -6.1788, -6.1989, -6.1738, -6.1497], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.6946, 0.6778, 0.6669, 0.6599, 0.6547, 0.6483, 0.6472, 0.6426, 0.6408, 0.6352, 0.6276, 0.6142, 0.6111, 0.6069, 0.6001, 0.5863, 0.5848, 0.5839, 0.582, 0.5665, 0.561, 0.5553, 0.555, 0.5543, 0.5473, 0.5433, 0.5376, 0.5322, 0.5317, 0.5276, 0.4971, 0.4716, 0.4748, 0.4262, 0.4392, 0.4769, 0.4824, 0.3227, 0.326, 0.4609, 0.4612, 0.4934, 0.3013, 0.3007, 0.1337, 0.2411, 0.4272, 0.3293, 0.2441, 0.2723, 0.1496, 0.1084, 0.1833, 0.192, 0.2163, 0.183, 0.22, 0.2222, 0.2578, 0.0154, 0.1814, 0.1277, 0.0608, 0.041, 0.1573, 0.0719, 0.1229, 0.1353, -0.2477, 0.064, -0.0844, -0.1014, -0.0858, 0.0597, -0.0058, -0.0617, -0.1408, -0.0493, -0.1351, -0.0417, -0.1169, -0.2547, 1.1189, 1.1099, 1.0304, 1.0024, 0.9479, 0.9472, 0.9467, 0.9417, 0.9299, 0.9287, 0.9242, 0.9236, 0.9188, 0.9171, 0.8932, 0.8869, 0.8769, 0.875, 0.8714, 0.8659, 0.8625, 0.8606, 0.853, 0.8388, 0.8275, 0.8266, 0.8257, 0.8236, 0.8202, 0.8189, 0.8056, 0.7567, 0.7554, 0.7831, 0.7993, 0.7078, 0.774, 0.7545, 0.6463, 0.565, 0.4386, 0.5775, 0.2624, 0.252, 0.3939, 0.3093, 0.5573, 0.3797, 0.3729, 0.4888, 0.345, 0.0413, 0.2861, 0.1524, 0.1967, 0.1211, 0.1791, 0.0788, 0.2503, 0.0931, 0.0379, 0.0283, 0.1255, 0.1775, 0.2328, 0.1933, 0.1573, -0.2432, -0.0082, 0.1003, 0.087, 0.0352, 0.0849, -0.001, -0.0178, -0.1582, -0.0941, -0.2482, 0.0289, 0.0549, -0.2006, -0.3182, -0.1449, 1.1123, 1.1058, 1.0898, 1.0517, 0.983, 0.9605, 0.9564, 0.9387, 0.9176, 0.9124, 0.908, 0.9011, 0.898, 0.896, 0.8896, 0.8832, 0.8809, 0.8803, 0.8648, 0.8601, 0.8553, 0.8518, 0.8514, 0.8464, 0.8447, 0.8431, 0.841, 0.836, 0.831, 0.8306, 0.8217, 0.8038, 0.7877, 0.8075, 0.7779, 0.7946, 0.7377, 0.8143, 0.7043, 0.673, 0.5534, 0.3274, 0.3552, 0.3307, 0.455, 0.3335, 0.4315, 0.1554, 0.4121, 0.2564, 0.4102, 0.0971, 0.1675, 0.2275, 0.1621, 0.1912, 0.366, 0.3258, 0.3463, 0.2894, -0.0826, 0.1416, 0.2042, -0.0812, -0.0286, 0.1743, 0.1219, 0.0013, 0.0428, -0.1145, -0.0483, 0.0394, 0.1176, 0.0895, 0.0481, -0.0371, 0.1699, -0.2079, -0.1398, -0.1899, -0.1458, 0.9319, 0.9178, 0.8996, 0.8992, 0.8985, 0.8983, 0.8972, 0.8634, 0.8513, 0.8442, 0.8348, 0.8152, 0.8078, 0.7921, 0.7885, 0.7884, 0.7859, 0.7848, 0.7842, 0.7818, 0.78, 0.7774, 0.7753, 0.7615, 0.7607, 0.7606, 0.7589, 0.7547, 0.7522, 0.7447, 0.7444, 0.7165, 0.7437, 0.6715, 0.7036, 0.6659, 0.7245, 0.5744, 0.5979, 0.5393, 0.6126, 0.5372, 0.3967, 0.3767, 0.5612, 0.6472, 0.1272, 0.4302, 0.5048, 0.3035, 0.571, 0.1594, 0.3005, 0.1358, 0.3167, 0.3933, 0.1577, 0.0265, -0.0911, 0.0498, 0.0196, 0.0381, -0.0015, 0.0612, -0.12, 0.0038, 0.049, 0.0308, 0.0415, -0.0245, 0.0297, -0.2957, -0.0157, -0.0084, -0.1253, 0.1698, -0.123, 0.0309, -0.2393, -0.0667, -0.0528, -0.1634, 0.8322, 0.826, 0.8071, 0.7967, 0.7925, 0.7706, 0.7644, 0.7623, 0.7618, 0.7602, 0.7549, 0.7495, 0.7494, 0.7492, 0.7324, 0.7241, 0.7228, 0.7164, 0.7143, 0.7025, 0.6978, 0.687, 0.685, 0.6848, 0.6839, 0.6674, 0.6633, 0.6608, 0.6542, 0.6513, 0.6477, 0.6458, 0.6377, 0.6361, 0.614, 0.5859, 0.5984, 0.578, 0.5511, 0.6055, 0.3174, 0.3357, 0.402, 0.3905, 0.2875, 0.2987, 0.4017, 0.2151, 0.3014, 0.1695, 0.1685, 0.2087, 0.0218, 0.2257, 0.3747, -0.0475, 0.1269, 0.1159, 0.2345, 0.2162, 0.1857, 0.0929, -0.079, 0.0217, 0.053, 0.0833, 0.0063, -0.0949, -0.0424, -0.096, -0.1099, 0.0961, -0.1823, -0.0498, -0.083, -0.0021, 0.0966, -0.107, 0.9879, 0.9383, 0.8907, 0.889, 0.8884, 0.8778, 0.8601, 0.8539, 0.853, 0.8497, 0.8266, 0.8242, 0.813, 0.8118, 0.8056, 0.8032, 0.8003, 0.7975, 0.7971, 0.7918, 0.7835, 0.7822, 0.7772, 0.7728, 0.7678, 0.7618, 0.7608, 0.7552, 0.7518, 0.7443, 0.7406, 0.7217, 0.7094, 0.7125, 0.6101, 0.6951, 0.5958, 0.4442, 0.5942, 0.5946, 0.4954, 0.634, 0.4728, 0.4727, 0.5506, 0.5962, 0.3594, 0.2206, 0.0934, 0.0958, 0.2633, 0.2562, 0.3744, 0.0533, 0.1782, 0.1711, 0.2581, 0.3325, 0.0848, 0.2752, 0.2221, 0.2342, 0.2359, -0.052, 0.2124, -0.0333, 0.044, 0.2122, -0.0897, -0.052, 0.0167, -0.0633, -0.0236, 0.0054, -0.147, 0.0048, -0.0499, -0.2289, -0.2082, -0.0098, -0.1351, -0.1715, -0.2732, -0.1462, 1.0273, 0.9797, 0.973, 0.9512, 0.9418, 0.9375, 0.9356, 0.9204, 0.9089, 0.9073, 0.8982, 0.8966, 0.8867, 0.8865, 0.8852, 0.8807, 0.8805, 0.8723, 0.8666, 0.8544, 0.8444, 0.8392, 0.8202, 0.8196, 0.8159, 0.8138, 0.8132, 0.8102, 0.8093, 0.8081, 0.7992, 0.8014, 0.7977, 0.7998, 0.7529, 0.6973, 0.7339, 0.7332, 0.754, 0.7241, 0.6227, 0.5663, 0.6627, 0.3658, 0.5373, 0.622, 0.4331, 0.1544, 0.6636, 0.4175, 0.2376, 0.1897, 0.4006, 0.4919, 0.1533, 0.209, 0.1779, 0.2857, 0.2263, 0.2148, 0.2326, 0.2657, 0.4978, 0.2447, 0.2278, -0.0002, 0.0607, 0.1296, -0.1868, -0.0386, 0.092, 0.0561, 0.0081, -0.045, -0.1018, 0.0086, -0.0478, -0.0132, -0.1905, -0.412, -0.3083, -0.0322, 0.033, 0.0399, -0.2378, -0.2185, 1.1345, 1.0983, 1.0895, 1.0602, 1.0067, 0.9732, 0.9609, 0.9549, 0.938, 0.9254, 0.9197, 0.9098, 0.9032, 0.9029, 0.8982, 0.8877, 0.875, 0.8724, 0.8697, 0.8697, 0.8679, 0.8633, 0.8604, 0.8586, 0.8572, 0.8557, 0.8556, 0.8549, 0.8532, 0.8486, 0.8417, 0.8319, 0.816, 0.8268, 0.7496, 0.7227, 0.7588, 0.6508, 0.7083, 0.6395, 0.7107, 0.7102, 0.5831, 0.6495, 0.3284, 0.3078, 0.5019, 0.3217, 0.41, 0.2823, 0.1675, 0.2449, 0.0655, 0.0541, 0.2904, 0.1992, 0.2072, 0.2492, 0.1998, 0.3097, 0.2435, 0.0567, 0.262, 0.1547, 0.0118, 0.0389, -0.0556, -0.2894, -0.1051, 0.0468, -0.0205, -0.0698, -0.0269, -0.244, -0.1626, -0.2885, -0.1872, -0.0926, -0.1778, -0.2454, 1.8084, 1.7995, 1.7733, 1.7145, 1.7047, 1.6214, 1.6201, 1.613, 1.5933, 1.574, 1.554, 1.533, 1.5242, 1.4968, 1.4867, 1.4812, 1.4791, 1.4755, 1.4719, 1.4664, 1.4479, 1.4105, 1.4096, 1.3748, 1.3721, 1.365, 1.3534, 1.3255, 1.3125, 1.3077, 1.213, 1.1156, 1.1272, 1.0261, 1.0409, 0.9514, 0.8652, 0.6378, 0.9914, 0.4623, 1.0259, 0.322, 0.0205, 1.0271, 0.3238, 0.7587, 0.3998, 0.29, 0.1012, 0.3477, 0.0312, 0.2442, -0.1566, -0.0069, 0.1024, 0.1649, 0.0009, 0.0307, -0.0794, 0.0173, -0.3545, -0.0147, -0.1011, 0.2432, -0.3446, -0.0216, -0.1481, -0.1077, -0.1191, -0.0607, -0.3712, -0.105, 0.0127, 1.1773, 1.1439, 1.0462, 1.0416, 1.0333, 1.024, 1.0157, 0.9989, 0.9784, 0.9763, 0.9568, 0.9565, 0.9519, 0.9486, 0.9443, 0.9436, 0.935, 0.9302, 0.928, 0.9245, 0.9218, 0.9163, 0.911, 0.9107, 0.8994, 0.8974, 0.897, 0.8952, 0.8934, 0.8906, 0.8835, 0.8847, 0.8722, 0.8632, 0.7661, 0.8335, 0.8829, 0.7216, 0.6028, 0.6194, 0.7518, 0.4783, 0.5633, 0.5986, 0.4537, 0.219, 0.5109, 0.1858, 0.3506, 0.3532, 0.1755, 0.2965, 0.3556, 0.2751, 0.1352, 0.2663, 0.2262, 0.1485, 0.2849, -0.0819, 0.2141, 0.0652, -0.1084, 0.1703, 0.1851, 0.2786, -0.0046, 0.0883, -0.0218, 0.0255, 0.0764, -0.2521, 0.1523, -0.1244, -0.1301, 0.0083, -0.2406, -0.027, 0.06, -0.1156, -0.0616, -0.1391, 0.1602, -0.2579, -0.3517, -0.1508, -0.1181, -0.2481, -0.471]}, \"token.table\": {\"Topic\": [1, 2, 5, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 1, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 8, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 7, 8, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 1, 2, 6, 7, 1, 2, 3, 4, 6, 1, 3, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 4, 5, 6, 7, 8, 1, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 7, 9, 1, 2, 4, 5, 6, 7, 9, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 1, 3, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 5, 3, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 8, 2, 1, 2, 1, 2, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 8, 1, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 1, 3, 8, 1, 2, 3, 4, 5, 6, 7, 2, 5, 7, 1, 2, 3, 4, 5, 6, 7, 1, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 4, 5, 7, 8, 9, 1, 3, 1, 2, 3, 5, 6, 1, 2, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 5, 1, 3, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 6, 1, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 6, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 6, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 6, 3, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 1, 2, 4, 6, 9, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 1, 2, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 2, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 2, 6, 9, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 2, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 5, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 9, 2, 4, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 6, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 1, 2, 3, 4, 5, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 4, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 8, 1, 3, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 1, 2, 6, 8, 1, 2, 3, 4, 5, 6, 1, 3, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"Freq\": [0.39352847293239507, 0.19676423646619753, 0.19676423646619753, 0.07439710484801654, 0.14879420969603308, 0.07439710484801654, 0.07439710484801654, 0.07439710484801654, 0.07439710484801654, 0.07439710484801654, 0.29758841939206615, 0.10036228485925912, 0.10036228485925912, 0.10036228485925912, 0.2509057121481478, 0.15054342728888867, 0.10036228485925912, 0.05018114242962956, 0.10036228485925912, 0.05018114242962956, 0.29504107328261675, 0.12391725077869904, 0.17702464396957004, 0.10621478638174203, 0.05900821465652335, 0.06490903612217569, 0.06490903612217569, 0.029504107328261676, 0.05900821465652335, 0.017702464396957007, 0.2948364597029775, 0.2948364597029775, 0.2948364597029775, 0.1786471988948198, 0.10421086602197822, 0.13398539917111485, 0.20842173204395645, 0.059549066298273265, 0.029774533149136632, 0.07443633287284158, 0.07443633287284158, 0.10421086602197822, 0.014887266574568316, 0.18990881354421407, 0.11221884436703558, 0.15537993835435696, 0.12085106316449985, 0.09495440677210704, 0.07768996917717848, 0.11221884436703558, 0.060425531582249926, 0.060425531582249926, 0.01726443759492855, 0.2045382043051828, 0.09297191104781037, 0.11156629325737244, 0.22313258651474488, 0.11156629325737244, 0.037188764419124144, 0.07437752883824829, 0.07437752883824829, 0.05578314662868622, 0.018594382209562072, 0.2500934740643935, 0.15005608443863608, 0.0500186948128787, 0.2500934740643935, 0.0500186948128787, 0.1000373896257574, 0.0500186948128787, 0.0500186948128787, 0.0500186948128787, 0.21689035272850224, 0.21689035272850224, 0.21689035272850224, 0.24668503499389777, 0.12334251749694888, 0.12334251749694888, 0.12334251749694888, 0.12334251749694888, 0.24668503499389777, 0.2391300951722682, 0.17399738442495777, 0.11723887934515871, 0.10328186989930649, 0.0958381315281853, 0.08839439315706411, 0.05117570130145817, 0.07536785100760203, 0.03907962644838624, 0.016748411335022674, 0.21364785040516796, 0.16023588780387596, 0.15538025484012216, 0.11653519113009161, 0.09711265927507634, 0.07526231093818417, 0.046128513155661265, 0.07526231093818417, 0.041272880191907446, 0.01942253185501527, 0.1655592744599242, 0.11589149212194694, 0.2648948391358787, 0.06622370978396967, 0.0827796372299621, 0.14900334701393178, 0.06622370978396967, 0.033111854891984836, 0.033111854891984836, 0.016555927445992418, 0.1870597939656672, 0.1407402259360734, 0.13183261669961308, 0.11579892007398444, 0.11579892007398444, 0.08907609236460343, 0.08373152682272722, 0.05879022096063826, 0.060571742807930325, 0.016033696625628615, 0.4164360961037839, 0.11898174174393826, 0.05949087087196913, 0.05949087087196913, 0.11898174174393826, 0.05949087087196913, 0.05949087087196913, 0.10988793348334276, 0.21977586696668552, 0.10988793348334276, 0.05494396674167138, 0.21977586696668552, 0.10988793348334276, 0.16483190022501415, 0.05494396674167138, 0.14350792772054652, 0.14350792772054652, 0.14350792772054652, 0.14350792772054652, 0.14350792772054652, 0.14350792772054652, 0.14350792772054652, 0.2503608724753827, 0.11822596755781961, 0.14372568605068267, 0.10199887397145221, 0.11358965510457178, 0.09040809283833263, 0.05795390566559785, 0.05563574943897393, 0.046363124532478275, 0.01854524981299131, 0.2691033483351988, 0.11620371859929039, 0.12231970378872672, 0.11620371859929039, 0.08868178524682688, 0.11926171119400855, 0.06115985189436336, 0.04281189632605435, 0.045869888920772525, 0.021405948163027176, 0.2348845355551661, 0.11744226777758306, 0.11744226777758306, 0.18203551505525373, 0.08220958744430813, 0.0880817008331873, 0.05284902049991237, 0.07046536066654983, 0.041104793722154066, 0.011744226777758306, 0.4891394865237373, 0.19963266799617252, 0.19963266799617252, 0.19963266799617252, 0.139611117612934, 0.09307407840862265, 0.09307407840862265, 0.279222235225868, 0.04653703920431133, 0.04653703920431133, 0.04653703920431133, 0.09307407840862265, 0.09307407840862265, 0.21334622694486655, 0.10667311347243327, 0.10667311347243327, 0.10667311347243327, 0.10667311347243327, 0.10667311347243327, 0.10667311347243327, 0.10667311347243327, 0.19709481938039544, 0.15329597062919645, 0.14599616250399663, 0.12044683406579722, 0.10949712187799747, 0.09672245765889777, 0.05292360890769878, 0.06204836906419857, 0.04014894468859907, 0.020074472344299536, 0.2235964595351941, 0.09782345104664743, 0.10481084040712224, 0.10481084040712224, 0.1257730084885467, 0.14673517656997115, 0.06288650424427335, 0.048911725523323714, 0.05589911488379853, 0.027949557441899264, 0.08784723093889063, 0.30746530828611723, 0.08784723093889063, 0.08784723093889063, 0.043923615469445315, 0.13177084640833595, 0.13177084640833595, 0.043923615469445315, 0.043923615469445315, 0.11631577027745361, 0.1744736554161804, 0.05815788513872681, 0.05815788513872681, 0.1744736554161804, 0.05815788513872681, 0.1744736554161804, 0.05815788513872681, 0.05815788513872681, 0.1466344324683606, 0.1466344324683606, 0.1466344324683606, 0.1466344324683606, 0.1466344324683606, 0.1466344324683606, 0.1466344324683606, 0.2534089856040411, 0.12670449280202056, 0.12670449280202056, 0.12670449280202056, 0.12670449280202056, 0.12670449280202056, 0.12670449280202056, 0.12670449280202056, 0.06521509975175337, 0.1956452992552601, 0.13043019950350673, 0.1956452992552601, 0.06521509975175337, 0.06521509975175337, 0.13043019950350673, 0.06521509975175337, 0.06521509975175337, 0.06521509975175337, 0.2935449002326914, 0.2935449002326914, 0.34144340959986014, 0.09105157589329604, 0.06828868191997203, 0.11381446986662005, 0.06828868191997203, 0.09105157589329604, 0.06828868191997203, 0.06828868191997203, 0.06828868191997203, 0.02276289397332401, 0.36138239300355324, 0.3830815290416261, 0.09577038226040653, 0.047885191130203265, 0.047885191130203265, 0.1436555733906098, 0.1436555733906098, 0.047885191130203265, 0.047885191130203265, 0.047885191130203265, 0.35903701234626434, 0.48064067185115084, 0.24032033592557542, 0.267066534899268, 0.143805057253452, 0.09978310095137485, 0.08510911551734915, 0.08804391260415428, 0.10565269512498514, 0.07923952134373885, 0.06456553590971315, 0.035217565041661715, 0.026413173781246285, 0.20509356429134531, 0.10254678214567266, 0.10254678214567266, 0.20509356429134531, 0.10254678214567266, 0.10254678214567266, 0.10254678214567266, 0.20311579144551756, 0.18618947549172443, 0.18618947549172443, 0.05077894786137939, 0.20311579144551756, 0.06770526381517251, 0.03385263190758626, 0.03385263190758626, 0.03385263190758626, 0.01692631595379313, 0.28981499035821195, 0.28981499035821195, 0.28981499035821195, 0.3554585112424061, 0.1493066580937686, 0.1493066580937686, 0.0746533290468843, 0.1493066580937686, 0.0746533290468843, 0.1493066580937686, 0.0746533290468843, 0.0746533290468843, 0.0746533290468843, 0.0746533290468843, 0.17378127073616434, 0.08689063536808217, 0.08689063536808217, 0.08689063536808217, 0.08689063536808217, 0.08689063536808217, 0.17378127073616434, 0.08689063536808217, 0.19952938378518278, 0.0798117535140731, 0.11971763027110967, 0.11971763027110967, 0.11971763027110967, 0.11971763027110967, 0.03990587675703655, 0.0798117535140731, 0.0798117535140731, 0.03990587675703655, 0.25766950596217286, 0.11231747695787021, 0.08809213879048644, 0.12112669083691886, 0.11451978042763238, 0.0902944422602486, 0.07928292491143779, 0.07708062144167564, 0.039641462455718895, 0.02202303469762161, 0.38783070466426467, 0.19046601109702407, 0.1015818725850795, 0.08888413851194457, 0.1269773407313494, 0.08888413851194457, 0.08888413851194457, 0.0634886703656747, 0.05079093629253975, 0.1650705429507542, 0.025395468146269876, 0.09498925900917332, 0.11082080217736887, 0.0633261726727822, 0.1266523453455644, 0.07915771584097776, 0.14248388851375998, 0.07915771584097776, 0.0633261726727822, 0.22164160435473773, 0.0316630863363911, 0.12119588049988857, 0.1666443356873468, 0.06059794024994428, 0.09089691037491643, 0.07574742531243035, 0.12119588049988857, 0.07574742531243035, 0.03029897012497214, 0.22724227593729107, 0.01514948506248607, 0.14499772187480472, 0.13291457838523765, 0.10874829140610354, 0.19333029583307296, 0.09666514791653648, 0.10874829140610354, 0.0604157174478353, 0.07249886093740236, 0.02416628697913412, 0.03624943046870118, 0.2147742729020367, 0.1431828486013578, 0.0715914243006789, 0.1431828486013578, 0.0715914243006789, 0.1431828486013578, 0.0715914243006789, 0.0715914243006789, 0.0715914243006789, 0.0715914243006789, 0.1511869951162585, 0.08246563369977737, 0.1511869951162585, 0.1992919481077953, 0.08933776984142548, 0.1030820421247217, 0.07559349755812925, 0.061849225274833025, 0.06872136141648114, 0.020616408424944343, 0.21412249582437706, 0.21412249582437706, 0.21412249582437706, 0.21412249582437706, 0.13255747937751527, 0.25491822957214477, 0.1019672918288579, 0.08157383346308632, 0.12236075019462948, 0.11216402101174369, 0.09177056264597211, 0.03059018754865737, 0.04078691673154316, 0.02039345836577158, 0.38540494613936355, 0.13764462362120128, 0.08258677417272077, 0.1009393906555476, 0.09176308241413418, 0.08258677417272077, 0.04588154120706709, 0.03670523296565367, 0.027528924724240254, 0.018352616482826836, 0.22295551933756916, 0.19856975941002253, 0.10799407967913506, 0.11844511964808362, 0.08012463976193891, 0.08360831975158843, 0.05573887983439229, 0.05225519984474277, 0.05922255982404181, 0.020902079937897108, 0.15250544830544055, 0.1211072677719675, 0.2018454462866125, 0.2063309006485372, 0.1031654503242686, 0.07176726979079555, 0.04485454361924722, 0.049339997981171946, 0.026912726171548332, 0.02242727180962361, 0.0469106691471923, 0.1407320074415769, 0.0469106691471923, 0.0938213382943846, 0.0469106691471923, 0.1407320074415769, 0.0469106691471923, 0.3283746840303461, 0.07926670125346251, 0.11097338175484751, 0.31706680501385004, 0.11097338175484751, 0.20609342325900254, 0.047560020752077506, 0.031706680501385004, 0.031706680501385004, 0.047560020752077506, 0.015853340250692502, 0.3222485725846243, 0.0966745717753873, 0.08056214314615608, 0.2094615721800058, 0.11278700040461852, 0.04833728588769365, 0.06444971451692487, 0.032224857258462435, 0.016112428629231217, 0.016112428629231217, 0.14261721368396538, 0.1640097957365602, 0.08557032821037922, 0.09270118889457749, 0.07843946752618096, 0.10696291026297403, 0.07843946752618096, 0.04278516410518961, 0.18540237778915497, 0.021392582052594804, 0.13730845752245663, 0.13730845752245663, 0.13730845752245663, 0.13730845752245663, 0.13730845752245663, 0.13730845752245663, 0.13730845752245663, 0.4002178919137714, 0.11005992027628714, 0.07003813108491, 0.08004357838275428, 0.07003813108491, 0.09004902568059857, 0.07003813108491, 0.06003268378706571, 0.04002178919137714, 0.02001089459568857, 0.08745815143986821, 0.17491630287973642, 0.17491630287973642, 0.08745815143986821, 0.08745815143986821, 0.17491630287973642, 0.08745815143986821, 0.08745815143986821, 0.22080401331817912, 0.10093897751688188, 0.1766432106545433, 0.1703345245597382, 0.0757042331376614, 0.06308686094805117, 0.05046948875844094, 0.06308686094805117, 0.05677817485324606, 0.02523474437922047, 0.11833724371271744, 0.16093865144929573, 0.08993630522166525, 0.12780422320973484, 0.12780422320973484, 0.05680187698210437, 0.10413677446719134, 0.11833724371271744, 0.06626885647912177, 0.02366744874254349, 0.21910132675415228, 0.13795268721557735, 0.1947567348925798, 0.13389525523864862, 0.06897634360778868, 0.04868918372314495, 0.0527466157000737, 0.06897634360778868, 0.056804047677002444, 0.016229727907714982, 0.09565800980349955, 0.1913160196069991, 0.09565800980349955, 0.09565800980349955, 0.09565800980349955, 0.09565800980349955, 0.1913160196069991, 0.09565800980349955, 0.19150300254575456, 0.19150300254575456, 0.19150300254575456, 0.19150300254575456, 0.19150300254575456, 0.37941935931128734, 0.20193548671795894, 0.11779570058547605, 0.10096774335897947, 0.2355914011709521, 0.0841397861324829, 0.03365591445299316, 0.06731182890598632, 0.0841397861324829, 0.06731182890598632, 0.01682795722649658, 0.3168827805209907, 0.09053793729171163, 0.09053793729171163, 0.2716138118751349, 0.09053793729171163, 0.04526896864585581, 0.04526896864585581, 0.04526896864585581, 0.04526896864585581, 0.12834807367828502, 0.385044221034855, 0.06417403683914251, 0.06417403683914251, 0.12834807367828502, 0.06417403683914251, 0.06417403683914251, 0.12834807367828502, 0.06417403683914251, 0.21776043506526876, 0.1757999525609977, 0.13962712281593645, 0.10707157604538133, 0.1063481194504801, 0.10273083647597397, 0.046301222073678414, 0.04774813526348086, 0.03906665612466616, 0.018809871467431852, 0.11516988710438014, 0.1727548306565702, 0.11516988710438014, 0.05758494355219007, 0.1727548306565702, 0.11516988710438014, 0.1727548306565702, 0.05758494355219007, 0.3540265945585124, 0.14382330403939567, 0.06637998647972107, 0.0885066486396281, 0.07744331755967458, 0.0995699797195816, 0.04425332431981405, 0.05531665539976756, 0.04425332431981405, 0.011063331079953512, 0.27227671748592674, 0.27227671748592674, 0.3355150514904564, 0.1677575257452282, 0.1677575257452282, 0.1677575257452282, 0.13222163466332645, 0.13222163466332645, 0.13222163466332645, 0.13222163466332645, 0.2644432693266529, 0.2870594430840306, 0.2870594430840306, 0.2870594430840306, 0.3352873185234914, 0.08382182963087285, 0.08382182963087285, 0.08382182963087285, 0.10477728703859106, 0.10477728703859106, 0.04191091481543643, 0.06286637222315464, 0.06286637222315464, 0.020955457407718214, 0.39225418930192174, 0.22688071872038906, 0.16408337693170996, 0.09520887303444898, 0.10938891795447331, 0.10331175584589145, 0.07495166600584281, 0.0729259453029822, 0.08710599022300652, 0.04051441405721233, 0.0243086484343274, 0.2430727174130842, 0.2430727174130842, 0.2430727174130842, 0.2430727174130842, 0.21063465625397867, 0.13540799330612915, 0.15045332589569907, 0.10531732812698934, 0.04513599776870972, 0.07522666294784953, 0.04513599776870972, 0.060181330358279626, 0.15045332589569907, 0.030090665179139813, 0.2574266915240853, 0.1348425527030923, 0.1348425527030923, 0.20839303599568812, 0.07355048329259581, 0.0490336555283972, 0.0612920694104965, 0.0490336555283972, 0.0122584138820993, 0.036775241646297904, 0.17671303749369782, 0.1060278224962187, 0.2827408599899165, 0.1060278224962187, 0.07068521499747912, 0.07068521499747912, 0.03534260749873956, 0.07068521499747912, 0.07068521499747912, 0.11056758453230724, 0.22113516906461447, 0.11056758453230724, 0.11056758453230724, 0.22113516906461447, 0.11056758453230724, 0.10681350114408475, 0.1898906687005951, 0.13054983473165913, 0.11868166793787194, 0.20175883549438228, 0.11868166793787194, 0.023736333587574387, 0.05934083396893597, 0.023736333587574387, 0.023736333587574387, 0.21286456826983632, 0.12900882925444626, 0.09030618047811238, 0.10965750486627932, 0.09030618047811238, 0.1548105951053355, 0.07740529755266776, 0.05805397316450082, 0.04515309023905619, 0.032252207313611565, 0.2174781998452575, 0.144985466563505, 0.144985466563505, 0.144985466563505, 0.0724927332817525, 0.0724927332817525, 0.0724927332817525, 0.0724927332817525, 0.0724927332817525, 0.0724927332817525, 0.18604735036191153, 0.15222055938701853, 0.07751972931746313, 0.12121266766003326, 0.14517331126724914, 0.08738587668514027, 0.10148037292467901, 0.06060633383001663, 0.04792128721443176, 0.02114174435930813, 0.1572562137372909, 0.194501106464544, 0.11587299959589856, 0.12414964242417703, 0.11173467818175932, 0.08690474969692392, 0.08690474969692392, 0.05379817838381004, 0.045521535555531574, 0.020691607070696172, 0.1376060573126439, 0.1100848458501151, 0.05504242292505755, 0.2752121146252878, 0.19264848023770142, 0.08256363438758633, 0.05504242292505755, 0.027521211462528776, 0.027521211462528776, 0.027521211462528776, 0.2908358339302437, 0.2908358339302437, 0.2908358339302437, 0.12660981744334796, 0.10852270066572682, 0.2803503100531276, 0.10852270066572682, 0.16278405099859022, 0.06330490872167398, 0.04521779194405284, 0.036174233555242276, 0.036174233555242276, 0.027130675166431705, 0.15743058964760487, 0.15743058964760487, 0.027781868761342036, 0.1018668521249208, 0.17595183548849957, 0.027781868761342036, 0.07408498336357876, 0.19447308132939425, 0.06482436044313142, 0.009260622920447345, 0.1548246812612128, 0.0774123406306064, 0.0774123406306064, 0.0774123406306064, 0.1548246812612128, 0.0774123406306064, 0.1548246812612128, 0.1548246812612128, 0.1567142260996109, 0.1567142260996109, 0.1567142260996109, 0.1567142260996109, 0.1567142260996109, 0.1567142260996109, 0.1567142260996109, 0.24347984952920776, 0.24347984952920776, 0.16777667681412525, 0.12964561390182405, 0.11439318873690359, 0.11439318873690359, 0.05338348807722167, 0.1372718264842843, 0.08388833840706263, 0.04575727549476143, 0.12964561390182405, 0.022878637747380717, 0.36228298543953735, 0.14917534694569184, 0.0852430553975382, 0.0852430553975382, 0.07458767347284592, 0.07458767347284592, 0.05327690962346138, 0.05327690962346138, 0.0426215276987691, 0.010655381924692274, 0.1775303796912387, 0.1775303796912387, 0.1775303796912387, 0.1775303796912387, 0.3605780347814824, 0.060096339130247076, 0.060096339130247076, 0.060096339130247076, 0.060096339130247076, 0.12019267826049415, 0.060096339130247076, 0.060096339130247076, 0.060096339130247076, 0.10326441734708283, 0.13768588979611043, 0.27537177959222087, 0.10326441734708283, 0.20652883469416566, 0.06884294489805522, 0.03442147244902761, 0.03442147244902761, 0.03442147244902761, 0.03442147244902761, 0.1665770608127487, 0.12493279560956154, 0.08328853040637435, 0.09716995214077008, 0.18045848254714444, 0.09716995214077008, 0.09716995214077008, 0.06940710867197863, 0.06940710867197863, 0.027762843468791452, 0.24371162559736087, 0.03046395319967011, 0.12185581279868044, 0.27417557879703097, 0.12185581279868044, 0.06092790639934022, 0.06092790639934022, 0.09139185959901032, 0.03046395319967011, 0.03046395319967011, 0.1567301267023808, 0.1567301267023808, 0.13060843891865068, 0.2350951900535712, 0.05224337556746027, 0.026121687783730134, 0.0783650633511904, 0.0783650633511904, 0.0783650633511904, 0.026121687783730134, 0.36367143560979726, 0.18407430336073244, 0.0977894736603891, 0.1610650154406409, 0.11504643960045777, 0.10929411762043488, 0.08628482970034333, 0.12079876158048065, 0.057523219800228884, 0.04026625386016022, 0.023009287920091555, 0.22840448979200328, 0.16466370194307214, 0.15138437114121148, 0.058429055528186884, 0.10623464641488525, 0.07170838633004754, 0.07170838633004754, 0.08233185097153607, 0.04780559088669836, 0.02390279544334918, 0.24952363359190274, 0.12878639153130464, 0.1207372420605981, 0.10463894311918502, 0.08451606944241867, 0.0804914947070654, 0.06036862103029905, 0.09658979364847847, 0.05231947155959251, 0.02817202314747289, 0.2572049364519257, 0.15096811487395642, 0.10064540991597094, 0.07827976326797739, 0.15096811487395642, 0.10064540991597094, 0.04473129329598709, 0.05032270495798547, 0.04473129329598709, 0.016774234985995156, 0.24989465776199904, 0.11994943572575954, 0.10995364941527958, 0.09995786310479962, 0.19991572620959924, 0.0799662904838397, 0.04997893155239981, 0.029987358931439886, 0.03998314524191985, 0.019991572620959925, 0.18159337135089407, 0.14242617360854437, 0.12462290190747632, 0.13530486492811714, 0.1175015932270491, 0.08901635850534023, 0.06053112378363135, 0.06765243246405857, 0.04984916076299053, 0.02848523472170887, 0.2611643370100386, 0.17410955800669242, 0.17410955800669242, 0.08705477900334621, 0.08705477900334621, 0.08705477900334621, 0.08705477900334621, 0.08705477900334621, 0.08705477900334621, 0.20501927730602637, 0.12127900911060716, 0.13282939093066498, 0.10684103183553487, 0.12994179547565052, 0.08951545910544813, 0.0721898863753614, 0.06063950455530358, 0.06063950455530358, 0.02021316818510119, 0.13058751761260065, 0.13058751761260065, 0.13058751761260065, 0.13058751761260065, 0.13058751761260065, 0.13058751761260065, 0.2611750352252013, 0.11834097197642407, 0.11834097197642407, 0.11834097197642407, 0.11834097197642407, 0.11834097197642407, 0.11834097197642407, 0.23668194395284814, 0.360479786997357, 0.246794253567558, 0.09786668675954886, 0.18296815350698264, 0.09361161342217716, 0.09361161342217716, 0.07233624673531872, 0.08510146674743378, 0.055315953385831965, 0.04680580671108858, 0.021275366686858446, 0.2317239436423474, 0.2317239436423474, 0.2317239436423474, 0.2317239436423474, 0.17432500921006458, 0.11621667280670972, 0.2614875138150969, 0.05810833640335486, 0.11621667280670972, 0.11621667280670972, 0.05810833640335486, 0.05810833640335486, 0.02905416820167743, 0.02905416820167743, 0.19389944510005833, 0.1508106795222676, 0.08617753115558148, 0.10772191394447685, 0.08617753115558148, 0.12926629673337223, 0.06463314836668611, 0.08617753115558148, 0.06463314836668611, 0.06463314836668611, 0.1860702763140029, 0.1860702763140029, 0.09303513815700146, 0.09303513815700146, 0.09303513815700146, 0.09303513815700146, 0.09303513815700146, 0.09303513815700146, 0.09303513815700146, 0.09303513815700146, 0.15667495387106006, 0.15667495387106006, 0.15667495387106006, 0.15667495387106006, 0.15667495387106006, 0.15667495387106006, 0.23037082343313908, 0.1356978822962326, 0.11676329406885132, 0.07889411761408872, 0.11991905877341485, 0.08204988231865228, 0.08836141172777937, 0.05680376468214388, 0.07889411761408872, 0.012623058818254197, 0.18983635668509571, 0.14061952347044127, 0.12187025367438244, 0.1031209838783236, 0.08202805535775741, 0.12187025367438244, 0.07265342045972799, 0.04452951576563974, 0.10546464260283095, 0.018749269796058836, 0.2088989870503527, 0.12631101542579465, 0.18946652313869197, 0.03400681184540625, 0.07287173966872769, 0.0680136236908125, 0.10202043553621876, 0.13116913140370984, 0.04372304380123661, 0.019432463911660716, 0.1165884659647789, 0.03886282198825963, 0.1165884659647789, 0.07772564397651927, 0.19431410994129816, 0.03886282198825963, 0.1165884659647789, 0.1165884659647789, 0.15545128795303853, 0.03886282198825963, 0.16132475590975034, 0.23578233556040434, 0.13650556269286568, 0.09927677286753868, 0.08686717625909635, 0.07445757965065401, 0.07445757965065401, 0.04963838643376934, 0.06204798304221167, 0.012409596608442335, 0.18739914125920556, 0.1484201198772908, 0.12143464353596521, 0.11393867788559699, 0.12293383666603885, 0.0704620771134613, 0.0809564290239768, 0.05247175955257756, 0.07645884963375588, 0.025486283211251955, 0.17210900362185555, 0.08605450181092777, 0.14342416968487962, 0.1147393357479037, 0.05736966787395185, 0.17210900362185555, 0.05736966787395185, 0.07171208484243981, 0.1147393357479037, 0.014342416968487962, 0.2287612349349355, 0.16125791970823322, 0.13125644627414332, 0.0993798807504228, 0.09375460448153095, 0.07125349940596351, 0.07312859149559413, 0.07687877567485538, 0.04500221015113485, 0.01875092089630619, 0.4852140537429553, 0.16173801791431844, 0.16173801791431844, 0.23699220834110762, 0.12457282746135144, 0.11545774252515499, 0.10026593429816091, 0.11241938087975618, 0.11545774252515499, 0.06684395619877394, 0.06684395619877394, 0.04253706303558342, 0.018230169872392895, 0.0932616516069914, 0.0932616516069914, 0.1865233032139828, 0.2797849548209742, 0.0932616516069914, 0.0932616516069914, 0.0932616516069914, 0.0932616516069914, 0.0932616516069914, 0.34967436501071225, 0.34967436501071225, 0.22766210552780097, 0.22766210552780097, 0.22766210552780097, 0.19414901948650523, 0.1390357494387231, 0.1127316887340998, 0.11899456033043869, 0.1290151548845809, 0.10145851986068982, 0.07014416187899544, 0.047597824132175476, 0.06137614164412101, 0.026304060704623287, 0.36680038271866544, 0.36680038271866544, 0.27363432917035985, 0.1195713035030144, 0.12876909608016934, 0.10117571834870448, 0.08737902948297205, 0.1195713035030144, 0.05518675546292972, 0.048288411030063504, 0.048288411030063504, 0.020695033298598644, 0.43533060836233906, 0.14511020278744635, 0.14511020278744635, 0.14511020278744635, 0.14511020278744635, 0.14511020278744635, 0.14511020278744635, 0.26757208411126815, 0.26757208411126815, 0.26757208411126815, 0.26757208411126815, 0.3518304671311365, 0.3292892722832939, 0.29673917650781695, 0.132965095484377, 0.10539916105468908, 0.10377763550000155, 0.09891305883593898, 0.09080543106250136, 0.05675339441406335, 0.04053813886718811, 0.05675339441406335, 0.017836781101562768, 0.1537500861737197, 0.1537500861737197, 0.1537500861737197, 0.10982149012408551, 0.08785719209926841, 0.17571438419853683, 0.04392859604963421, 0.0658928940744513, 0.0658928940744513, 0.021964298024817103, 0.2345647283027644, 0.0850297140097521, 0.12607854146273587, 0.15833119160436596, 0.11435030504759763, 0.07330147759461388, 0.05570912297190654, 0.06157324117947565, 0.06743735938704476, 0.02345647283027644, 0.2707584909928964, 0.1830473476581121, 0.21791350911680013, 0.12203156510540807, 0.09588194401139205, 0.08716540364672004, 0.11331502474073606, 0.06101578255270403, 0.05229924218803203, 0.05229924218803203, 0.01743308072934401, 0.22851944417218903, 0.12695524676232725, 0.07617314805739635, 0.07617314805739635, 0.12695524676232725, 0.20312839481972358, 0.050782098704930896, 0.025391049352465448, 0.025391049352465448, 0.025391049352465448, 0.16974659232009393, 0.12730994424007044, 0.12730994424007044, 0.08487329616004696, 0.12730994424007044, 0.16974659232009393, 0.04243664808002348, 0.04243664808002348, 0.04243664808002348, 0.04243664808002348, 0.21288552324701415, 0.1490198662729099, 0.06386565697410425, 0.10644276162350708, 0.08515420929880566, 0.1490198662729099, 0.1490198662729099, 0.04257710464940283, 0.04257710464940283, 0.021288552324701414, 0.10612598097649191, 0.10612598097649191, 0.10612598097649191, 0.10612598097649191, 0.10612598097649191, 0.10612598097649191, 0.21225196195298382, 0.10612598097649191, 0.15150640039932722, 0.22725960059899084, 0.07575320019966361, 0.07575320019966361, 0.15150640039932722, 0.07575320019966361, 0.15150640039932722, 0.07575320019966361, 0.12430757557699357, 0.08287171705132905, 0.1657434341026581, 0.1657434341026581, 0.20717929262832263, 0.08287171705132905, 0.08287171705132905, 0.041435858525664525, 0.08287171705132905, 0.21559122655380528, 0.12935473593228317, 0.08623649062152211, 0.043118245310761055, 0.12935473593228317, 0.08623649062152211, 0.17247298124304422, 0.12935473593228317, 0.043118245310761055, 0.2807609238929575, 0.1329920165808746, 0.1477689073120829, 0.06280178560763523, 0.08496712170444766, 0.07388445365604145, 0.07019023097323937, 0.0664960082904373, 0.06280178560763523, 0.01847111341401036, 0.4142423424518555, 0.13808078081728517, 0.13808078081728517, 0.13808078081728517, 0.13808078081728517, 0.13808078081728517, 0.39538800541140673, 0.1129680015461162, 0.0564840007730581, 0.0564840007730581, 0.16945200231917432, 0.0564840007730581, 0.0564840007730581, 0.12003403561322468, 0.12003403561322468, 0.06001701780661234, 0.24006807122644935, 0.12003403561322468, 0.06001701780661234, 0.06001701780661234, 0.06001701780661234, 0.06001701780661234, 0.1262122744086564, 0.0841415162724376, 0.11218868836325015, 0.18230661859028147, 0.11218868836325015, 0.1262122744086564, 0.1262122744086564, 0.07011793022703133, 0.0420707581362188, 0.014023586045406268, 0.19118161332031133, 0.38236322664062267, 0.09559080666015567, 0.09559080666015567, 0.09559080666015567, 0.19118161332031133, 0.5339717390784479, 0.26445192723326666, 0.26445192723326666, 0.2663363574511231, 0.2663363574511231, 0.2663363574511231, 0.17573767317539246, 0.13668485691419413, 0.17573767317539246, 0.09763204065299581, 0.039052816261198324, 0.07810563252239665, 0.05857922439179749, 0.05857922439179749, 0.1562112650447933, 0.039052816261198324, 0.3286278130144165, 0.09283271554079563, 0.08354944398671607, 0.09840267847324337, 0.08911940691916381, 0.14481903624364117, 0.04641635777039781, 0.04827301208121373, 0.044559703459581906, 0.02042319741897504, 0.24038918355797068, 0.08903303094739655, 0.12464624332635517, 0.0979363340421362, 0.11574294023161551, 0.129097894873725, 0.06677477321054741, 0.0489681670210681, 0.062323121663177584, 0.026709909284218965, 0.1895169924409765, 0.1895169924409765, 0.1895169924409765, 0.1895169924409765, 0.1895169924409765, 0.38200105181707283, 0.1770581116534937, 0.1770581116534937, 0.1770581116534937, 0.1770581116534937, 0.19238409911999885, 0.09619204955999942, 0.28857614867999826, 0.09619204955999942, 0.09619204955999942, 0.09619204955999942, 0.09619204955999942, 0.21673373920701977, 0.18964202180614229, 0.08127515220263241, 0.08127515220263241, 0.10836686960350989, 0.08127515220263241, 0.16255030440526483, 0.08127515220263241, 0.02709171740087747, 0.13509915669000788, 0.13509915669000788, 0.13509915669000788, 0.13509915669000788, 0.13509915669000788, 0.13509915669000788, 0.2436848098325281, 0.10672327437920938, 0.11028071685851637, 0.12273176553609079, 0.11383815933782335, 0.09427222570163496, 0.0889360619826745, 0.05336163718960469, 0.04980419471029771, 0.0177872123965349, 0.19703375192981817, 0.14982774886329925, 0.14367044411549243, 0.1375131393676856, 0.08620226646929545, 0.09235957121710227, 0.07594009188961742, 0.051310872898390154, 0.051310872898390154, 0.014367044411549243, 0.40309467006108984, 0.23110622061460862, 0.23110622061460862, 0.23110622061460862, 0.3701093463318885, 0.14804373853275543, 0.07402186926637772, 0.07402186926637772, 0.14804373853275543, 0.07402186926637772, 0.07402186926637772, 0.29397842220609716, 0.29397842220609716, 0.29397842220609716, 0.10427371635416945, 0.2085474327083389, 0.10427371635416945, 0.10427371635416945, 0.10427371635416945, 0.10427371635416945, 0.2085474327083389, 0.37948482164601677, 0.24587989544764982, 0.2950558745371798, 0.1475279372685899, 0.04917597908952996, 0.09835195817905992, 0.04917597908952996, 0.04917597908952996, 0.04917597908952996, 0.1361482182917205, 0.1361482182917205, 0.272296436583441, 0.1361482182917205, 0.272296436583441, 0.2074669856080496, 0.1037334928040248, 0.1037334928040248, 0.2074669856080496, 0.1037334928040248, 0.2074669856080496, 0.1037334928040248, 0.26128668460113036, 0.26128668460113036, 0.18783342168342285, 0.18783342168342285, 0.18783342168342285, 0.18783342168342285, 0.18783342168342285, 0.22873791868472915, 0.22873791868472915, 0.22873791868472915, 0.22873791868472915, 0.195627454578167, 0.0782509818312668, 0.11737647274690019, 0.11737647274690019, 0.23475294549380038, 0.0782509818312668, 0.0391254909156334, 0.0391254909156334, 0.11737647274690019, 0.2576225996884161, 0.15797612245044382, 0.07534245839944244, 0.1093680847733842, 0.11179848665723717, 0.07534245839944244, 0.0534688414447656, 0.0680512527478835, 0.07048165463173647, 0.019443215070823857, 0.36734142668547903, 0.36734142668547903, 0.28196965130898954, 0.28196965130898954, 0.28196965130898954, 0.10905663413462714, 0.21811326826925428, 0.10905663413462714, 0.07270442275641809, 0.07270442275641809, 0.14540884551283617, 0.18176105689104524, 0.03635221137820904, 0.03635221137820904, 0.03635221137820904, 0.17767463161842906, 0.14901743297029535, 0.13755455351104184, 0.13182311378141512, 0.1031659151332814, 0.10889735486290814, 0.06304583702589418, 0.06877727675552092, 0.02865719864813372, 0.03438863837776046, 0.12055406786798216, 0.12055406786798216, 0.12055406786798216, 0.12055406786798216, 0.12055406786798216, 0.12055406786798216, 0.12055406786798216, 0.12055406786798216, 0.12055406786798216, 0.2148486393787429, 0.12890918362724574, 0.04296972787574858, 0.12890918362724574, 0.2148486393787429, 0.04296972787574858, 0.08593945575149715, 0.12890918362724574, 0.08593945575149715, 0.3444415104478083, 0.3444415104478083, 0.26281726438908226, 0.26281726438908226, 0.26281726438908226, 0.26281726438908226, 0.22067192609238392, 0.1533938998447059, 0.12379156829572757, 0.12917381039554182, 0.09418923674674924, 0.06996914729758515, 0.05920466309795666, 0.05651354204804954, 0.07535138939739938, 0.016146726299442728, 0.49086997913756303, 0.16362332637918767, 0.16362332637918767, 0.2333886424506204, 0.2333886424506204, 0.23947213609853374, 0.11789397469466277, 0.12157816140387098, 0.09210466773020529, 0.09210466773020529, 0.10684141456703813, 0.06999954747495601, 0.058946987347331385, 0.08105210760258065, 0.02210512025524927, 0.24524867949767515, 0.1407514160595353, 0.116581980978605, 0.08672561999627933, 0.09881033753674448, 0.09809947179907005, 0.08174955983255838, 0.06824311081674439, 0.04762800442418619, 0.016349911966511678, 0.1914313943905267, 0.11115371287191873, 0.10497850660125659, 0.10497850660125659, 0.1296793316839052, 0.14820495049589164, 0.08645288778927013, 0.061752062706621515, 0.043226443894635064, 0.018525618811986454, 0.35070133469596276, 0.22066261818558594, 0.08826504727423437, 0.08826504727423437, 0.08826504727423437, 0.08826504727423437, 0.17653009454846874, 0.044132523637117185, 0.08826504727423437, 0.044132523637117185, 0.044132523637117185, 0.3282678268604516, 0.1641339134302258, 0.1641339134302258, 0.1641339134302258, 0.3574444345541732, 0.25433207202204183, 0.07740541322409969, 0.1142651338070043, 0.09583527351555199, 0.09952124557384245, 0.13269499409845661, 0.07371944116580922, 0.05160360881606646, 0.07371944116580922, 0.02580180440803323, 0.19759467065483663, 0.1466682091458581, 0.11203821531975272, 0.1059270399386753, 0.12833468300262585, 0.07944527995400648, 0.0855564553350839, 0.07740822149364733, 0.04888940304861937, 0.016296467682873125, 0.09710697431244186, 0.09710697431244186, 0.2913209229373256, 0.09710697431244186, 0.19421394862488373, 0.16008261208828176, 0.14007228557724652, 0.10005163255517609, 0.20010326511035217, 0.060030979533105655, 0.060030979533105655, 0.14007228557724652, 0.060030979533105655, 0.060030979533105655, 0.12720038050490065, 0.12720038050490065, 0.12720038050490065, 0.12720038050490065, 0.12720038050490065, 0.12720038050490065, 0.12720038050490065, 0.29293823497420585, 0.2849689317622283, 0.2849689317622283, 0.12823601929300274, 0.04274533976433425, 0.07124223294055708, 0.05699378635244567, 0.04274533976433425, 0.05699378635244567, 0.014248446588111417, 0.014248446588111417, 0.24287387784164685, 0.14774827568700183, 0.13358063281290578, 0.08298190826256267, 0.11334114299276854, 0.0850058572445764, 0.05667057149638427, 0.06679031640645289, 0.050598724550343095, 0.022263438802150964, 0.3760875883299817, 0.14103284562374313, 0.04701094854124771, 0.04701094854124771, 0.18804379416499084, 0.09402189708249542, 0.04701094854124771, 0.04701094854124771, 0.19200565459867908, 0.1280037697324527, 0.032000942433113175, 0.09600282729933954, 0.22400659703179224, 0.032000942433113175, 0.09600282729933954, 0.09600282729933954, 0.06400188486622635, 0.15091406550443404, 0.15091406550443404, 0.3018281310088681, 0.15091406550443404, 0.15091406550443404, 0.3553011886537569, 0.20984312112952233, 0.20984312112952233, 0.41968624225904466, 0.1325379989100478, 0.1325379989100478, 0.1325379989100478, 0.2650759978200956, 0.1325379989100478, 0.1325379989100478, 0.1325379989100478, 0.17308245131008002, 0.17308245131008002, 0.17308245131008002, 0.17308245131008002, 0.17308245131008002, 0.3968939482176602, 0.25744237169270373, 0.1239537345187092, 0.17734918938830702, 0.07818620177333965, 0.08581412389756791, 0.08009318230439672, 0.05148847433854074, 0.08200016283545378, 0.04386055221431249, 0.019069805310570648, 0.27613703665809364, 0.27613703665809364, 0.13492311502412727, 0.07709892287092986, 0.07709892287092986, 0.289120960765987, 0.1734725764595922, 0.09637365358866233, 0.07709892287092986, 0.03854946143546493, 0.03854946143546493, 0.019274730717732464, 0.07870828493087988, 0.1967707123271997, 0.07870828493087988, 0.07870828493087988, 0.1967707123271997, 0.07870828493087988, 0.15741656986175975, 0.07870828493087988, 0.03935414246543994, 0.24413720955523538, 0.10463023266652945, 0.06975348844435296, 0.27901395377741184, 0.10463023266652945, 0.06975348844435296, 0.06975348844435296, 0.03487674422217648, 0.03487674422217648, 0.03487674422217648, 0.18719656432704207, 0.13311755685478546, 0.12479770955136138, 0.11647786224793728, 0.13727748050649752, 0.09151832033766501, 0.07071870207910477, 0.07071870207910477, 0.037439312865408414, 0.02911946556198432, 0.21207848875091748, 0.1391765082427896, 0.09941179160199257, 0.13254905546932344, 0.10603924437545874, 0.07952943328159405, 0.11929414992239108, 0.0463921694142632, 0.039764716640797025, 0.026509811093864685, 0.26870589804470074, 0.26870589804470074, 0.26870589804470074, 0.2596623239874611, 0.1418323618418905, 0.13965032550586143, 0.10473774412939607, 0.09164552611322156, 0.07200719908895979, 0.05891498107278529, 0.05891498107278529, 0.056732944736756205, 0.015274254352203594, 0.05229684593111096, 0.10459369186222192, 0.3137810755866658, 0.10459369186222192, 0.20918738372444384, 0.05229684593111096, 0.05229684593111096, 0.05229684593111096, 0.05229684593111096, 0.13859378718871054, 0.13859378718871054, 0.13859378718871054, 0.13859378718871054, 0.09239585812580704, 0.09239585812580704, 0.13859378718871054, 0.04619792906290352, 0.04619792906290352, 0.2428199391982664, 0.14569196351895983, 0.06475198378620436, 0.06475198378620436, 0.09712797567930655, 0.08093997973275546, 0.14569196351895983, 0.11331597162585764, 0.03237599189310218, 0.01618799594655109, 0.3652756291287349, 0.3652756291287349, 0.1701313178495437, 0.3402626356990874, 0.1701313178495437, 0.17199816077714816, 0.4299954019428704, 0.04299954019428704, 0.04299954019428704, 0.04299954019428704, 0.04299954019428704, 0.04299954019428704, 0.12899862058286113, 0.04299954019428704, 0.2506943626417675, 0.10805791493179634, 0.19018193027996155, 0.09509096513998078, 0.09941328173725263, 0.09076864854270893, 0.04754548256999039, 0.051867799167262245, 0.04322316597271853, 0.017289266389087413, 0.1790774851245782, 0.10232999149975897, 0.2814074766243371, 0.10232999149975897, 0.05116499574987948, 0.10232999149975897, 0.02558249787493974, 0.07674749362481922, 0.07674749362481922, 0.02558249787493974, 0.17491595563136655, 0.1166106370875777, 0.40813722980652195, 0.05830531854378885, 0.05830531854378885, 0.05830531854378885, 0.05830531854378885, 0.05830531854378885, 0.05830531854378885, 0.16739880508596083, 0.10590536648295482, 0.14826751307613675, 0.1325525232109241, 0.14280142964475842, 0.10248906433834337, 0.058077136458394574, 0.06695952203438434, 0.05671061560055, 0.019131292009824094, 0.19612120395669813, 0.0937343989498925, 0.17593225649056743, 0.10382887268295783, 0.10671300803526222, 0.08796612824528371, 0.06633511310300083, 0.06489304542684865, 0.08796612824528371, 0.015862744437674114, 0.15150631798963138, 0.15150631798963138, 0.10100421199308758, 0.10100421199308758, 0.10100421199308758, 0.20200842398617516, 0.05050210599654379, 0.05050210599654379, 0.05050210599654379, 0.05050210599654379, 0.20687938139293977, 0.15515953604470484, 0.10343969069646988, 0.10343969069646988, 0.10343969069646988, 0.20687938139293977, 0.05171984534823494, 0.05171984534823494, 0.05171984534823494, 0.05171984534823494, 0.22898340830648597, 0.07632780276882865, 0.07632780276882865, 0.1526556055376573, 0.07632780276882865, 0.22898340830648597, 0.07632780276882865, 0.07632780276882865, 0.12893591678251762, 0.35457377115192346, 0.19340387517377644, 0.06446795839125881, 0.032233979195629404, 0.06446795839125881, 0.032233979195629404, 0.06446795839125881, 0.032233979195629404, 0.032233979195629404, 0.23087233353844663, 0.19902787374004022, 0.07165003454641447, 0.03980557474800804, 0.08757226444561769, 0.06368891959681286, 0.14330006909282894, 0.13533895414322733, 0.023883344848804826, 0.007961114949601607, 0.11969727877108076, 0.11969727877108076, 0.11969727877108076, 0.23939455754216152, 0.05984863938554038, 0.05984863938554038, 0.05984863938554038, 0.05984863938554038, 0.11969727877108076, 0.23524578817520614, 0.15805576393021661, 0.13600147128879106, 0.0955686014461775, 0.0992443168864151, 0.06616287792427673, 0.07719002424498951, 0.08454145512546471, 0.03675715440237596, 0.011027146320712788, 0.07510391141943673, 0.1126558671291551, 0.3379676013874653, 0.1126558671291551, 0.2253117342583102, 0.037551955709718365, 0.037551955709718365, 0.037551955709718365, 0.037551955709718365, 0.08366452834990391, 0.11155270446653855, 0.2788817611663464, 0.1533849686414905, 0.2231054089330771, 0.027888176116634637, 0.027888176116634637, 0.027888176116634637, 0.04183226417495196, 0.013944088058317318, 0.0657955835673054, 0.0877274447564072, 0.3728416402147306, 0.109659305945509, 0.19738675070191622, 0.0438637223782036, 0.0219318611891018, 0.0219318611891018, 0.0438637223782036, 0.0219318611891018, 0.13363260248918266, 0.13363260248918266, 0.2672652049783653, 0.13363260248918266, 0.13363260248918266, 0.2390188359922099, 0.2390188359922099, 0.21111340042296461, 0.1319458752643529, 0.2375025754758352, 0.10555670021148231, 0.052778350105741154, 0.07916752515861174, 0.052778350105741154, 0.052778350105741154, 0.052778350105741154, 0.026389175052870577, 0.22965268614875353, 0.1100419121129444, 0.16426546300917785, 0.0924989985876924, 0.0956886192286473, 0.11163672243342185, 0.06857684378053057, 0.059007981857665835, 0.04943911993480111, 0.017542913525252005, 0.2131659692942448, 0.14002078375210197, 0.12539174664367342, 0.11076270953524485, 0.14002078375210197, 0.08568436020651017, 0.07314518554214283, 0.03970738643716325, 0.05224656110153059, 0.022988486884673462, 0.19009736259405483, 0.12243558946735736, 0.18043139500452662, 0.12404658406561206, 0.06443978393018808, 0.11115862727957444, 0.0708837623232069, 0.053162821742405164, 0.05638481093891457, 0.02416491897382053, 0.22229441145350257, 0.11114720572675128, 0.11114720572675128, 0.11114720572675128, 0.11114720572675128, 0.22229441145350257, 0.11114720572675128, 0.16145282459779758, 0.16145282459779758, 0.21527043279706345, 0.08072641229889879, 0.18836162869743053, 0.08072641229889879, 0.026908804099632932, 0.026908804099632932, 0.026908804099632932, 0.026908804099632932, 0.1801615045250707, 0.1801615045250707, 0.3603230090501414, 0.1801615045250707, 0.1801615045250707, 0.14600006031016152, 0.1345490751877959, 0.15172555287134432, 0.1259608363460217, 0.10878435866247328, 0.1202353437848389, 0.06298041817301085, 0.07156865701478506, 0.057254925611828045, 0.020039223964139815, 0.18075885209195428, 0.09037942604597714, 0.09037942604597714, 0.27113827813793145, 0.18075885209195428, 0.09037942604597714, 0.09037942604597714, 0.09037942604597714, 0.20233160356587804, 0.15736902499568292, 0.15415741224066898, 0.11561805918050173, 0.10437741453795295, 0.07868451249784146, 0.07386709336532055, 0.049779997702716024, 0.049779997702716024, 0.016058063775069686, 0.19348857816518467, 0.10458842062982954, 0.07321189444088068, 0.1150472626928125, 0.1568826309447443, 0.1359649468187784, 0.07844131547237215, 0.06275305237789773, 0.05752363134640625, 0.031376526188948865, 0.49558099924108756, 0.24779049962054378, 0.3376619484740154, 0.08441548711850386, 0.08441548711850386, 0.08441548711850386, 0.08441548711850386, 0.08441548711850386, 0.08441548711850386, 0.08441548711850386, 0.22310775645298916, 0.10528680641601736, 0.17547801069336227, 0.13411533674421258, 0.09275266279506292, 0.09024583407087203, 0.05515023193219957, 0.05765706065639046, 0.05138998884591323, 0.015040972345145337, 0.28663008143347307, 0.28663008143347307, 0.19510908367343258, 0.17937448015138155, 0.12902374888081833, 0.1258768281764081, 0.1101422246543571, 0.07552609690584487, 0.05035073127056325, 0.07237917620143466, 0.04090996915733264, 0.018881524226461218, 0.2931592620102808, 0.2717217686743718, 0.2717217686743718, 0.22389074200784098, 0.13384772620033972, 0.128980536156691, 0.1703516515277051, 0.0997773958947987, 0.06570706558925768, 0.04867190043648717, 0.06570706558925768, 0.04623830541466281, 0.014601570130946151, 0.1941911120229429, 0.14222447641116945, 0.12307887381735817, 0.11487361556286763, 0.10940344339320726, 0.0875227547145658, 0.0875227547145658, 0.0738473242904149, 0.0437613773572829, 0.024615774763471633, 0.15455180263702448, 0.15455180263702448, 0.30910360527404895, 0.15455180263702448, 0.15455180263702448, 0.20354834941804398, 0.13569889961202933, 0.06784944980601466, 0.27139779922405866, 0.06784944980601466, 0.06784944980601466, 0.06784944980601466, 0.06784944980601466, 0.06784944980601466, 0.14872132702627583, 0.14872132702627583, 0.0892327962157655, 0.11897706162102067, 0.178465592431531, 0.05948853081051034, 0.14872132702627583, 0.05948853081051034, 0.02974426540525517, 0.02974426540525517, 0.35022952101253446, 0.35022952101253446, 0.20324318254781742, 0.13065633163788262, 0.14517370181986958, 0.1393667537470748, 0.09291116916471653, 0.08710422109192174, 0.058069480727947834, 0.04645558458235827, 0.0696833768735374, 0.026131266327576525, 0.33435994035523375, 0.33435994035523375, 0.18887495895172954, 0.14344933591270598, 0.13866769138228244, 0.10280535740410594, 0.11475946873016478, 0.09563289060847065, 0.08846042381283535, 0.05498891209987062, 0.05737973436508239, 0.016735755856482362, 0.23893007201114722, 0.11946503600557361, 0.11946503600557361, 0.11946503600557361, 0.11946503600557361, 0.23893007201114722, 0.11946503600557361, 0.23947987284563407, 0.23947987284563407, 0.23947987284563407, 0.05852463029422317, 0.11704926058844634, 0.05852463029422317, 0.05852463029422317, 0.11704926058844634, 0.11704926058844634, 0.05852463029422317, 0.351147781765339, 0.15626370653497618, 0.13021975544581346, 0.18230765762413886, 0.11719777990123213, 0.09115382881206943, 0.07813185326748809, 0.09115382881206943, 0.06510987772290673, 0.039065926633744044, 0.039065926633744044, 0.16647038596494543, 0.33294077192989086, 0.08323519298247271, 0.08323519298247271, 0.08323519298247271, 0.08323519298247271, 0.08323519298247271, 0.08323519298247271, 0.08323519298247271, 0.17471526949051494, 0.17471526949051494, 0.3494305389810299, 0.17471526949051494, 0.20627193881356873, 0.3094079082203531, 0.10313596940678436, 0.10313596940678436, 0.10313596940678436, 0.10313596940678436, 0.10313596940678436, 0.21222931727452204, 0.21222931727452204, 0.10611465863726102, 0.10611465863726102, 0.10611465863726102, 0.10611465863726102, 0.10611465863726102, 0.10611465863726102, 0.10611465863726102, 0.17176061537563297, 0.12882046153172472, 0.17176061537563297, 0.08588030768781649, 0.2147007692195412, 0.08588030768781649, 0.042940153843908244, 0.042940153843908244, 0.042940153843908244, 0.042940153843908244, 0.21079236208164462, 0.42158472416328924, 0.21079236208164462, 0.19432769643766454, 0.3238794940627742, 0.06477589881255484, 0.06477589881255484, 0.03238794940627742, 0.09716384821883227, 0.03238794940627742, 0.12955179762510968, 0.06477589881255484, 0.20301964725892357, 0.15790417009027388, 0.13785284690420738, 0.10777586212510758, 0.10276303132859095, 0.08521812354078273, 0.055141138761682945, 0.06015396955819958, 0.07519246194774948, 0.020051323186066526, 0.21229841566171623, 0.15164172547265445, 0.15164172547265445, 0.06065669018906178, 0.21229841566171623, 0.09098503528359267, 0.03032834509453089, 0.03032834509453089, 0.03032834509453089, 0.03032834509453089, 0.2670878311536584, 0.2670878311536584, 0.2670878311536584, 0.17692555992188513, 0.14743796660157096, 0.157267164375009, 0.06880438441406644, 0.1867547576953232, 0.08846277996094257, 0.05897518664062838, 0.03931679109375225, 0.03931679109375225, 0.02948759332031419, 0.2369340525356423, 0.2369340525356423, 0.11846702626782114, 0.11846702626782114, 0.2369340525356423, 0.11846702626782114, 0.14381718056789136, 0.0898857378549321, 0.14381718056789136, 0.10786288542591853, 0.12584003299690494, 0.0898857378549321, 0.0898857378549321, 0.12584003299690494, 0.053931442712959264, 0.01797714757098642, 0.18450164895481797, 0.10762596189031047, 0.04612541223870449, 0.13837623671611346, 0.18450164895481797, 0.04612541223870449, 0.07687568706450748, 0.13837623671611346, 0.061500549651605986, 0.015375137412901497, 0.22068760209875996, 0.11034380104937998, 0.11034380104937998, 0.11034380104937998, 0.11034380104937998, 0.22068760209875996, 0.11034380104937998, 0.23569800884501432, 0.1616214917794384, 0.1414188053070086, 0.07407651706557593, 0.15488726295529515, 0.0707094026535043, 0.04377248735693123, 0.0505067161810745, 0.053873830593146134, 0.016835572060358166, 0.25238806087913446, 0.17208276878122802, 0.08604138439061401, 0.10898575356144442, 0.11758989200050582, 0.10611770741509062, 0.06022896907342981, 0.043020692195307006, 0.0344165537562456, 0.0172082768781228, 0.19079595181087422, 0.12083743614688701, 0.06995851566398721, 0.20987554699196165, 0.13991703132797442, 0.05723878554326227, 0.06995851566398721, 0.06995851566398721, 0.05087892048289979, 0.019079595181087424, 0.2115935243711412, 0.4231870487422824, 0.2115935243711412, 0.20021027902336688, 0.20021027902336688, 0.10010513951168344, 0.10010513951168344, 0.10010513951168344, 0.20021027902336688, 0.344553915417082, 0.11250740095251657, 0.07734883815485515, 0.07031712559532285, 0.09141226327391971, 0.11953911351204886, 0.056253700476258287, 0.056253700476258287, 0.049221987916726, 0.028126850238129143, 0.24923708560546187, 0.05538601902343598, 0.08307902853515396, 0.05538601902343598, 0.13846504755858993, 0.13846504755858993, 0.13846504755858993, 0.05538601902343598, 0.02769300951171799, 0.02769300951171799, 0.21080321339777944, 0.1124283804788157, 0.14053547559851962, 0.08432128535911178, 0.07026773779925981, 0.07026773779925981, 0.1124283804788157, 0.12648192803866767, 0.05621419023940785, 0.014053547559851963, 0.22473019577607337, 0.14982013051738224, 0.14982013051738224, 0.07491006525869112, 0.07491006525869112, 0.07491006525869112, 0.07491006525869112, 0.14982013051738224, 0.07491006525869112, 0.14703881982617123, 0.32348540361757666, 0.14703881982617123, 0.058815527930468486, 0.029407763965234243, 0.08822329189570273, 0.029407763965234243, 0.08822329189570273, 0.029407763965234243, 0.029407763965234243, 0.2475375605190694, 0.09901502420762776, 0.09571452340070684, 0.10891652662839053, 0.15182303711836256, 0.12211852985607423, 0.0528080129107348, 0.0528080129107348, 0.04290651048997203, 0.019803004841525553, 0.24638277992239405, 0.1368793221791078, 0.09809684756169393, 0.09809684756169393, 0.10265949163433086, 0.09125288145273854, 0.0866902373801016, 0.05703305090796158, 0.06615833905323544, 0.02053189832686617, 0.29618181439494035, 0.09778251288636787, 0.10345106435804137, 0.12329099450889862, 0.09919965075428624, 0.07510830699967387, 0.06660547979216362, 0.068022617660082, 0.04959982537714312, 0.01983993015085725, 0.17828968343682117, 0.06685863128880794, 0.22286210429602646, 0.15600347300721853, 0.08914484171841058, 0.06685863128880794, 0.04457242085920529, 0.022286210429602646, 0.15600347300721853, 0.022286210429602646, 0.21645330875632157, 0.10661133117848674, 0.10015003837979057, 0.11953391677587907, 0.13891779517196756, 0.10338068477913866, 0.07430486718500591, 0.06461292798696167, 0.06461292798696167, 0.012922585597392332, 0.17388799098170063, 0.11065599426108222, 0.11065599426108222, 0.17388799098170063, 0.07903999590077301, 0.17388799098170063, 0.04742399754046381, 0.06323199672061841, 0.06323199672061841, 0.015807999180154603, 0.2762287963912863, 0.2762287963912863, 0.2762287963912863, 0.1723585273655661, 0.131320782754717, 0.13952833167688683, 0.16004720398231137, 0.08617926368278304, 0.0656603913773585, 0.07386794029952833, 0.06976416583844341, 0.07386794029952833, 0.02462264676650944, 0.1372332190538304, 0.1372332190538304, 0.0686166095269152, 0.0686166095269152, 0.0686166095269152, 0.1372332190538304, 0.0686166095269152, 0.0686166095269152, 0.1372332190538304, 0.0686166095269152, 0.08947427491637158, 0.17894854983274316, 0.08947427491637158, 0.08947427491637158, 0.17894854983274316, 0.08947427491637158, 0.17894854983274316, 0.08947427491637158, 0.1755903688883268, 0.28428916867633863, 0.0585301229627756, 0.07525301523785434, 0.10033735365047246, 0.06689156910031496, 0.05016867682523623, 0.1170602459255512, 0.0585301229627756, 0.01672289227507874, 0.11924566375174828, 0.32792557531730776, 0.02981141593793707, 0.05962283187587414, 0.08943424781381121, 0.05962283187587414, 0.05962283187587414, 0.14905707968968535, 0.05962283187587414, 0.02981141593793707, 0.32904857834633455, 0.14102081929128624, 0.04700693976376208, 0.07834489960627013, 0.15668979921254025, 0.10968285944877819, 0.031337959842508055, 0.06267591968501611, 0.04700693976376208, 0.015668979921254027, 0.09738667756534604, 0.09738667756534604, 0.27592891976848044, 0.11361779049290371, 0.2434666939133651, 0.04869333878267302, 0.032462225855115344, 0.032462225855115344, 0.032462225855115344, 0.032462225855115344, 0.16002390183170362, 0.16002390183170362, 0.16002390183170362, 0.16002390183170362, 0.16002390183170362, 0.16002390183170362, 0.16002390183170362, 0.21894423801831467, 0.0656832714054944, 0.10947211900915733, 0.08757769520732586, 0.10947211900915733, 0.15326096661282027, 0.1313665428109888, 0.04378884760366293, 0.04378884760366293, 0.021894423801831465, 0.35999098093277526, 0.2682279428795824, 0.10729117715183295, 0.22352328573298533, 0.0715274514345553, 0.06258652000523589, 0.03576372571727765, 0.04470465714659706, 0.09835024572251354, 0.08046838286387471, 0.017881862858638825, 0.19737294528922383, 0.1450572248511163, 0.14981319943639881, 0.12127735192470379, 0.1307893010952688, 0.06658364419395503, 0.06658364419395503, 0.04518175856018377, 0.05707169502339002, 0.019023898341130007, 0.1822909194846538, 0.11342546101267349, 0.17824001016277263, 0.11747637033455469, 0.0972218237251487, 0.07696727711574272, 0.06886545847198033, 0.06886545847198033, 0.07696727711574272, 0.024305455931287176, 0.29802315860005935, 0.13214558167352858, 0.13214558167352858, 0.04404852722450952, 0.08809705444901904, 0.08809705444901904, 0.13214558167352858, 0.08809705444901904, 0.04404852722450952, 0.26429116334705716, 0.10459067366957295, 0.10459067366957295, 0.10459067366957295, 0.10459067366957295, 0.2091813473391459, 0.10459067366957295, 0.2091813473391459, 0.10459067366957295, 0.21197764260954804, 0.21197764260954804, 0.21197764260954804, 0.21197764260954804, 0.12460789889805692, 0.12460789889805692, 0.12460789889805692, 0.12460789889805692, 0.12460789889805692, 0.12460789889805692, 0.12460789889805692, 0.12460789889805692, 0.12460789889805692, 0.39477392059650246, 0.04934674007456281, 0.09869348014912561, 0.09869348014912561, 0.04934674007456281, 0.14804022022368843, 0.04934674007456281, 0.04934674007456281, 0.4178990266124027, 0.044774895708471714, 0.10447475665310067, 0.0746248261807862, 0.044774895708471714, 0.1940245480700441, 0.029849930472314476, 0.05969986094462895, 0.014924965236157238, 0.014924965236157238, 0.21657385528907727, 0.13987061487419575, 0.1669423467853304, 0.09023910637044887, 0.07670324041488154, 0.11279888296306108, 0.04060759786670199, 0.0721912850963591, 0.05865541914079176, 0.031583687229657105, 0.27750842308577156, 0.10703896319022618, 0.12686099341063842, 0.09911015110206127, 0.09118133901389637, 0.09514574505797882, 0.055501684617154316, 0.06739490274940167, 0.07135930879348412, 0.011893218132247352, 0.3378048349225127, 0.16890241746125634, 0.16890241746125634, 0.22609615462253677, 0.11304807731126838, 0.11304807731126838, 0.11304807731126838, 0.11304807731126838, 0.11304807731126838, 0.11304807731126838, 0.11304807731126838, 0.23924621218996736, 0.23924621218996736, 0.18446450899328198, 0.12297633932885466, 0.06148816966442733, 0.09223225449664099, 0.12297633932885466, 0.09223225449664099, 0.09223225449664099, 0.030744084832213664, 0.15372042416106832, 0.030744084832213664, 0.41702285587729404, 0.13900761862576466, 0.09267174575050978, 0.04633587287525489, 0.09267174575050978, 0.04633587287525489, 0.04633587287525489, 0.04633587287525489, 0.04633587287525489, 0.4308923321827908, 0.1077230830456977, 0.1077230830456977, 0.1077230830456977, 0.1077230830456977, 0.1077230830456977, 0.20357034330357607, 0.13459196251476105, 0.11103836907467786, 0.12113276626328494, 0.11776796720041591, 0.08075517750885662, 0.07066078032024954, 0.08411997657172565, 0.053836785005904414, 0.02187119390864867, 0.1474355010663326, 0.1474355010663326, 0.08846130063979957, 0.11794840085306608, 0.05897420042653304, 0.17692260127959913, 0.05897420042653304, 0.05897420042653304, 0.08846130063979957, 0.05897420042653304, 0.2367537905520217, 0.2367537905520217, 0.2367537905520217, 0.12582159887222957, 0.13550018340086262, 0.12582159887222957, 0.13550018340086262, 0.16453593698676175, 0.06775009170043131, 0.12582159887222957, 0.06775009170043131, 0.04839292264316522, 0.019357169057266088, 0.18325947676282248, 0.36651895352564495, 0.18325947676282248, 0.18325947676282248, 0.14910778495814825, 0.14910778495814825, 0.2982155699162965, 0.14910778495814825, 0.14910778495814825, 0.2583406282446182, 0.13320688643863124, 0.10898745254069829, 0.20182861581610795, 0.09687773559173182, 0.04440229547954375, 0.06458515706115454, 0.060548584744832386, 0.008073144632644318, 0.024219433897932956, 0.19844431090650488, 0.15875544872520392, 0.03968886218130098, 0.07937772436260196, 0.11906658654390292, 0.07937772436260196, 0.15875544872520392, 0.11906658654390292, 0.03968886218130098, 0.264272437459391, 0.10382131471618931, 0.15101282140536626, 0.07550641070268313, 0.04719150668917696, 0.06606810936484775, 0.06606810936484775, 0.056629808027012354, 0.14157452006753088, 0.018876602675670782, 0.19151384023180962, 0.13166576515936912, 0.1436353801738572, 0.0837873051014167, 0.19151384023180962, 0.0718176900869286, 0.059848075072440506, 0.059848075072440506, 0.047878460057952404, 0.023939230028976202, 0.20151006683526307, 0.16120805346821046, 0.12090604010115784, 0.08060402673410523, 0.20151006683526307, 0.08060402673410523, 0.040302013367052615, 0.08060402673410523, 0.040302013367052615, 0.040302013367052615, 0.19454446144943968, 0.19454446144943968, 0.19454446144943968, 0.19454446144943968, 0.1793438997134565, 0.4099289136307577, 0.05124111420384471, 0.05124111420384471, 0.025620557101922355, 0.05124111420384471, 0.025620557101922355, 0.12810278550961177, 0.05124111420384471, 0.025620557101922355, 0.14114374464245966, 0.14114374464245966, 0.14114374464245966, 0.14114374464245966, 0.14114374464245966, 0.14114374464245966, 0.3849941490529109, 0.1888848139152859, 0.19882611991082724, 0.08284421662951134, 0.10272682862059407, 0.1292369779420377, 0.0861579852946918, 0.0795304479643309, 0.05633406730806772, 0.053020298642887265, 0.019882611991082724, 0.18795077928058312, 0.37590155856116625, 0.18795077928058312, 0.09397538964029156, 0.09397538964029156, 0.09397538964029156, 0.17828837232149436, 0.08914418616074718, 0.08914418616074718, 0.26743255848224157, 0.08914418616074718, 0.08914418616074718, 0.08914418616074718, 0.08914418616074718, 0.39447692875834595, 0.0733172178075118, 0.1466344356150236, 0.0733172178075118, 0.0733172178075118, 0.0733172178075118, 0.1466344356150236, 0.0733172178075118, 0.36658608903755896, 0.198097164584071, 0.198097164584071, 0.198097164584071, 0.198097164584071, 0.20086403144048806, 0.13056162043631725, 0.09038881414821963, 0.19082082986846366, 0.10043201572024403, 0.09038881414821963, 0.050216007860122015, 0.07030241100417083, 0.040172806288097614, 0.020086403144048807, 0.18940730805787567, 0.11012052794062539, 0.11012052794062539, 0.11893017017587543, 0.11893017017587543, 0.0836916012348753, 0.10571570682300038, 0.07488195899962527, 0.05726267452912521, 0.03083374782337511, 0.26558134688301444, 0.1314216974266463, 0.12320784133748092, 0.10130422509970653, 0.09856627306998474, 0.07392470480248856, 0.06844880074304495, 0.06297289668360136, 0.04654518450527057, 0.024641568267496184, 0.1940395345075725, 0.1164237207045435, 0.077615813803029, 0.13582767415530075, 0.09701976725378625, 0.13582767415530075, 0.077615813803029, 0.1164237207045435, 0.0388079069015145, 0.01940395345075725, 0.23624583665238383, 0.23624583665238383, 0.23624583665238383, 0.13464471098431638, 0.3029505997147119, 0.13464471098431638, 0.06732235549215819, 0.06732235549215819, 0.10098353323823729, 0.06732235549215819, 0.06732235549215819, 0.033661177746079095, 0.2483673113527391, 0.14659729109112893, 0.1502319346719007, 0.11630859458469733, 0.09450073310006658, 0.0739044194756931, 0.06300048873337773, 0.04967346227054782, 0.03876953152823245, 0.01817321790385896, 0.15657241417185136, 0.1217785443558844, 0.1217785443558844, 0.08698467453991743, 0.1391754792638679, 0.20876321889580182, 0.052190804723950456, 0.052190804723950456, 0.03479386981596697, 0.03479386981596697, 0.33117975237255703, 0.06132958377269575, 0.13083644538175093, 0.0981273340363132, 0.057240944854516036, 0.1717228345635481, 0.028620472427258018, 0.07768413944541462, 0.0245318335090783, 0.016354555672718867, 0.1679216905536933, 0.1679216905536933, 0.1679216905536933, 0.1679216905536933, 0.1679216905536933, 0.29021018434564827, 0.12253318894594037, 0.10641040092673769, 0.11285951613441876, 0.08706305530369447, 0.09028761290753501, 0.05804203686912965, 0.07094026728449179, 0.04836836405760804, 0.01612278801920268, 0.30060983195658975, 0.16456069127572637, 0.10970712751715092, 0.14627617002286789, 0.1279916487700094, 0.09142260626429242, 0.14627617002286789, 0.07313808501143394, 0.05485356375857546, 0.03656904250571697, 0.03656904250571697, 0.21627587326576964, 0.21627587326576964, 0.21627587326576964, 0.21627587326576964, 0.15938442287911417, 0.11953831715933562, 0.07969221143955708, 0.11953831715933562, 0.07969221143955708, 0.11953831715933562, 0.07969221143955708, 0.07969221143955708, 0.07969221143955708, 0.07969221143955708, 0.19994712470005177, 0.18328486430838078, 0.06664904156668393, 0.2499339058750647, 0.04998678117501294, 0.06664904156668393, 0.04998678117501294, 0.06664904156668393, 0.04998678117501294, 0.016662260391670982, 0.19668612569763363, 0.07867445027905345, 0.1180116754185802, 0.07867445027905345, 0.07867445027905345, 0.19668612569763363, 0.07867445027905345, 0.07867445027905345, 0.03933722513952673, 0.03933722513952673, 0.17557093200279714, 0.13330385577990153, 0.12354991511307947, 0.15931436422476036, 0.09753940666822064, 0.11704728800186477, 0.05527233044532503, 0.06827758466775445, 0.04876970333411032, 0.026010508444858835, 0.09597383310229607, 0.09597383310229607, 0.2879214993068882, 0.09597383310229607, 0.09597383310229607, 0.09597383310229607, 0.16861057997584392, 0.13488846398067514, 0.14612916931239806, 0.22481410663445855, 0.08992564265378342, 0.06744423199033757, 0.05620352665861464, 0.05620352665861464, 0.033722115995168785, 0.022481410663445854, 0.22775915050074985, 0.13046397941305088, 0.10392893275276935, 0.17468905718018676, 0.08845015553427178, 0.07739388609248782, 0.07076012442741743, 0.05749260109727666, 0.05528134720891987, 0.017690031106854357, 0.15451053337646845, 0.15451053337646845, 0.3090210667529369, 0.15451053337646845, 0.15451053337646845, 0.21100783336509193, 0.12961909763855647, 0.10851831430204728, 0.12961909763855647, 0.12660470001905516, 0.0663167476290289, 0.07234554286803152, 0.08440313334603677, 0.057273554770524954, 0.015071988097506568, 0.33400180216800607, 0.055666967028001016, 0.14844524540800272, 0.09277827838000169, 0.07422262270400136, 0.18555655676000338, 0.01855565567600034, 0.055666967028001016, 0.01855565567600034, 0.01855565567600034, 0.2890821868682938, 0.0873969402159958, 0.1378182518790703, 0.12101114799137878, 0.10420404410368729, 0.07731267788338089, 0.0638669947732277, 0.04705989088553619, 0.050421311663074495, 0.0201685246652298, 0.15343839148017485, 0.10229226098678322, 0.10229226098678322, 0.10229226098678322, 0.10229226098678322, 0.05114613049339161, 0.15343839148017485, 0.15343839148017485, 0.2028738176957382, 0.0676246058985794, 0.1352492117971588, 0.0676246058985794, 0.1352492117971588, 0.0676246058985794, 0.1352492117971588, 0.1352492117971588, 0.2139603864915379, 0.07132012883051263, 0.2852805153220505, 0.07132012883051263, 0.07132012883051263, 0.07132012883051263, 0.07132012883051263, 0.07132012883051263, 0.07132012883051263, 0.19208872358939205, 0.10976498490822403, 0.05488249245411202, 0.13720623113528005, 0.19208872358939205, 0.05488249245411202, 0.08232373868116802, 0.13720623113528005, 0.02744124622705601, 0.02744124622705601, 0.353381703277219, 0.0353381703277219, 0.10601451098316571, 0.10601451098316571, 0.0353381703277219, 0.21202902196633142, 0.0353381703277219, 0.0706763406554438, 0.42710745746777423, 0.14236915248925808, 0.14236915248925808, 0.14236915248925808, 0.08687656201063587, 0.21719140502658968, 0.08687656201063587, 0.08687656201063587, 0.15203398351861278, 0.08687656201063587, 0.17375312402127174, 0.043438281005317936, 0.021719140502658968, 0.021719140502658968, 0.2226489944958538, 0.2226489944958538, 0.2226489944958538, 0.2226489944958538, 0.2226489944958538, 0.17255026927076436, 0.12078518848953505, 0.10353016156245862, 0.08627513463538218, 0.1380402154166115, 0.22431535005199366, 0.03451005385415287, 0.03451005385415287, 0.03451005385415287, 0.05176508078122931, 0.26910848565306494, 0.26910848565306494, 0.26910848565306494, 0.21950759463932565, 0.1463383964262171, 0.07316919821310855, 0.07316919821310855, 0.07316919821310855, 0.21950759463932565, 0.07316919821310855, 0.07316919821310855, 0.3621700360370413, 0.09054250900926032, 0.09054250900926032, 0.13581376351389046, 0.04527125450463016, 0.13581376351389046, 0.04527125450463016, 0.04527125450463016, 0.04527125450463016, 0.24380510834913421, 0.12190255417456711, 0.0812683694497114, 0.12190255417456711, 0.12190255417456711, 0.0406341847248557, 0.0812683694497114, 0.1625367388994228, 0.0406341847248557, 0.11781255285159317, 0.11781255285159317, 0.058906276425796586, 0.17671882927738977, 0.11781255285159317, 0.058906276425796586, 0.11781255285159317, 0.058906276425796586, 0.058906276425796586, 0.058906276425796586, 0.190965736571281, 0.06365524552376033, 0.190965736571281, 0.2546209820950413, 0.12731049104752065, 0.06365524552376033, 0.06365524552376033, 0.06365524552376033, 0.06365524552376033, 0.18930321491149218, 0.1032562990426321, 0.0860469158688601, 0.13767506539017615, 0.0860469158688601, 0.1720938317377202, 0.0860469158688601, 0.06883753269508808, 0.05162814952131605, 0.01720938317377202, 0.2699153243894613, 0.2699153243894613, 0.15527863791449126, 0.15527863791449126, 0.3105572758289825, 0.15527863791449126, 0.15527863791449126, 0.181323716567987, 0.362647433135974, 0.181323716567987, 0.181323716567987, 0.2319210173064196, 0.10730673935073146, 0.1765368937705582, 0.11422975479271413, 0.07961467758280076, 0.08653769302478344, 0.058845631256852736, 0.07269166214081808, 0.0553841235358614, 0.017307538604956686, 0.19366765541941577, 0.13334494307566333, 0.16826861864309894, 0.1079459062993465, 0.1047710267023069, 0.09524638791118809, 0.06667247153783167, 0.06984735113487126, 0.041273434761514835, 0.015874397985198015, 0.22093648002227825, 0.16948551892119976, 0.10290192220215699, 0.07263665096622847, 0.13014066631449267, 0.09079581370778558, 0.08171623233700702, 0.08171623233700702, 0.03329179835952138, 0.018159162741557117, 0.1777626264300025, 0.13332196982250188, 0.04444065660750063, 0.13332196982250188, 0.22220328303750314, 0.04444065660750063, 0.08888131321500126, 0.13332196982250188, 0.04444065660750063, 0.3718068082569927, 0.1239356027523309, 0.1239356027523309, 0.1239356027523309, 0.1239356027523309, 0.1239356027523309, 0.12071006458275375, 0.12071006458275375, 0.12071006458275375, 0.12071006458275375, 0.12071006458275375, 0.2414201291655075, 0.12071006458275375, 0.17292425954564644, 0.3458485190912929, 0.17292425954564644, 0.24608494084893792, 0.1281692400254885, 0.11278893122242989, 0.11278893122242989, 0.1025353920203908, 0.08202831361631265, 0.056394465611214945, 0.08715508321733219, 0.04101415680815632, 0.03076061760611724, 0.1762948637012987, 0.19129868188864327, 0.131283409139265, 0.12378150004559271, 0.08252100003039514, 0.07126813638988672, 0.06001527274937828, 0.08627195457723129, 0.06376622729621442, 0.01500381818734457, 0.19581579237001637, 0.1566526338960131, 0.07832631694800656, 0.07832631694800656, 0.11748947542200983, 0.11748947542200983, 0.1566526338960131, 0.03916315847400328, 0.03916315847400328, 0.2402778340121798, 0.1201389170060899, 0.06006945850304495, 0.1201389170060899, 0.09010418775456742, 0.16519101088337362, 0.09010418775456742, 0.06006945850304495, 0.04505209387728371, 0.015017364625761238, 0.1918317182424736, 0.07673268729698944, 0.07673268729698944, 0.26856440553946304, 0.11509903094548417, 0.07673268729698944, 0.07673268729698944, 0.03836634364849472, 0.03836634364849472, 0.18959151234026025, 0.18959151234026025, 0.18959151234026025, 0.18959151234026025, 0.1969221049257559, 0.11539384492464676, 0.1492594298481844, 0.13671662061724454, 0.094071069232049, 0.08529110277039109, 0.06773116984707528, 0.08654538369348508, 0.055188360616135405, 0.012542809230939866, 0.16582237283637014, 0.11844455202597867, 0.11844455202597867, 0.26057801445715306, 0.04737782081039147, 0.04737782081039147, 0.07106673121558721, 0.07106673121558721, 0.09475564162078294, 0.023688910405195735, 0.22359190550561023, 0.22359190550561023, 0.22359190550561023, 0.23139476492260638, 0.23139476492260638, 0.23139476492260638, 0.21256042638968067, 0.18398087326165638, 0.15897376427463514, 0.08037999317256833, 0.06430399453805466, 0.09288354766607895, 0.0678764386790577, 0.06251777246755315, 0.05180044004454403, 0.023220886916519738, 0.2617546293332057, 0.2617546293332057, 0.22360466488284664, 0.0838517493310675, 0.09316861036785277, 0.1583866376253497, 0.0838517493310675, 0.09316861036785277, 0.11180233244142332, 0.06521802725749694, 0.06521802725749694, 0.018633722073570554, 0.127645022229864, 0.28720130001719407, 0.09573376667239801, 0.09573376667239801, 0.063822511114932, 0.09573376667239801, 0.063822511114932, 0.127645022229864, 0.063822511114932, 0.031911255557466, 0.3554772444387243, 0.1442609395440905, 0.1154087516352724, 0.1154087516352724, 0.288521879088181, 0.0577043758176362, 0.0288521879088181, 0.08655656372645429, 0.0577043758176362, 0.08655656372645429, 0.0288521879088181, 0.15136611405651393, 0.15136611405651393, 0.15136611405651393, 0.25227685676085654, 0.050455371352171305, 0.050455371352171305, 0.050455371352171305, 0.050455371352171305, 0.10091074270434261, 0.24044060289110888, 0.12022030144555444, 0.12022030144555444, 0.12022030144555444, 0.12022030144555444, 0.24044060289110888, 0.12022030144555444, 0.1666587419020554, 0.13421190719546056, 0.12683762658032535, 0.11798848984216312, 0.12241305821124424, 0.11651363371913608, 0.08406679901254122, 0.05456967655200044, 0.05604453267502748, 0.022122841845405583, 0.22438300320713248, 0.22438300320713248, 0.22438300320713248, 0.1359890993726938, 0.0679945496863469, 0.0679945496863469, 0.23798092390221415, 0.16998637421586724, 0.10199182452952034, 0.0679945496863469, 0.03399727484317345, 0.03399727484317345, 0.03399727484317345, 0.1486829925695711, 0.1486829925695711, 0.2973659851391422, 0.1486829925695711, 0.1486829925695711, 0.39258136160216245, 0.18751042719719083, 0.07500417087887634, 0.1312572990380336, 0.15000834175775268, 0.1125062563183145, 0.1125062563183145, 0.07500417087887634, 0.07500417087887634, 0.05625312815915725, 0.03750208543943817, 0.11452559270923679, 0.34357677812771037, 0.03817519756974559, 0.07635039513949118, 0.11452559270923679, 0.07635039513949118, 0.07635039513949118, 0.11452559270923679, 0.07635039513949118, 0.03817519756974559, 0.206468297978657, 0.11646929629565267, 0.12176335521818234, 0.12352804152569222, 0.13764553198577134, 0.09352837429802412, 0.05999933445533622, 0.06529339337786588, 0.054705275532806553, 0.019411549382608778, 0.22778891550639607, 0.1407282305222761, 0.12283904867622406, 0.12045382443008379, 0.08586807286104983, 0.08109762436876929, 0.05724538190736655, 0.0882532971071901, 0.05724538190736655, 0.01788918184605205, 0.22775650131060987, 0.11755174261192768, 0.1102047586986822, 0.1102047586986822, 0.11387825065530494, 0.11387825065530494, 0.06244936326258658, 0.05142888739271836, 0.06612285521920931, 0.02938793565298192, 0.21831208228360796, 0.10915604114180398, 0.12900259407667744, 0.10584828231932507, 0.1190793176092407, 0.1157715587867618, 0.08930948820693053, 0.056231899982141446, 0.036385347047267994, 0.019846552934873453, 0.3442350628676762, 0.10150521084559683, 0.09267867077206667, 0.09267867077206667, 0.08826540073530158, 0.09267867077206667, 0.0661990505514762, 0.039719430330885715, 0.0661990505514762, 0.017653080147060317, 0.18968545126943875, 0.20804210784390056, 0.1193182677340018, 0.09178328287230908, 0.08260495458507816, 0.08566439734748847, 0.07036718353543696, 0.07342662629784726, 0.055069969723385445, 0.021416099336872118, 0.1286840503111259, 0.02573681006222518, 0.10294724024890071, 0.1286840503111259, 0.15442086037335107, 0.07721043018667553, 0.10294724024890071, 0.07721043018667553, 0.18015767043557626, 0.02573681006222518, 0.3988781655843128, 0.25839253962903685, 0.3875888094435553, 0.12919626981451843, 0.12919626981451843, 0.23543517849870907, 0.11771758924935453, 0.11771758924935453, 0.23543517849870907, 0.11771758924935453, 0.11771758924935453, 0.29000279438350335, 0.29000279438350335, 0.29000279438350335, 0.22911787201824116, 0.10501235800836053, 0.11137674334220056, 0.1177411286760406, 0.0795548166730004, 0.11137674334220056, 0.09228358734068047, 0.07000823867224036, 0.06364385333840032, 0.022275348668440113, 0.14109860313125555, 0.14109860313125555, 0.14109860313125555, 0.14109860313125555, 0.14109860313125555, 0.14109860313125555, 0.14109860313125555, 0.1458829783329734, 0.1458829783329734, 0.43764893499892016, 0.13312414854154028, 0.13312414854154028, 0.13312414854154028, 0.26624829708308057, 0.13312414854154028, 0.13312414854154028, 0.2522672675400899, 0.2745261440877449, 0.07419625515884996, 0.06677662964296498, 0.08161588067473496, 0.051937378611194976, 0.03709812757942498, 0.10387475722238995, 0.051937378611194976, 0.014839251031769994, 0.2627282696488242, 0.2627282696488242, 0.17653417645922262, 0.08826708822961131, 0.08826708822961131, 0.26480126468883397, 0.08826708822961131, 0.08826708822961131, 0.08826708822961131, 0.08826708822961131, 0.27843809866457175, 0.09281269955485726, 0.09281269955485726, 0.09281269955485726, 0.09281269955485726, 0.09281269955485726, 0.09281269955485726, 0.1856253991097145, 0.2748680438713758, 0.10689312817220172, 0.09925790473133016, 0.08653253232987758, 0.12470864953423533, 0.11707342609336378, 0.06871701096784395, 0.05090148960581034, 0.048356415125519824, 0.02545074480290517, 0.3615832990968986, 0.1421787328961274, 0.2843574657922548, 0.0710893664480637, 0.0710893664480637, 0.0710893664480637, 0.0710893664480637, 0.0710893664480637, 0.0710893664480637, 0.0710893664480637, 0.3525219591728269, 0.17095246539749923, 0.17095246539749923, 0.34190493079499845, 0.21264970364875588, 0.10632485182437794, 0.10632485182437794, 0.21264970364875588, 0.10632485182437794, 0.10632485182437794, 0.10632485182437794, 0.24768232932739273, 0.2352982128610231, 0.08668881526458745, 0.11145704819732673, 0.12384116466369637, 0.049536465865478545, 0.049536465865478545, 0.03715234939910891, 0.049536465865478545, 0.012384116466369636, 0.23075988577224366, 0.1201215843745926, 0.16121581060800586, 0.06954407516423781, 0.09799392409506238, 0.0474164148847076, 0.09167173544376803, 0.12328267870023976, 0.03477203758211891, 0.01896656595388304, 0.1657459801871859, 0.1657459801871859, 0.1657459801871859, 0.1657459801871859, 0.1657459801871859, 0.1657459801871859, 0.1657459801871859, 0.15260997219982297, 0.15260997219982297, 0.30521994439964595, 0.15260997219982297, 0.15260997219982297, 0.21843397400036804, 0.2366368051670654, 0.10011557141683536, 0.13652123375023004, 0.08191274025013802, 0.06370990908344068, 0.04550707791674335, 0.06370990908344068, 0.03640566233339468, 0.01820283116669734, 0.3458870811039196, 0.3458870811039196, 0.3457928045001749, 0.057632134083362485, 0.09605355680560414, 0.1344749795278458, 0.07684284544448332, 0.15368569088896664, 0.03842142272224166, 0.07684284544448332, 0.03842142272224166, 0.01921071136112083], \"Term\": [\"abalone\", \"abalone\", \"abalone\", \"absent\", \"absent\", \"absent\", \"absent\", \"absent\", \"absent\", \"absent\", \"absent\", \"accumulated\", \"accumulated\", \"accumulated\", \"accumulated\", \"accumulated\", \"accumulated\", \"accumulated\", \"accumulated\", \"accumulated\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"ach\", \"ach\", \"ach\", \"acktr\", \"acktr\", \"acktr\", \"acktr\", \"acktr\", \"acktr\", \"acktr\", \"acktr\", \"acktr\", \"acktr\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity\", \"actor\", \"actor\", \"actor\", \"actor\", \"actor\", \"actor\", \"actor\", \"actor\", \"actor\", \"actor\", \"adwords\", \"adwords\", \"adwords\", \"adwords\", \"adwords\", \"adwords\", \"adwords\", \"adwords\", \"adwords\", \"agg\", \"agg\", \"agg\", \"ajor\", \"ajor\", \"ajor\", \"ajor\", \"ajor\", \"ajor\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"alignment\", \"alignment\", \"alignment\", \"alignment\", \"alignment\", \"alignment\", \"alignment\", \"alignment\", \"alignment\", \"alignment\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"ambig\", \"ambig\", \"ambig\", \"ambig\", \"ambig\", \"ambig\", \"ambig\", \"anhedonia\", \"anhedonia\", \"anhedonia\", \"anhedonia\", \"anhedonia\", \"anhedonia\", \"anhedonia\", \"anhedonia\", \"anns\", \"anns\", \"anns\", \"anns\", \"anns\", \"anns\", \"anns\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"astrophysics\", \"asymmetries\", \"asymmetries\", \"asymmetries\", \"atari\", \"atari\", \"atari\", \"atari\", \"atari\", \"atari\", \"atari\", \"atari\", \"atari\", \"balls\", \"balls\", \"balls\", \"balls\", \"balls\", \"balls\", \"balls\", \"balls\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"bcm\", \"bcm\", \"bcm\", \"bcm\", \"bcm\", \"bcm\", \"bcm\", \"bcm\", \"bcm\", \"bdia\", \"bdia\", \"bdia\", \"bdia\", \"bdia\", \"bdia\", \"bdia\", \"bdia\", \"bdia\", \"behavioural\", \"behavioural\", \"behavioural\", \"behavioural\", \"behavioural\", \"behavioural\", \"behavioural\", \"bf\", \"bf\", \"bf\", \"bf\", \"bf\", \"bf\", \"bf\", \"bf\", \"bipolar\", \"bipolar\", \"bipolar\", \"bipolar\", \"bipolar\", \"bipolar\", \"bipolar\", \"bipolar\", \"bipolar\", \"bipolar\", \"birds\", \"birds\", \"bits\", \"bits\", \"bits\", \"bits\", \"bits\", \"bits\", \"bits\", \"bits\", \"bits\", \"bits\", \"bkx\", \"bls\", \"bls\", \"bls\", \"bls\", \"bls\", \"bls\", \"bls\", \"bls\", \"bls\", \"borgwardt\", \"boring\", \"boring\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"boyan\", \"boyan\", \"boyan\", \"boyan\", \"boyan\", \"boyan\", \"boyan\", \"breakdown\", \"breakdown\", \"breakdown\", \"breakdown\", \"breakdown\", \"breakdown\", \"breakdown\", \"breakdown\", \"breakdown\", \"breakdown\", \"broadcasting\", \"broadcasting\", \"broadcasting\", \"bskraibd\", \"buhmann\", \"buhmann\", \"buhmann\", \"buhmann\", \"buhmann\", \"buhmann\", \"buhmann\", \"buhmann\", \"buhmann\", \"buhmann\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"camera\", \"camera\", \"camera\", \"camera\", \"camera\", \"camera\", \"camera\", \"camera\", \"camera\", \"camera\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"categorizers\", \"category\", \"category\", \"category\", \"category\", \"category\", \"category\", \"category\", \"category\", \"category\", \"category\", \"causal\", \"causal\", \"causal\", \"causal\", \"causal\", \"causal\", \"causal\", \"causal\", \"causal\", \"causal\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cc\", \"cc\", \"cc\", \"cc\", \"cc\", \"cc\", \"cc\", \"cc\", \"cc\", \"cc\", \"ccd\", \"ccd\", \"ccd\", \"ccd\", \"ccd\", \"ccd\", \"ccd\", \"ccd\", \"ccd\", \"ccd\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"chaperones\", \"chaperones\", \"chaperones\", \"chaperones\", \"circles\", \"circles\", \"circles\", \"circles\", \"circles\", \"circles\", \"circles\", \"circles\", \"circles\", \"circles\", \"classes\", \"classes\", \"classes\", \"classes\", \"classes\", \"classes\", \"classes\", \"classes\", \"classes\", \"classes\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cmt\", \"cmt\", \"cmt\", \"cmt\", \"cmt\", \"cmt\", \"cmt\", \"cmt\", \"cn\", \"cn\", \"cn\", \"cn\", \"cn\", \"cn\", \"cn\", \"cn\", \"cn\", \"cn\", \"coherent\", \"coherent\", \"coherent\", \"coherent\", \"coherent\", \"coherent\", \"coherent\", \"coherent\", \"coherent\", \"coherent\", \"common\", \"common\", \"common\", \"common\", \"common\", \"common\", \"common\", \"common\", \"common\", \"common\", \"comon\", \"comon\", \"comon\", \"comon\", \"comon\", \"comon\", \"comon\", \"compression\", \"compression\", \"compression\", \"compression\", \"compression\", \"compression\", \"compression\", \"compression\", \"compression\", \"compression\", \"conserving\", \"conserving\", \"conserving\", \"conserving\", \"conserving\", \"conserving\", \"conserving\", \"conserving\", \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"cooper\", \"cooper\", \"cooper\", \"cooper\", \"cooper\", \"cooper\", \"cooper\", \"cooper\", \"counterfactual\", \"counterfactual\", \"counterfactual\", \"counterfactual\", \"counterfactual\", \"counterfactuals\", \"critic\", \"critic\", \"critic\", \"critic\", \"critic\", \"critic\", \"critic\", \"critic\", \"critic\", \"critic\", \"cvar\", \"cvar\", \"cvar\", \"cvar\", \"cvar\", \"cvar\", \"cvar\", \"cvar\", \"cvar\", \"cytometry\", \"cytometry\", \"cytometry\", \"cytometry\", \"cytometry\", \"cytometry\", \"cytometry\", \"cytometry\", \"cytometry\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"dayan\", \"dayan\", \"dayan\", \"dayan\", \"dayan\", \"dayan\", \"dayan\", \"dayan\", \"decay\", \"decay\", \"decay\", \"decay\", \"decay\", \"decay\", \"decay\", \"decay\", \"decay\", \"decay\", \"deduce\", \"deduce\", \"deepmind\", \"deepmind\", \"deepmind\", \"deepmind\", \"denotation\", \"denotation\", \"denotation\", \"denotation\", \"denotation\", \"deploy\", \"deploy\", \"deploy\", \"derivatives\", \"derivatives\", \"derivatives\", \"derivatives\", \"derivatives\", \"derivatives\", \"derivatives\", \"derivatives\", \"derivatives\", \"derivatives\", \"differed\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"diffusivity\", \"diffusivity\", \"diffusivity\", \"diffusivity\", \"dilated\", \"dilated\", \"dilated\", \"dilated\", \"dilated\", \"dilated\", \"dilated\", \"dilated\", \"dilated\", \"dilated\", \"dip\", \"dip\", \"dip\", \"dip\", \"dip\", \"dip\", \"dip\", \"dip\", \"dip\", \"dip\", \"discriminant\", \"discriminant\", \"discriminant\", \"discriminant\", \"discriminant\", \"discriminant\", \"discriminant\", \"discriminant\", \"discriminant\", \"disparities\", \"disparities\", \"disparities\", \"disparities\", \"disparities\", \"disparities\", \"disparity\", \"disparity\", \"disparity\", \"disparity\", \"disparity\", \"disparity\", \"disparity\", \"disparity\", \"disparity\", \"disparity\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distortions\", \"distortions\", \"distortions\", \"distortions\", \"distortions\", \"distortions\", \"distortions\", \"distortions\", \"distortions\", \"distortions\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distributions\", \"distributions\", \"distributions\", \"distributions\", \"distributions\", \"distributions\", \"distributions\", \"distributions\", \"distributions\", \"distributions\", \"dng\", \"dng\", \"dng\", \"dng\", \"dng\", \"dng\", \"dng\", \"dng\", \"dng\", \"dng\", \"dopamine\", \"dopamine\", \"dopamine\", \"dp\", \"dp\", \"dp\", \"dp\", \"dp\", \"dp\", \"dp\", \"dp\", \"dp\", \"dp\", \"dyna\", \"dyna\", \"dyna\", \"dyna\", \"dyna\", \"dyna\", \"dyna\", \"dyna\", \"dyna\", \"dyna\", \"economic\", \"economic\", \"economic\", \"economic\", \"economic\", \"economic\", \"economic\", \"economic\", \"edgeworth\", \"edgeworth\", \"edgeworth\", \"edgeworth\", \"edgeworth\", \"edgeworth\", \"edgeworth\", \"eeckman\", \"eeckman\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"eigenvalue\", \"eigenvalue\", \"eigenvalue\", \"eigenvalue\", \"eigenvalue\", \"eigenvalue\", \"eigenvalue\", \"eigenvalue\", \"eigenvalue\", \"eigenvalue\", \"employees\", \"employees\", \"employees\", \"employees\", \"encoders\", \"encoders\", \"encoders\", \"encoders\", \"encoders\", \"encoders\", \"encoders\", \"encoders\", \"encoders\", \"entity\", \"entity\", \"entity\", \"entity\", \"entity\", \"entity\", \"entity\", \"entity\", \"entity\", \"entity\", \"entropy\", \"entropy\", \"entropy\", \"entropy\", \"entropy\", \"entropy\", \"entropy\", \"entropy\", \"entropy\", \"entropy\", \"epical\", \"epical\", \"epical\", \"epical\", \"epical\", \"epical\", \"epical\", \"epical\", \"epical\", \"epical\", \"episode\", \"episode\", \"episode\", \"episode\", \"episode\", \"episode\", \"episode\", \"episode\", \"episode\", \"episode\", \"epx\", \"equation\", \"equation\", \"equation\", \"equation\", \"equation\", \"equation\", \"equation\", \"equation\", \"equation\", \"equation\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"estimators\", \"estimators\", \"estimators\", \"estimators\", \"estimators\", \"estimators\", \"estimators\", \"estimators\", \"estimators\", \"estimators\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"eu\", \"eu\", \"eu\", \"eu\", \"eu\", \"eu\", \"eu\", \"eu\", \"eu\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"exemplar\", \"exemplar\", \"exemplar\", \"exemplar\", \"exemplar\", \"exemplar\", \"exemplar\", \"exemplars\", \"exemplars\", \"exemplars\", \"exemplars\", \"exemplars\", \"exemplars\", \"exemplars\", \"experiences\", \"experiments\", \"experiments\", \"experiments\", \"experiments\", \"experiments\", \"experiments\", \"experiments\", \"experiments\", \"experiments\", \"experiments\", \"explanatory\", \"explanatory\", \"explanatory\", \"explanatory\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"extraction\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"faces\", \"faces\", \"faces\", \"faces\", \"faces\", \"faces\", \"faces\", \"faces\", \"faces\", \"faces\", \"facility\", \"facility\", \"facility\", \"facility\", \"facility\", \"facility\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"feedback\", \"feedback\", \"feedback\", \"feedback\", \"feedback\", \"feedback\", \"feedback\", \"feedback\", \"feedback\", \"feedback\", \"fibers\", \"fibers\", \"fibers\", \"fibers\", \"fibers\", \"fibers\", \"fibers\", \"fibers\", \"fibers\", \"fibers\", \"fields\", \"fields\", \"fields\", \"fields\", \"fields\", \"fields\", \"fields\", \"fields\", \"fields\", \"fields\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"flagged\", \"flagged\", \"flagged\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"forwards\", \"forwards\", \"forwards\", \"forwards\", \"forwards\", \"forwards\", \"forwards\", \"forwards\", \"forwards\", \"freund\", \"freund\", \"fulfilling\", \"fulfilling\", \"fulfilling\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"gam\", \"gam\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"generations\", \"generations\", \"generations\", \"generations\", \"generations\", \"generations\", \"generations\", \"genk\", \"genk\", \"genk\", \"genk\", \"gerhard\", \"gilchrist\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gretton\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"hamming\", \"hamming\", \"hamming\", \"hamming\", \"hamming\", \"hamming\", \"hamming\", \"hamming\", \"hamming\", \"hamming\", \"hardness\", \"hardness\", \"hardness\", \"hardness\", \"hardness\", \"hardness\", \"hardness\", \"hardness\", \"hardness\", \"hardness\", \"hdp\", \"hdp\", \"hdp\", \"hdp\", \"hdp\", \"hdp\", \"hdp\", \"hdp\", \"hdp\", \"hdp\", \"healthy\", \"healthy\", \"healthy\", \"healthy\", \"healthy\", \"healthy\", \"healthy\", \"healthy\", \"helplessness\", \"helplessness\", \"helplessness\", \"helplessness\", \"helplessness\", \"helplessness\", \"helplessness\", \"helplessness\", \"herault\", \"herault\", \"herault\", \"herault\", \"herault\", \"herault\", \"herault\", \"herault\", \"herault\", \"hh\", \"hh\", \"hh\", \"hh\", \"hh\", \"hh\", \"hh\", \"hh\", \"hh\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"hint\", \"hint\", \"hint\", \"hint\", \"hint\", \"hint\", \"hints\", \"hints\", \"hints\", \"hints\", \"hints\", \"hints\", \"hints\", \"horizon\", \"horizon\", \"horizon\", \"horizon\", \"horizon\", \"horizon\", \"horizon\", \"horizon\", \"horizon\", \"hyper\", \"hyper\", \"hyper\", \"hyper\", \"hyper\", \"hyper\", \"hyper\", \"hyper\", \"hyper\", \"hyper\", \"iiwt\", \"iiwt\", \"iiwt\", \"iiwt\", \"iiwt\", \"iiwt\", \"iiwtl\", \"iiwtll\", \"iiwtll\", \"iixtll\", \"iixtll\", \"iixtll\", \"ilated\", \"ilated\", \"ilated\", \"ilated\", \"ilated\", \"ilated\", \"ilated\", \"ilated\", \"ilated\", \"ilated\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"imaginary\", \"imaginary\", \"imaginary\", \"imaginary\", \"imaginary\", \"imbens\", \"income\", \"income\", \"income\", \"income\", \"incorporation\", \"incorporation\", \"incorporation\", \"incorporation\", \"incorporation\", \"incorporation\", \"incorporation\", \"inf\", \"inf\", \"inf\", \"inf\", \"inf\", \"inf\", \"inf\", \"inf\", \"inf\", \"inflections\", \"inflections\", \"inflections\", \"inflections\", \"inflections\", \"inflections\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"instructed\", \"intercept\", \"intercept\", \"intercept\", \"interleave\", \"interleave\", \"interleave\", \"interleave\", \"interleave\", \"interleave\", \"interleave\", \"interview\", \"interview\", \"interview\", \"intrator\", \"intrator\", \"intrator\", \"intrator\", \"intrator\", \"intrator\", \"intrator\", \"invalid\", \"isolet\", \"isolet\", \"isolet\", \"isolet\", \"isolet\", \"isolet\", \"isolet\", \"isolet\", \"italy\", \"italy\", \"italy\", \"italy\", \"italy\", \"iterated\", \"iterated\", \"iterated\", \"iterated\", \"iterated\", \"iterated\", \"iterated\", \"iwk\", \"iwk\", \"iyer\", \"iyer\", \"iyer\", \"iyer\", \"iyer\", \"jegelka\", \"jegelka\", \"jegelka\", \"jegelka\", \"jutten\", \"jutten\", \"jutten\", \"jutten\", \"jutten\", \"jutten\", \"jutten\", \"jutten\", \"jutten\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"klingner\", \"klingner\", \"knew\", \"knew\", \"knew\", \"kurtosis\", \"kurtosis\", \"kurtosis\", \"kurtosis\", \"kurtosis\", \"kurtosis\", \"kurtosis\", \"kurtosis\", \"kurtosis\", \"kurtosis\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"lades\", \"lades\", \"lades\", \"lades\", \"lades\", \"lades\", \"lades\", \"lades\", \"lades\", \"lambda\", \"lambda\", \"lambda\", \"lambda\", \"lambda\", \"lambda\", \"lambda\", \"lambda\", \"lambda\", \"lambertian\", \"lambertian\", \"laplace\", \"laplace\", \"laplace\", \"laplace\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"latents\", \"latents\", \"latents\", \"laws\", \"laws\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"lhe\", \"lighting\", \"lighting\", \"lighting\", \"lighting\", \"lighting\", \"lighting\", \"lighting\", \"lighting\", \"lighting\", \"lighting\", \"lightness\", \"lightness\", \"lightness\", \"lightness\", \"liitll\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linkage\", \"linkage\", \"linkage\", \"linkage\", \"linkage\", \"linsker\", \"linsker\", \"linsker\", \"linsker\", \"linsker\", \"linsker\", \"linsker\", \"linsker\", \"linsker\", \"live\", \"live\", \"live\", \"live\", \"live\", \"live\", \"live\", \"lmj\", \"lmnn\", \"lmnn\", \"lmnn\", \"lmnn\", \"lmnn\", \"lmnn\", \"lmnn\", \"lmnn\", \"lmnn\", \"lmnn\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"lowlik\", \"lowlik\", \"lowlik\", \"lowlik\", \"lowlik\", \"lowlik\", \"lowlik\", \"lowlik\", \"lstd\", \"lstd\", \"lstd\", \"lstd\", \"lstd\", \"lstd\", \"lstd\", \"lstd\", \"lstd\", \"lw\", \"lw\", \"lw\", \"lw\", \"lw\", \"lwi\", \"lwk\", \"lwk\", \"lwk\", \"mabs\", \"mabs\", \"mabs\", \"mabs\", \"mabs\", \"mabs\", \"mabs\", \"malsburg\", \"malsburg\", \"malsburg\", \"malsburg\", \"malsburg\", \"manifest\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matroid\", \"matroid\", \"mcts\", \"mcts\", \"mcts\", \"mcts\", \"mcts\", \"mcts\", \"mcts\", \"mcts\", \"mcts\", \"mcts\", \"mdd\", \"mdd\", \"mdd\", \"mdd\", \"mdd\", \"mdd\", \"mdd\", \"mdd\", \"mdd\", \"mdps\", \"mdps\", \"mdps\", \"mdps\", \"mdps\", \"mdps\", \"mdps\", \"mdps\", \"mdps\", \"mdps\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"men\", \"men\", \"men\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"microclustering\", \"microclustering\", \"microclustering\", \"microclustering\", \"microclustering\", \"microclustering\", \"microclustering\", \"microclustering\", \"microclustering\", \"miller\", \"miller\", \"miller\", \"miller\", \"miller\", \"miller\", \"miller\", \"miller\", \"miller\", \"minimax\", \"minimax\", \"minimax\", \"minimax\", \"minimax\", \"minimax\", \"minimax\", \"minimax\", \"minimax\", \"minimax\", \"mis\", \"mis\", \"misno\", \"misno\", \"misno\", \"mistake\", \"mistake\", \"mistake\", \"mistake\", \"mistake\", \"mistake\", \"mistake\", \"mistake\", \"mistake\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mmi\", \"mmi\", \"mmi\", \"mmi\", \"mmi\", \"mmi\", \"mmi\", \"mmi\", \"mmi\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"modular\", \"modular\", \"modular\", \"modular\", \"modular\", \"modular\", \"modular\", \"modular\", \"modular\", \"modular\", \"monotone\", \"monotone\", \"monotone\", \"monotone\", \"monotone\", \"monotone\", \"monotone\", \"monotone\", \"monotone\", \"monotone\", \"morphology\", \"morphology\", \"morphology\", \"morphology\", \"morphology\", \"morphology\", \"morphology\", \"morphology\", \"motor\", \"motor\", \"motor\", \"motor\", \"motor\", \"motor\", \"motor\", \"motor\", \"motor\", \"motor\", \"mtl\", \"mtl\", \"mtl\", \"mtl\", \"mtl\", \"mtl\", \"mtl\", \"mtl\", \"mtl\", \"mtl\", \"mujoco\", \"mujoco\", \"mujoco\", \"mujoco\", \"mujoco\", \"mujoco\", \"mujoco\", \"mujoco\", \"mujoco\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"nbd\", \"nbd\", \"nbd\", \"nbd\", \"nbd\", \"nbd\", \"nbd\", \"nbd\", \"nbd\", \"nbnb\", \"nbnb\", \"nbnb\", \"nbnb\", \"nbnb\", \"nbnb\", \"nbnb\", \"nbnb\", \"nbnb\", \"nbnb\", \"ndb\", \"ndb\", \"ndb\", \"ndb\", \"ndb\", \"ndb\", \"ndb\", \"ndb\", \"ndb\", \"ndb\", \"negbin\", \"negbin\", \"negbin\", \"negbin\", \"negbin\", \"nests\", \"nests\", \"net\", \"net\", \"net\", \"net\", \"net\", \"net\", \"net\", \"net\", \"net\", \"net\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"nion\", \"nion\", \"nion\", \"nion\", \"nion\", \"nion\", \"nion\", \"nk\", \"nk\", \"nk\", \"nk\", \"nk\", \"nk\", \"nk\", \"nk\", \"nk\", \"nk\", \"nltcs\", \"nltcs\", \"nltcs\", \"nltcs\", \"nltcs\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"normalgamma\", \"normalgamma\", \"normalgamma\", \"normalgamma\", \"normalgamma\", \"normalgamma\", \"normalgamma\", \"normalgamma\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"occlusions\", \"occlusions\", \"omniglot\", \"omniglot\", \"omniglot\", \"omniglot\", \"omniglot\", \"omniglot\", \"omniglot\", \"omniglot\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"operationnelle\", \"operationnelle\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimised\", \"optimistic\", \"optimistic\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"ormalgamma\", \"ormalgamma\", \"ormalgamma\", \"ormalgamma\", \"ormalgamma\", \"orthant\", \"orthant\", \"orthant\", \"orthant\", \"orthant\", \"orthant\", \"orthant\", \"orthant\", \"orthant\", \"outcome\", \"outcome\", \"outcome\", \"outcome\", \"outcome\", \"outcome\", \"outcome\", \"outcome\", \"outcome\", \"outcome\", \"outperforming\", \"outperforming\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"paint\", \"paint\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parse\", \"parse\", \"parse\", \"parse\", \"parse\", \"parse\", \"parse\", \"participant\", \"participant\", \"participant\", \"participants\", \"participants\", \"participants\", \"participants\", \"participants\", \"participants\", \"participants\", \"participants\", \"partition\", \"partition\", \"partition\", \"partition\", \"partition\", \"partition\", \"partition\", \"partition\", \"partition\", \"partition\", \"pbx\", \"pbx\", \"pbx\", \"pbx\", \"pbx\", \"pbx\", \"pbx\", \"pbx\", \"pbx\", \"pdfs\", \"pdfs\", \"pdfs\", \"pdfs\", \"pen\", \"pen\", \"pen\", \"pen\", \"pen\", \"pen\", \"pen\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"percentile\", \"percentile\", \"percentile\", \"percentile\", \"percentile\", \"percentile\", \"percentile\", \"percentile\", \"percentile\", \"percentile\", \"percep\", \"percep\", \"percep\", \"perceptron\", \"perceptron\", \"perceptron\", \"perceptron\", \"perceptron\", \"perceptron\", \"perceptron\", \"perceptron\", \"perceptron\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"pflat\", \"pflat\", \"pflat\", \"pflat\", \"pflat\", \"pflat\", \"pflat\", \"pflat\", \"pflat\", \"pflat\", \"phillips\", \"phillips\", \"phillips\", \"pi\", \"pi\", \"pi\", \"pi\", \"pi\", \"pi\", \"pi\", \"pi\", \"pi\", \"pi\", \"pik\", \"pik\", \"pik\", \"pik\", \"pik\", \"pik\", \"pitch\", \"pitch\", \"pitch\", \"pitch\", \"pitch\", \"pitch\", \"pitch\", \"pitch\", \"pitch\", \"pitch\", \"planning\", \"planning\", \"planning\", \"planning\", \"planning\", \"planning\", \"planning\", \"planning\", \"planning\", \"planning\", \"plit\", \"plit\", \"plit\", \"plit\", \"plit\", \"plit\", \"plit\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"points\", \"points\", \"points\", \"points\", \"points\", \"points\", \"points\", \"points\", \"points\", \"points\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"polyhedron\", \"polyhedron\", \"polyhedron\", \"polymatroid\", \"polymatroid\", \"polymatroid\", \"polymatroid\", \"polymatroid\", \"polymatroid\", \"polynomial\", \"polynomial\", \"polynomial\", \"polynomial\", \"polynomial\", \"polynomial\", \"polynomial\", \"polynomial\", \"polynomial\", \"polynomial\", \"polynomials\", \"polynomials\", \"polynomials\", \"polynomials\", \"polynomials\", \"polynomials\", \"polynomials\", \"polynomials\", \"polynomials\", \"polynomials\", \"predictions\", \"predictions\", \"predictions\", \"predictions\", \"predictions\", \"predictions\", \"predictions\", \"predictions\", \"predictions\", \"predictions\", \"predictor\", \"predictor\", \"predictor\", \"predictor\", \"predictor\", \"predictor\", \"predictor\", \"predictor\", \"predictor\", \"primitives\", \"primitives\", \"primitives\", \"primitives\", \"primitives\", \"primitives\", \"primitives\", \"primitives\", \"primitives\", \"primitives\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"procedural\", \"procedural\", \"procedural\", \"procedural\", \"procedural\", \"procedural\", \"procedural\", \"procedural\", \"procedural\", \"procedural\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"programs\", \"programs\", \"programs\", \"programs\", \"programs\", \"programs\", \"programs\", \"programs\", \"programs\", \"programs\", \"projectivity\", \"projectivity\", \"projectivity\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"psychiatric\", \"psychiatric\", \"psychiatric\", \"psychiatric\", \"psychiatric\", \"psychiatric\", \"psychiatric\", \"psychiatric\", \"px\", \"px\", \"px\", \"px\", \"px\", \"px\", \"px\", \"px\", \"px\", \"px\", \"pxy\", \"pxy\", \"pxy\", \"pxy\", \"pxy\", \"pxy\", \"pxy\", \"pxy\", \"pxy\", \"pxy\", \"py\", \"py\", \"py\", \"py\", \"py\", \"py\", \"py\", \"py\", \"py\", \"py\", \"pyp\", \"pyp\", \"pyp\", \"pyp\", \"pyp\", \"pyp\", \"pyp\", \"pyp\", \"pyp\", \"pyp\", \"qbcm\", \"qbcm\", \"qbcm\", \"qbcm\", \"qbcm\", \"qbcm\", \"qbcm\", \"qd\", \"qd\", \"qd\", \"qd\", \"qd\", \"qd\", \"qd\", \"qd\", \"qd\", \"qd\", \"qvalue\", \"rad\", \"rad\", \"rad\", \"rad\", \"rad\", \"rad\", \"rad\", \"rad\", \"rad\", \"rad\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rating\", \"ratings\", \"ratings\", \"ratings\", \"ratings\", \"ratings\", \"ratings\", \"ratings\", \"ratings\", \"ratings\", \"rats\", \"rats\", \"rats\", \"rats\", \"rats\", \"rats\", \"rats\", \"rats\", \"rearing\", \"rearing\", \"rearing\", \"rearing\", \"receptors\", \"receptors\", \"receptors\", \"receptors\", \"receptors\", \"receptors\", \"receptors\", \"receptors\", \"receptors\", \"recipe\", \"recipe\", \"recipe\", \"recipe\", \"recipe\", \"recipe\", \"recipe\", \"recipe\", \"recipes\", \"recipes\", \"recipes\", \"recipes\", \"recipes\", \"recipes\", \"recipes\", \"recipes\", \"recipes\", \"recipes\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regularizing\", \"regularizing\", \"regularizing\", \"regulatory\", \"regulatory\", \"regulatory\", \"regulatory\", \"regulatory\", \"regulatory\", \"regulatory\", \"regulatory\", \"rehder\", \"rehder\", \"relationships\", \"relationships\", \"relationships\", \"relationships\", \"relationships\", \"relationships\", \"relationships\", \"relationships\", \"relationships\", \"relationships\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"rendered\", \"rendered\", \"rendered\", \"rendered\", \"rendered\", \"rendered\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"retina\", \"retina\", \"retina\", \"retina\", \"retina\", \"retina\", \"retina\", \"retina\", \"retina\", \"retina\", \"retinas\", \"retinas\", \"retinas\", \"reward\", \"reward\", \"reward\", \"reward\", \"reward\", \"reward\", \"reward\", \"reward\", \"reward\", \"reward\", \"rights\", \"rights\", \"rights\", \"rights\", \"rigoll\", \"rigoll\", \"rigoll\", \"rigoll\", \"rigoll\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"risks\", \"risks\", \"risks\", \"risks\", \"risks\", \"risks\", \"risks\", \"risks\", \"risks\", \"rnn\", \"rnn\", \"rnn\", \"rnn\", \"rnn\", \"rnn\", \"rnn\", \"rnn\", \"rnn\", \"rnn\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robustness\", \"robustness\", \"robustness\", \"robustness\", \"robustness\", \"robustness\", \"robustness\", \"robustness\", \"robustness\", \"robustness\", \"roles\", \"roles\", \"roles\", \"roles\", \"romma\", \"romma\", \"romma\", \"romma\", \"romma\", \"romma\", \"romma\", \"romma\", \"romma\", \"romma\", \"rouge\", \"rouge\", \"rouge\", \"rouge\", \"rouge\", \"rouge\", \"rvm\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sanger\", \"sanger\", \"sanger\", \"sanger\", \"sanger\", \"sanger\", \"sas\", \"sas\", \"sas\", \"sas\", \"sas\", \"sas\", \"sas\", \"sas\", \"sbr\", \"schema\", \"schema\", \"schema\", \"schema\", \"schema\", \"schema\", \"schema\", \"schema\", \"schemas\", \"schemas\", \"schemas\", \"schemas\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"segments\", \"segments\", \"segments\", \"segments\", \"segments\", \"segments\", \"segments\", \"segments\", \"segments\", \"segments\", \"seligman\", \"seligman\", \"seligman\", \"sensory\", \"sensory\", \"sensory\", \"sensory\", \"sensory\", \"sensory\", \"sensory\", \"sensory\", \"sensory\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"sh\", \"sh\", \"sh\", \"sh\", \"sh\", \"sh\", \"sh\", \"sh\", \"sh\", \"sh\", \"shape\", \"shape\", \"shape\", \"shape\", \"shape\", \"shape\", \"shape\", \"shape\", \"shape\", \"shape\", \"shock\", \"shock\", \"shock\", \"shock\", \"shock\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"shrimp\", \"si\", \"si\", \"si\", \"si\", \"si\", \"si\", \"si\", \"si\", \"si\", \"si\", \"siegel\", \"siegel\", \"siegel\", \"siegel\", \"silicon\", \"silicon\", \"silicon\", \"silicon\", \"silicon\", \"silicon\", \"silicon\", \"silicon\", \"silicon\", \"silicon\", \"smoothing\", \"smoothing\", \"smoothing\", \"smoothing\", \"smoothing\", \"smoothing\", \"smoothing\", \"smoothing\", \"smoothing\", \"smoothing\", \"sos\", \"sos\", \"sos\", \"sos\", \"sos\", \"sos\", \"sos\", \"sos\", \"sos\", \"sos\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"splines\", \"splines\", \"splines\", \"splines\", \"splines\", \"splines\", \"split\", \"split\", \"split\", \"split\", \"split\", \"split\", \"split\", \"split\", \"split\", \"split\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"steorts\", \"steorts\", \"steorts\", \"steorts\", \"steorts\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"stereo\", \"stereo\", \"stereo\", \"stereo\", \"stereo\", \"stereo\", \"stereo\", \"stereo\", \"stereo\", \"stereo\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stock\", \"stock\", \"stock\", \"stock\", \"stock\", \"stock\", \"stock\", \"stock\", \"stocks\", \"stocks\", \"stocks\", \"stocks\", \"stocks\", \"stocks\", \"stocks\", \"stocks\", \"streams\", \"streams\", \"streams\", \"streams\", \"streams\", \"streams\", \"streams\", \"streams\", \"streams\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"subband\", \"subband\", \"subband\", \"subband\", \"subband\", \"subband\", \"subband\", \"subband\", \"subbands\", \"subbands\", \"subbands\", \"subbands\", \"subjects\", \"subjects\", \"subjects\", \"subjects\", \"subjects\", \"subjects\", \"subjects\", \"subjects\", \"subjects\", \"subjects\", \"sublinearly\", \"sublinearly\", \"sublinearly\", \"sublinearly\", \"sublinearly\", \"submodular\", \"submodular\", \"submodular\", \"submodular\", \"submodular\", \"submodular\", \"submodular\", \"submodular\", \"submodular\", \"submodular\", \"sumit\", \"sumit\", \"sumit\", \"summaries\", \"summaries\", \"summaries\", \"summaries\", \"summaries\", \"summaries\", \"summaries\", \"summaries\", \"surface\", \"surface\", \"surface\", \"surface\", \"surface\", \"surface\", \"surface\", \"surface\", \"surface\", \"sutton\", \"sutton\", \"sutton\", \"sutton\", \"sutton\", \"sutton\", \"sutton\", \"sutton\", \"sutton\", \"symplectic\", \"symplectic\", \"symplectic\", \"symplectic\", \"symplectic\", \"symplectic\", \"symplectic\", \"symplectic\", \"symplectic\", \"symplectic\", \"synapse\", \"synapse\", \"synapse\", \"synapse\", \"synapse\", \"synapse\", \"synapse\", \"synapse\", \"synapse\", \"synthesis\", \"synthesis\", \"synthesis\", \"synthesis\", \"synthesis\", \"synthesis\", \"synthesis\", \"synthesis\", \"synthesis\", \"synthesis\", \"synthesizing\", \"synthesizing\", \"syria\", \"syria\", \"syria\", \"syria\", \"syria\", \"syriasizes\", \"syriasizes\", \"syriasizes\", \"syriasizes\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"systems\", \"systems\", \"systems\", \"systems\", \"systems\", \"systems\", \"systems\", \"systems\", \"systems\", \"systems\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"td\", \"td\", \"td\", \"td\", \"td\", \"td\", \"td\", \"td\", \"td\", \"teacher\", \"teacher\", \"teacher\", \"teacher\", \"teacher\", \"teacher\", \"tense\", \"tense\", \"tense\", \"tense\", \"tense\", \"tense\", \"tense\", \"terence\", \"terence\", \"terence\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"textons\", \"textons\", \"textons\", \"textons\", \"textons\", \"textons\", \"textons\", \"textons\", \"textons\", \"texture\", \"texture\", \"texture\", \"texture\", \"texture\", \"texture\", \"texture\", \"texture\", \"texture\", \"texture\", \"thompson\", \"thompson\", \"thompson\", \"thompson\", \"thompson\", \"thompson\", \"thompson\", \"thompson\", \"thompson\", \"tid\", \"tid\", \"tid\", \"tid\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"timesteps\", \"timesteps\", \"timesteps\", \"timesteps\", \"timesteps\", \"timesteps\", \"timesteps\", \"timesteps\", \"timesteps\", \"timesteps\", \"todays\", \"todays\", \"todays\", \"tomorrows\", \"tomorrows\", \"tomorrows\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"transfonnation\", \"transfonnation\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"triphone\", \"trpo\", \"trpo\", \"trpo\", \"trpo\", \"trpo\", \"trpo\", \"trpo\", \"trpo\", \"trpo\", \"trpo\", \"trust\", \"trust\", \"trust\", \"trust\", \"trust\", \"trust\", \"trust\", \"trust\", \"trust\", \"turtle\", \"turtle\", \"turtle\", \"turtle\", \"turtle\", \"turtle\", \"turtle\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"ubmodular\", \"ubmodular\", \"ubmodular\", \"uct\", \"uct\", \"uct\", \"uct\", \"uct\", \"uct\", \"uct\", \"uct\", \"uct\", \"uct\", \"ucto\", \"ucto\", \"ucto\", \"ucto\", \"ucto\", \"undergraduates\", \"unique\", \"unique\", \"unique\", \"unique\", \"unique\", \"unique\", \"unique\", \"unique\", \"unique\", \"unique\", \"universal\", \"universal\", \"universal\", \"universal\", \"universal\", \"universal\", \"universal\", \"universal\", \"universal\", \"universal\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"values\", \"values\", \"values\", \"values\", \"values\", \"values\", \"values\", \"values\", \"values\", \"values\", \"variables\", \"variables\", \"variables\", \"variables\", \"variables\", \"variables\", \"variables\", \"variables\", \"variables\", \"variables\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vestibular\", \"vestibular\", \"vestibular\", \"vestibular\", \"vestibular\", \"vestibular\", \"vestibular\", \"vestibular\", \"vestibular\", \"vestibular\", \"victoria\", \"voted\", \"voted\", \"voted\", \"voted\", \"vthd\", \"vthd\", \"vthd\", \"vthd\", \"vthd\", \"vthd\", \"vy\", \"vy\", \"vy\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"whitened\", \"whitened\", \"whitened\", \"whitened\", \"whitened\", \"whitened\", \"whitened\", \"willett\", \"willett\", \"willett\", \"wsas\", \"wsas\", \"wsas\", \"wsas\", \"wsas\", \"wsas\", \"wt\", \"wt\", \"wt\", \"wt\", \"wt\", \"wt\", \"wt\", \"wt\", \"wt\", \"wt\", \"wtll\", \"wtll\", \"xa\", \"xa\", \"xa\", \"xa\", \"xa\", \"xa\", \"xa\", \"xa\", \"xf\", \"xf\", \"xf\", \"xf\", \"xf\", \"xf\", \"xf\", \"xf\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xiaoping\", \"xij\", \"xij\", \"xij\", \"xij\", \"xij\", \"xij\", \"xij\", \"xij\", \"xij\", \"xiw\", \"xlw\", \"xlw\", \"xlw\", \"xsa\", \"xsa\", \"xsa\", \"xsa\", \"xsa\", \"xsa\", \"xsa\", \"xu\", \"xu\", \"xu\", \"xu\", \"xu\", \"xu\", \"xu\", \"xu\", \"xu\", \"xu\", \"yi\", \"yi\", \"yi\", \"yi\", \"yi\", \"yi\", \"yi\", \"yi\", \"yi\", \"yi\", \"yoked\", \"yoked\", \"yoked\", \"yoked\", \"yoked\", \"yoked\", \"yoked\", \"ysa\", \"ysa\", \"ysa\", \"ysa\", \"ysa\", \"yt\", \"yt\", \"yt\", \"yt\", \"yt\", \"yt\", \"yt\", \"yt\", \"yt\", \"yt\", \"ytxtlllxtll\", \"ytxtlllxtll\", \"zk\", \"zk\", \"zk\", \"zk\", \"zk\", \"zk\", \"zk\", \"zk\", \"zk\", \"zk\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [9, 7, 4, 3, 10, 8, 2, 5, 6, 1]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el28481335360893575844802989362\", ldavis_el28481335360893575844802989362_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el28481335360893575844802989362\", ldavis_el28481335360893575844802989362_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el28481335360893575844802989362\", ldavis_el28481335360893575844802989362_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eSmF0KirdIfT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}